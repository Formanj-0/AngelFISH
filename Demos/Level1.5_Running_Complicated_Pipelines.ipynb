{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    # Get the number of GPUs available\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "    # Get the name of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "\n",
    "    # Get the current GPU memory usage\n",
    "    for i in range(num_gpus):\n",
    "        gpu_memory_allocated = torch.cuda.memory_allocated(i)\n",
    "        gpu_memory_reserved = torch.cuda.memory_reserved(i)\n",
    "        print(f\"GPU {i} memory allocated: {gpu_memory_allocated / (1024 ** 3):.2f} GB\")\n",
    "        print(f\"GPU {i} memory reserved: {gpu_memory_reserved / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import numba\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)\n",
    "\n",
    "matplotlib_logger = logging.getLogger('matplotlib')\n",
    "matplotlib_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.IndependentSteps import Pycromanager2H5, FFF2H5, Make_Output_Dir_JF, Make_Analysis_Dir_JF, \\\n",
    "                                    ConsolidateImageShapes, TrimZSlices, AutomaticSpotDetection_JF, Avg_Parameters\n",
    "\n",
    "from src.SequentialSteps import CellSegmentationStepClass_JF, BIGFISH_SpotDetection, SimpleCellposeSegmentaion\n",
    "\n",
    "from src.FinalizationSteps import Save_Outputs, Save_Images, Save_Parameters, Save_Masks, return_to_NAS\n",
    "\n",
    "from src.Parameters import Parameters, Experiment, Settings, ScopeClass, DataContainer\n",
    "\n",
    "from src.GeneralOutput import OutputClass\n",
    "\n",
    "from src.GeneralStep import StepClass\n",
    "\n",
    "from src.Displays import Display\n",
    "\n",
    "from src.GUI import GUI, StepGUI\n",
    "\n",
    "from src.Pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "scope = ScopeClass() \n",
    "data = DataContainer() # you can also initialize these with parameters, but it is not necessary due to defaults\n",
    "settings = Settings(name='demo') # you also must give a name for the analysis your are doing\n",
    "experiment = Experiment()\n",
    "\n",
    "experiment.FISHChannel = 0\n",
    "experiment.nucChannel = 2\n",
    "experiment.voxel_size_z = 500\n",
    "\n",
    "settings.num_chunks_to_run = 1 # this is the limit to how many chunks you want to run and it will stop after that\n",
    "\n",
    "scope.spot_yx = 130\n",
    "scope.spot_z = 360\n",
    "scope.voxel_size_yx = 100\n",
    "\n",
    "settings.load_in_mask = True\n",
    "\n",
    "settings.cellpose_min_size = 500\n",
    "settings.cellpose_diameter = [180, 90] # most of these options can be done for individually cyto and nuc segmentation, \n",
    "                                        # and a list can be or a single float can be passed for both\n",
    "                                        # always in the order cyto, nuc\n",
    "settings.cellpose_pretrained_model = [r\"C:\\Users\\Jack\\Documents\\GitHub\\FISH_Processing\\models\\GAPDH_cyto\", r'C:\\Users\\Jack\\Documents\\GitHub\\FISH_Processing\\models\\DAPI_nuclei'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.todict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_to_run = dataset_to_avg =  ['smFISH_images/Eric_smFISH_images/20220225/DUSP1_Dex_0min_20220224',\n",
    "                                    'smFISH_images/Eric_smFISH_images/20220225/DUSP1_Dex_10min_20220224']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF2H5()\n",
    "SimpleCellposeSegmentaion()\n",
    "BIGFISH_SpotDetection()\n",
    "Save_Masks()\n",
    "Save_Parameters()\n",
    "Save_Outputs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(dataset_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline().clear_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "scope = ScopeClass() \n",
    "data = DataContainer() # you can also initialize these with parameters, but it is not necessary due to defaults\n",
    "settings = Settings(name='demo1') # you also must give a name for the analysis your are doing\n",
    "experiment = Experiment()\n",
    "\n",
    "experiment.FISHChannel = 0\n",
    "experiment.nucChannel = 2\n",
    "experiment.voxel_size_z = 500\n",
    "\n",
    "settings.num_chunks_to_run = 1 # this is the limit to how many chunks you want to run and it will stop after that\n",
    "\n",
    "scope.spot_yx = 130\n",
    "scope.spot_z = 360\n",
    "scope.voxel_size_yx = 100\n",
    "\n",
    "settings.load_in_mask = True\n",
    "dataset_to_run = dataset_to_avg =  ['smFISH_images/Eric_smFISH_images/20220225/DUSP1_Dex_0min_20220224',\n",
    "                                    'smFISH_images/Eric_smFISH_images/20220225/DUSP1_Dex_10min_20220224']\n",
    "\n",
    "settings.dataset_to_avg = dataset_to_avg\n",
    "settings.previous_analysis_name = 'demo'\n",
    "settings.params_to_avg = ['bigfish_threshold']\n",
    "\n",
    "settings.todict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF2H5()\n",
    "Avg_Parameters()\n",
    "BIGFISH_SpotDetection()\n",
    "Save_Outputs()\n",
    "return_to_NAS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline(dataset_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
