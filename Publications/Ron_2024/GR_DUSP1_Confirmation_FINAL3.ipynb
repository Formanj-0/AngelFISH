{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da491983",
   "metadata": {},
   "source": [
    "# DUSP1 & GR Confirmation and Visualization Notebook\n",
    "\n",
    "This notebook integrates DUSP1 smiFISH and GR ICC data using modular classes for data loading, SNR-based classification, measurement extraction, filtering, visualization, and normalization. It performs:\n",
    "\n",
    "- **DUSP1 spot and cluster detection**, quality filtering, and measurement.\n",
    "- **GR quantification and gating** from ICC images.\n",
    "- **Visualization and inspection** of background-corrected measurements.\n",
    "- **Per-condition normalization** using background-dependent scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Input**\n",
    "- Big-FISH CSV files (`spots`, `clusters`)\n",
    "- CellProperties CSV files (`cell_props`, `cell_results`)\n",
    "- GR ICC cell properties from `GR_Confirmation`\n",
    "\n",
    "---\n",
    "\n",
    "### **Workflow Overview**\n",
    "\n",
    "1. **Load Experimental Data**\n",
    "   - Use `DUSP1AnalysisManager` or `AnalysisManager` to identify and load HDF5 or CSV datasets.\n",
    "\n",
    "2. **Classify Spots by Signal Quality (SNR Analysis)**\n",
    "   - Use `SNRAnalysis` to apply MG, absolute, weighted, or combined SNR methods to label high-confidence spots.\n",
    "\n",
    "3. **Extract and Quantify Cell-level Measurements**\n",
    "   - Use `DUSP1Measurement` and `measure_GR()` to obtain nuclear and cytoplasmic counts and intensities.\n",
    "\n",
    "4. **Apply Filtering Criteria**\n",
    "   - Use `DUSP1_filtering` to prune poor-quality spots, clusters, and cells.\n",
    "\n",
    "5. **Save Results**\n",
    "   - Export `FinalSpots`, `FinalClusters`, `FinalCellProps`, and `SSITcellresults` for downstream use.\n",
    "\n",
    "6. **Visualization and Correlation Inspection**\n",
    "   - Use `DUSP1DisplayManager` and seaborn/matplotlib plots to inspect image overlays, distributions, and background-intensity relationships.\n",
    "   - Use `GRConfirmation` to validate GR ICC metrics and visualize intensity distributions.\n",
    "\n",
    "7. **GR-ICC Per-condition Normalization**\n",
    "   - Fit linear models between background and intensity per (Dex, Time) condition.\n",
    "   - Apply reference-background normalization to obtain scaled nuclear and cytoplasmic intensities.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Classes**\n",
    "\n",
    "- **`DUSP1AnalysisManager`** – Loads HDF5 or CSV datasets and handles indexing by analysis name.\n",
    "- **`SNRAnalysis`** – Performs SNR-based classification of smiFISH spots.\n",
    "- **`DUSP1Measurement`** – Aggregates spot and cluster metrics to the cell level.\n",
    "- **`DUSP1_filtering`** – Applies thresholds to prune poor-quality data.\n",
    "- **`DUSP1DisplayManager`** – Visualizes images, segmentation overlays, and crops.\n",
    "- **`PostProcessingPlotter`** – Creates line, ridge, and bar plots summarizing smFISH results.\n",
    "- **`GR_Confirmation`** – Loads, validates, and extracts ICC GR intensity metrics.\n",
    "- **`GR_DisplayBasic`** – Basic visualization utilities for GR ICC overlays and inspection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d619e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from time import sleep\n",
    "\n",
    "# Today's date\n",
    "today = datetime.date.today()\n",
    "# Format date as 'Jun03' (for example)\n",
    "date_str = today.strftime(\"%b%d\")\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)\n",
    "\n",
    "matplotlib_logger = logging.getLogger('matplotlib')\n",
    "matplotlib_logger.setLevel(logging.WARNING)\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "print(src_path)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.Analysis_DUSP1 import DUSP1AnalysisManager, SNRAnalysis, DUSP1Measurement, DUSP1_filtering, DUSP1DisplayManager, PostProcessingPlotter, DUSP1DisplayManager\n",
    "from src.Analysis_GR import AnalysisManager, GR_Confirmation, GR_DisplayBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd82ba",
   "metadata": {},
   "source": [
    "**`DUSP1AnalysisManager`** \n",
    "   - Manages HDF5 file access.\n",
    "   - Extracts file paths from a log directory (if no direct locations are provided).\n",
    "   - Provides methods to select an analysis (by name) and load datasets from HDF5 files.\n",
    "   - Saves datasets as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c374e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = None\n",
    "log_location = r'/Volumes/share/Users/Eric/GR_DUSP1_reruns'\n",
    "save_dir = r'/Volumes/share/Users/Eric/GR_DUSP1_AllData/FinalAnalysis_GR_DUSP1'\n",
    "img_dir = r'/Volumes/share/Users/Eric/GR_DUSP1_AllData/FinalAnalysis_GR_DUSP1/Images'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "\n",
    "# Define filtering method and thresholds\n",
    "method = 'mg_abs'            # options: 'mg', 'absolute', 'mg_abs', 'weighted', 'rf', 'none'\n",
    "mac = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.list_analysis_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa5eb7",
   "metadata": {},
   "source": [
    "**`Combine Analyses function`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670416ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_combined_manager(loc, log_location, mac, analysis_names):\n",
    "    \"\"\"\n",
    "    Build a single DUSP1AnalysisManager that “knows about” all of the N analyses.\n",
    "    \n",
    "    Parameters:\n",
    "      - loc, log_location, mac: passed straight through to each temp manager\n",
    "      - analysis_names: list of the analysis_name strings for each analysis\n",
    "    Returns:\n",
    "      - a DUSP1AnalysisManager whose .location and .analysis_names\n",
    "        cover every file + analysis in analysis_names\n",
    "    \"\"\"\n",
    "    all_paths = []\n",
    "    all_anames = []\n",
    "\n",
    "    for name in analysis_names:\n",
    "        temp = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=mac)\n",
    "        try:\n",
    "            temp.select_analysis(name)\n",
    "            # grab its now-filtered list of files + names\n",
    "            all_paths   .extend(temp.location)\n",
    "            all_anames  .extend(temp.analysis_names)\n",
    "        finally:\n",
    "            temp.close()    # <— guarantee no HDF5 leak\n",
    "    # now build the combined\n",
    "    combo = DUSP1AnalysisManager(location=all_paths, mac=mac)\n",
    "    # manually inject the unioned analysis names\n",
    "    combo.analysis_names = all_anames\n",
    "    return combo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e7b38",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica D 3hr 100nM time-sweep R1 - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076abd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_D_slide1_BFmean_061825',\n",
    "    'DUSP1_D_slide2_BFmean_061825',\n",
    "    'DUSP1_D_slide3_BFmean_061825',\n",
    "    'DUSP1_D_slide4_BFmean_061825',\n",
    "    'DUSP1_D_slide5_BFmean_061825',\n",
    "    'DUSP1_D_slide6_BFmean_061825',\n",
    "    'DUSP1_D_slide7_BFmean_061825',\n",
    "    'DUSP1_D_slide8_BFmean_061825',\n",
    "    'DUSP1_D_slide9_BFmean_061825',\n",
    "    'DUSP1_D_slide10_BFmean_061825',\n",
    "    'DUSP1_D_slide11_BFmean_061825',\n",
    "    'DUSP1_D_slide12_BFmean_061825',        \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) apply your unique‐ID prefix logic ─────────────────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 10\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "# spots\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "# clusters\n",
    "max_cluster_id   = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix   = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) filtering ─────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post-filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_D_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "# Time sweep for 100nM Dex\n",
    "print(\"\\n>>> Time sweep for 100nM Dex\")\n",
    "plotter.plot_time_sweep(\n",
    "    dex_conc=100,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef0f62",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica E 3hr 100nM time-sweep R2 - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71083d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_E_slide1_BFmean_061825',\n",
    "    'DUSP1_E_slide2_BFmean_061825',\n",
    "    'DUSP1_E_slide3_BFmean_061825',\n",
    "    'DUSP1_E_slide4_BFmean_061825',\n",
    "    'DUSP1_E_slide5_BFmean_061825',\n",
    "    'DUSP1_E_slide6_BFmean_061825',\n",
    "    'DUSP1_E_slide7_BFmean_061825',\n",
    "    'DUSP1_E_slide8_BFmean_061825',\n",
    "    'DUSP1_E_slide9_BFmean_061825',\n",
    "    'DUSP1_E_slide10_BFmean_061825',\n",
    "    'DUSP1_E_slide11_BFmean_061825',\n",
    "    'DUSP1_E_slide12_BFmean_061825',        \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) apply your unique‐ID prefix logic ─────────────────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 20\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "# spots\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "# clusters\n",
    "max_cluster_id   = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix   = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) filtering ─────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post-filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_E_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "# Time sweep for 100nM Dex\n",
    "print(\"\\n>>> Time sweep for 100nM Dex\")\n",
    "plotter.plot_time_sweep(\n",
    "    dex_conc=100,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed78e43",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica F 3hr 100nM time-sweep R3 - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_F_slide1_BFmean_061825',\n",
    "    'DUSP1_F_slide2_BFmean_061825',\n",
    "    'DUSP1_F_slide3_BFmean_061825',\n",
    "    'DUSP1_F_slide4_BFmean_061825',\n",
    "    'DUSP1_F_slide5_BFmean_061825',\n",
    "    'DUSP1_F_slide6_BFmean_061825',\n",
    "    'DUSP1_F_slide7_BFmean_061825',\n",
    "    'DUSP1_F_slide8_BFmean_061825',\n",
    "    'DUSP1_F_slide9_BFmean_061825',\n",
    "    'DUSP1_F_slide10_BFmean_061825',\n",
    "    'DUSP1_F_slide11_BFmean_061825',\n",
    "    'DUSP1_F_slide12_BFmean_061825',       \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) apply your unique‐ID prefix logic ─────────────────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 30\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "# spots\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "# clusters\n",
    "max_cluster_id   = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix   = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) filtering ─────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post-filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_F_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "# Time sweep for 100nM Dex\n",
    "print(\"\\n>>> Time sweep for 100nM Dex\")\n",
    "plotter.plot_time_sweep(\n",
    "    dex_conc=100,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d007169",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica M 3hr 100nM time-sweep Partial Replica - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da40d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_M_slide1_BFmean_061825',\n",
    "    'DUSP1_M_slide2_BFmean_061825',\n",
    "    'DUSP1_M_slide3_BFmean_061825',      \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) apply your unique‐ID prefix logic ─────────────────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 40\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "# spots\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "# clusters\n",
    "max_cluster_id   = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix   = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) filtering ─────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post-filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_M_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "# Time sweep for 100nM Dex\n",
    "print(\"\\n>>> Time sweep for 100nM Dex\")\n",
    "plotter.plot_time_sweep(\n",
    "    dex_conc=100,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509bca6a",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica N 3hr 100nM time-sweep Partial Replica - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_N_slide1_BFmean_061825',\n",
    "    'DUSP1_N_slide2_BFmean_061825',\n",
    "    'DUSP1_N_slide3_BFmean_061825',\n",
    "    'DUSP1_N_slide4_BFmean_061825',\n",
    "    'DUSP1_N_slide5_BFmean_061825',\n",
    "    'DUSP1_N_slide6_BFmean_061825',\n",
    "    'DUSP1_N_slide7_BFmean_061825',      \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) apply your unique‐ID prefix logic ─────────────────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 50\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "# spots\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "# clusters\n",
    "max_cluster_id   = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix   = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) filtering ─────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post-filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_N_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "# Time sweep for 100nM Dex\n",
    "print(\"\\n>>> Time sweep for 100nM Dex\")\n",
    "plotter.plot_time_sweep(\n",
    "    dex_conc=100,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421dce4",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica J 3hr time-concentration sweep R1 - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafeb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_J_slide1_BFmean_061825',\n",
    "    'DUSP1_J_slide2_BFmean_061825',\n",
    "    'DUSP1_J_slide3_BFmean_061825',\n",
    "    'DUSP1_J_slide4_BFmean_061825',\n",
    "    'DUSP1_J_slide5_BFmean_061825',\n",
    "    'DUSP1_J_slide6_BFmean_061825',\n",
    "    'DUSP1_J_slide7_BFmean_061825',\n",
    "    'DUSP1_J_slide8_BFmean_061825',\n",
    "    'DUSP1_J_slide9_BFmean_061825',\n",
    "    'DUSP1_J_slide10_BFmean_061825',\n",
    "    'DUSP1_J_slide11_BFmean_061825',\n",
    "    'DUSP1_J_slide12_BFmean_061825',\n",
    "    'DUSP1_J_slide13_BFmean_061825',\n",
    "    'DUSP1_J_slide14_BFmean_061825',\n",
    "    'DUSP1_J_slide15_BFmean_061825',\n",
    "    'DUSP1_J_slide16_BFmean_061825',\n",
    "    'DUSP1_J_slide17_BFmean_061825',\n",
    "    'DUSP1_J_slide18_BFmean_061825',\n",
    "    'DUSP1_J_slide19_BFmean_061825',        \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) apply your unique‐ID prefix logic ─────────────────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 60\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "# spots\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "# clusters\n",
    "max_cluster_id   = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix   = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) filtering ─────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post-filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_J_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "# Time + concentration sweeps for [0.3, 1, 10] nM Dex\n",
    "print(\"\\n>>> Time+Conc sweeps for [0.3, 1, 10] nM Dex\")\n",
    "plotter.plot_time_conc_sweep(\n",
    "    conc_list=[0.3, 1, 10],\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561102f",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica K 3hr time-concentration sweep R2 - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_K_slide1_BFmean_061825',\n",
    "    'DUSP1_K_slide2_BFmean_061825',\n",
    "    'DUSP1_K_slide3_BFmean_061825',\n",
    "    'DUSP1_K_slide4_BFmean_061825',\n",
    "    'DUSP1_K_slide5_BFmean_061825',\n",
    "    'DUSP1_K_slide6_BFmean_061825',\n",
    "    'DUSP1_K_slide7_BFmean_061825',\n",
    "    'DUSP1_K_slide8_BFmean_061825',\n",
    "    'DUSP1_K_slide9_BFmean_061825',\n",
    "    'DUSP1_K_slide10_BFmean_061825',\n",
    "    'DUSP1_K_slide11_BFmean_061825',\n",
    "    'DUSP1_K_slide12_BFmean_061825',\n",
    "    'DUSP1_K_slide13_BFmean_061825',\n",
    "    'DUSP1_K_slide14_BFmean_061825',\n",
    "    'DUSP1_K_slide15_BFmean_061825',\n",
    "    'DUSP1_K_slide16_BFmean_061825',\n",
    "    'DUSP1_K_slide17_BFmean_061825',\n",
    "    'DUSP1_K_slide18_BFmean_061825',\n",
    "    'DUSP1_K_slide19_BFmean_061825',        \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) Prefix unique IDs (prefix=70 for K replicate) ─────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 70\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) Filtering ───────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post‐filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_K_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Time+Conc sweeps for [0.3, 1, 10] nM Dex\")\n",
    "plotter.plot_time_conc_sweep(\n",
    "    conc_list=[0.3, 1, 10],\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7a426",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica L 3hr 100nM time-concentration sweep R2 - By Group`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_L_day1_90th_BF90th',\n",
    "    'DUSP1_L_day2_mean_BFmean',\n",
    "    'DUSP1_L_day3_25th_BF25th',\n",
    "    'DUSP1_L_day4_25th_BF25th',       \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) Prefix unique IDs (prefix=70 for K replicate) ─────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 80\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) Filtering ───────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post‐filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_L_BFmix'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "print(\"\\n>>> Time+Conc sweeps for [0.3, 1, 10] nM Dex\")\n",
    "plotter.plot_time_conc_sweep(\n",
    "    conc_list=[0.3, 1, 10],\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38606859",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica G 75min concentration sweep R1 - By Group`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f91e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0) Define the three‐day analysis names for DUSP1_G\n",
    "# analysis_names = [\n",
    "#     'DUSP1_G_slide1_BFmean_061825',\n",
    "#     'DUSP1_G_slide2_BFmean_061825',\n",
    "#     'DUSP1_G_slide3_BFmean_061825',\n",
    "#     'DUSP1_G_slide4_BFmean_061825',\n",
    "#     'DUSP1_G_slide5_BFmean_061825',\n",
    "#     'DUSP1_G_slide6_BFmean_061825',\n",
    "#     'DUSP1_G_slide7_BFmean_061825',\n",
    "#     'DUSP1_G_slide8_BFmean_061825',\n",
    "#     'DUSP1_G_slide9_BFmean_061825',\n",
    "# ]\n",
    "\n",
    "# # 1) Load & concatenate all slides for DUSP1_G\n",
    "# dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "# spots_df    = dm.select_datasets(\"spotresults\",     dtype=\"dataframe\")\n",
    "# clusters_df = dm.select_datasets(\"clusterresults\",  dtype=\"dataframe\")\n",
    "# props_df    = dm.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "# dm.close()\n",
    "\n",
    "# print(f\"Loaded {len(analysis_names)} slides for G: \"\n",
    "#       f\"spots {spots_df.shape}, clusters {clusters_df.shape}, props {props_df.shape}\")\n",
    "\n",
    "# # 2) SNRAnalysis → DUSP1Measurement\n",
    "# abs_threshold = 6\n",
    "# mg_threshold  = 3\n",
    "\n",
    "# snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "#                     abs_threshold=abs_threshold,\n",
    "#                     mg_threshold=mg_threshold)\n",
    "# merged_spots, merged_clusters, merged_cellprops = snr.get_results()\n",
    "\n",
    "# dusp = DUSP1Measurement(merged_spots, merged_clusters, merged_cellprops)\n",
    "# cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "#                                       mg_threshold=mg_threshold)\n",
    "\n",
    "# # 3) Prefix unique IDs (prefix=90 for G replicate)\n",
    "# max_id   = merged_cellprops['unique_cell_id'].max()\n",
    "# digits   = len(str(max_id))\n",
    "# rep_prefix = 90\n",
    "# prefix   = rep_prefix * (10 ** digits)\n",
    "\n",
    "# for df in (merged_spots, merged_clusters, merged_cellprops, cell_level_results):\n",
    "#     df['unique_cell_id'] += prefix\n",
    "\n",
    "# # also prefix spot and cluster IDs\n",
    "# max_spot_id    = merged_spots['unique_spot_id'].max()\n",
    "# spot_prefix    = rep_prefix ** len(str(max_spot_id))\n",
    "# merged_spots   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "# max_cluster_id = merged_clusters['unique_cluster_id'].max()\n",
    "# cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "# merged_clusters['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# # ─── 4) Filtering ───────────────────────────────────────────────────────────────\n",
    "# filterer = DUSP1_filtering(method=method,\n",
    "#                            abs_threshold=abs_threshold,\n",
    "#                            mg_threshold=mg_threshold)\n",
    "# filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "#     filterer.apply_all(spots=merged_spots_df,\n",
    "#                        clusters=merged_clusters_df,\n",
    "#                        cellprops=merged_cellprops_df)\n",
    "\n",
    "# # ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# # build a fresh manager so the display code can find every HDF5\n",
    "# dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "# display_manager = DUSP1DisplayManager(\n",
    "#     dm_disp,\n",
    "#     cell_level_results=SSITcellresults,\n",
    "#     spots=filtered_spots,\n",
    "#     clusters=filtered_clusters,\n",
    "#     cellprops=filtered_cellprops,\n",
    "#     removed_spots=removed_spots\n",
    "# )\n",
    "# display_manager.default_display(\n",
    "#     per_h5=1,\n",
    "#     show=False,                 # Preview in notebook\n",
    "#     save=True,                 # Save PNGs\n",
    "#     outdir=img_dir,\n",
    "#     dpi_notebook=120,          # lighter for notebook\n",
    "#     dpi_save=600,              # publication quality\n",
    "#     show_removed=True,         # overlay removed spots in crops\n",
    "#     removed_color=\"red\"  # distinct color for removed\n",
    "# )\n",
    "# dm_disp.close()\n",
    "\n",
    "# # 6) Save only the final, post‐filter CSVs\n",
    "# output_dir = save_dir\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# rep_string = 'DUSP1_G_BFmean'\n",
    "# base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "# SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "# filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "# filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "# filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "# removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "# print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# # ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "# plotter = PostProcessingPlotter(\n",
    "#     clusters_df=filtered_clusters,\n",
    "#     cellprops_df=filtered_cellprops,\n",
    "#     ssit_df=SSITcellresults,\n",
    "#     is_tpl= False,\n",
    "# )\n",
    "# times = sorted(SSITcellresults['time'].unique())\n",
    "# nonzero = [t for t in times if t != 0]\n",
    "# if len(nonzero) != 1:\n",
    "#     raise ValueError(f\"Expected exactly one non-zero time for G sweep, found {nonzero}\")\n",
    "# timepoint_G = nonzero[0]\n",
    "\n",
    "# print(f\"\\n>>> Concentration sweep at t={timepoint_G} min for G replicate\")\n",
    "# plotter.plot_conc_sweep(\n",
    "#     timepoint=timepoint_G,\n",
    "#     save_dir=img_dir,\n",
    "#     display=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c586708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Define the three‐day analysis names for DUSP1_G\n",
    "analysis_names = [\n",
    "    'DUSP1_G_day1_BF75th',\n",
    "    'DUSP1_G_day2_BF75th',\n",
    "    'DUSP1_G_day3_75th',\n",
    "]\n",
    "\n",
    "# 1) Load & concatenate all slides for DUSP1_G\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "spots_df    = dm.select_datasets(\"spotresults\",     dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",  dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "dm.close()\n",
    "\n",
    "print(f\"Loaded {len(analysis_names)} slides for G: \"\n",
    "      f\"spots {spots_df.shape}, clusters {clusters_df.shape}, props {props_df.shape}\")\n",
    "\n",
    "# 2) SNRAnalysis → DUSP1Measurement\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                    abs_threshold=abs_threshold,\n",
    "                    mg_threshold=mg_threshold)\n",
    "merged_spots, merged_clusters, merged_cellprops = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots, merged_clusters, merged_cellprops)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                      mg_threshold=mg_threshold)\n",
    "\n",
    "# 3) Prefix unique IDs (prefix=90 for G replicate)\n",
    "max_id   = merged_cellprops['unique_cell_id'].max()\n",
    "digits   = len(str(max_id))\n",
    "rep_prefix = 90\n",
    "prefix   = rep_prefix * (10 ** digits)\n",
    "\n",
    "for df in (merged_spots, merged_clusters, merged_cellprops, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "# also prefix spot and cluster IDs\n",
    "max_spot_id    = merged_spots['unique_spot_id'].max()\n",
    "spot_prefix    = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) Filtering ───────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots,\n",
    "                       clusters=merged_clusters,\n",
    "                       cellprops=merged_cellprops)\n",
    "\n",
    "# # ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# # build a fresh manager so the display code can find every HDF5\n",
    "# dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "# display_manager = DUSP1DisplayManager(\n",
    "#     dm_disp,\n",
    "#     cell_level_results=SSITcellresults,\n",
    "#     spots=filtered_spots,\n",
    "#     clusters=filtered_clusters,\n",
    "#     cellprops=filtered_cellprops,\n",
    "#     removed_spots=removed_spots\n",
    "# )\n",
    "# display_manager.default_display(\n",
    "#     per_h5=1,\n",
    "#     show=False,                 # Preview in notebook\n",
    "#     save=True,                 # Save PNGs\n",
    "#     outdir=img_dir,\n",
    "#     dpi_notebook=120,          # lighter for notebook\n",
    "#     dpi_save=600,              # publication quality\n",
    "#     show_removed=True,         # overlay removed spots in crops\n",
    "#     removed_color=\"red\"  # distinct color for removed\n",
    "# )\n",
    "# dm_disp.close()\n",
    "\n",
    "# 6) Save only the final, post‐filter CSVs\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_G_BF75th'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "times = sorted(SSITcellresults['time'].unique())\n",
    "nonzero = [t for t in times if t != 0]\n",
    "if len(nonzero) != 1:\n",
    "    raise ValueError(f\"Expected exactly one non-zero time for G sweep, found {nonzero}\")\n",
    "timepoint_G = nonzero[0]\n",
    "\n",
    "print(f\"\\n>>> Concentration sweep at t={timepoint_G} min for G replicate\")\n",
    "plotter.plot_conc_sweep(\n",
    "    timepoint=timepoint_G,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7297b",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica H 75min concentration sweep R2 - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17142d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_H_slide1_BFmean_061825',\n",
    "    'DUSP1_H_slide2_BFmean_061825',\n",
    "    'DUSP1_H_slide3_BFmean_061825',\n",
    "    'DUSP1_H_slide4_BFmean_061825',\n",
    "    'DUSP1_H_slide5_BFmean_061825',\n",
    "    'DUSP1_H_slide6_BFmean_061825',\n",
    "    'DUSP1_H_slide7_BFmean_061825',\n",
    "    'DUSP1_H_slide8_BFmean_061825',    \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    # free all HDF5 handles\n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) Prefix unique IDs (prefix=70 for K replicate) ─────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 110\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) Filtering ───────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post‐filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_H_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "times = sorted(SSITcellresults['time'].unique())\n",
    "nonzero = [t for t in times if t != 0]\n",
    "if len(nonzero) != 1:\n",
    "    raise ValueError(f\"Expected exactly one non-zero time for H sweep, found {nonzero}\")\n",
    "timepoint = nonzero[0]\n",
    "\n",
    "print(f\"\\n>>> Concentration sweep at t={timepoint} min for H replicate\")\n",
    "plotter.plot_conc_sweep(\n",
    "    timepoint=timepoint,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca347d",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica I 75min concentration sweep R3 - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_I_slide1_BFmean_061825',\n",
    "    'DUSP1_I_slide2_BFmean_061825',\n",
    "    'DUSP1_I_slide3_BFmean_061825',\n",
    "    'DUSP1_I_slide4_BFmean_061825',\n",
    "    'DUSP1_I_slide5_BFmean_061825',\n",
    "    'DUSP1_I_slide6_BFmean_061825',\n",
    "    'DUSP1_I_slide7_BFmean_061825',\n",
    "    'DUSP1_I_slide8_BFmean_061825',\n",
    "    'DUSP1_I_slide9_BFmean_061825',      \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    \n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) Prefix unique IDs (prefix=70 for K replicate) ─────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 120\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) Filtering ───────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold)\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post‐filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_I_BFmean'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) post-processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl= False,\n",
    ")\n",
    "\n",
    "times = sorted(SSITcellresults['time'].unique())\n",
    "nonzero = [t for t in times if t != 0]\n",
    "if len(nonzero) != 1:\n",
    "    raise ValueError(f\"Expected exactly one non-zero time for I sweep, found {nonzero}\")\n",
    "timepoint = nonzero[0]\n",
    "\n",
    "print(f\"\\n>>> Concentration sweep at t={timepoint} min for I replicate\")\n",
    "plotter.plot_conc_sweep(\n",
    "    timepoint=timepoint,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c774b6",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica O Triptolide Time-sweep - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_O_BFmean_slide1_062025',\n",
    "    'DUSP1_O_BFmean_slide2_062025',\n",
    "    'DUSP1_O_BFmean_slide3_062025',\n",
    "    'DUSP1_O_BFmean_slide4_062025',\n",
    "    'DUSP1_O_BFmean_slide5_062025',\n",
    "    'DUSP1_O_BFmean_slide6_062025',\n",
    "    'DUSP1_O_BFmean_slide7_062025',\n",
    "    'DUSP1_O_BFmean_slide8_062025',\n",
    "    'DUSP1_O_BFmean_slide9_062025',      \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    \n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"spots column keys: {spots_df.keys()}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"clusters column keys: {clusters_df.keys()}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df,\n",
    "                        is_tpl=True)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) Prefix unique IDs (prefix=70 for K replicate) ─────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 130\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) Filtering ───────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold,\n",
    "                           is_tpl=True)\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post‐filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_O_BFmean_tpl'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "print(\"SSITcellresults keys:\", SSITcellresults.keys())\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) Post‐processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl=True\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> 100nM Dex TPL for O replicate\")\n",
    "plotter.plot_time_sweep(\n",
    "    dex_conc=100,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ff14f",
   "metadata": {},
   "source": [
    "**`DUSP1 Replica P Triptolide Time-sweep - By Slide`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) pull in all slides  ──────────────────────────────────────────\n",
    "analysis_names = [\n",
    "    'DUSP1_P_BFmean_slide1_062025',\n",
    "    'DUSP1_P_BFmean_slide2_062025',\n",
    "    'DUSP1_P_BFmean_slide3_062025',\n",
    "    'DUSP1_P_BFmean_slide4_062025',\n",
    "    'DUSP1_P_BFmean_slide5_062025',\n",
    "    'DUSP1_P_BFmean_slide6_062025',\n",
    "    'DUSP1_P_BFmean_slide7_062025',\n",
    "    'DUSP1_P_BFmean_slide8_062025',\n",
    "    'DUSP1_P_BFmean_slide9_062025',\n",
    "    'DUSP1_P_BFmean_slide10_062025',\n",
    "    'DUSP1_P_BFmean_slide11_062025',\n",
    "    'DUSP1_P_BFmean_slide12_062025',\n",
    "    'DUSP1_P_BFmean_slide23_062025',\n",
    "    'DUSP1_P_BFmean_slide14_062025',\n",
    "    'DUSP1_P_BFmean_slide15_062025',\n",
    "    'DUSP1_P_BFmean_slide16_062025',         \n",
    "]\n",
    "\n",
    "# build and use the combined manager\n",
    "dm = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "\n",
    "spots_df    = dm.select_datasets(\"spotresults\",      dtype=\"dataframe\")\n",
    "clusters_df = dm.select_datasets(\"clusterresults\",   dtype=\"dataframe\")\n",
    "props_df    = dm.select_datasets(\"cell_properties\",  dtype=\"dataframe\")\n",
    "dm.close()    \n",
    "\n",
    "print(f\"Loaded and concatenated {len(analysis_names)} days:\")\n",
    "print(f\"  spots:   {spots_df.shape}\")\n",
    "print(f\"  clusters:{clusters_df.shape}\")\n",
    "print(f\"  props:   {props_df.shape}\")\n",
    "\n",
    "# ─── 2) SNRAnalysis → DUSP1Measurement ────────────────────────────────────────\n",
    "abs_threshold = 6\n",
    "mg_threshold  = 3\n",
    "\n",
    "snr = SNRAnalysis(spots_df, props_df, clusters_df,\n",
    "                  abs_threshold=abs_threshold,\n",
    "                  mg_threshold=mg_threshold)\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr.get_results()\n",
    "\n",
    "dusp = DUSP1Measurement(merged_spots_df,\n",
    "                        merged_clusters_df,\n",
    "                        merged_cellprops_df,\n",
    "                        is_tpl=True)\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold,\n",
    "                                  mg_threshold=mg_threshold)\n",
    "\n",
    "# ─── 3) Prefix unique IDs (prefix=70 for K replicate) ─────────────────────────\n",
    "max_id     = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "rep_prefix = 140\n",
    "prefix     = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "for df in (merged_spots_df, merged_clusters_df, merged_cellprops_df, cell_level_results):\n",
    "    df['unique_cell_id'] += prefix\n",
    "\n",
    "max_spot_id  = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix  = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df   ['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# ─── 4) Filtering ───────────────────────────────────────────────────────────────\n",
    "filterer = DUSP1_filtering(method=method,\n",
    "                           abs_threshold=abs_threshold,\n",
    "                           mg_threshold=mg_threshold,\n",
    "                           is_tpl=True)\n",
    "filtered_spots, filtered_clusters, filtered_cellprops, SSITcellresults, removed_spots = \\\n",
    "    filterer.apply_all(spots=merged_spots_df,\n",
    "                       clusters=merged_clusters_df,\n",
    "                       cellprops=merged_cellprops_df)\n",
    "\n",
    "# ─── 5) main display ──────────────────────────────────────────────────────────\n",
    "# build a fresh manager so the display code can find every HDF5\n",
    "dm_disp = make_combined_manager(loc, log_location, mac, analysis_names)\n",
    "display_manager = DUSP1DisplayManager(\n",
    "    dm_disp,\n",
    "    cell_level_results=SSITcellresults,\n",
    "    spots=filtered_spots,\n",
    "    clusters=filtered_clusters,\n",
    "    cellprops=filtered_cellprops,\n",
    "    removed_spots=removed_spots\n",
    ")\n",
    "display_manager.default_display(\n",
    "    per_h5=1,\n",
    "    show=False,                 # Preview in notebook\n",
    "    save=True,                 # Save PNGs\n",
    "    outdir=img_dir,\n",
    "    dpi_notebook=120,          # lighter for notebook\n",
    "    dpi_save=600,              # publication quality\n",
    "    show_removed=True,         # overlay removed spots in crops\n",
    "    removed_color=\"red\"  # distinct color for removed\n",
    ")\n",
    "dm_disp.close()\n",
    "\n",
    "# ─── 6) Save final, post‐filter results to CSV ─────────────────────────────────\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rep_string = 'DUSP1_P_BFmean_tpl'\n",
    "base = f\"{rep_string}_MG{mg_threshold}_Abs{abs_threshold}_{date_str}_{method}\"\n",
    "\n",
    "SSITcellresults  .to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "filtered_spots   .to_csv(os.path.join(output_dir, f\"{base}_FinalSpots.csv\"),      index=False)\n",
    "filtered_clusters.to_csv(os.path.join(output_dir, f\"{base}_FinalClusters.csv\"),  index=False)\n",
    "filtered_cellprops.to_csv(os.path.join(output_dir, f\"{base}_FinalCellProps.csv\"), index=False)\n",
    "removed_spots    .to_csv(os.path.join(output_dir, f\"{base}_RemovedSpots.csv\"),   index=False)\n",
    "\n",
    "print(\"Saved final results to:\", output_dir)\n",
    "\n",
    "# ─── 7) Post‐processing plots with PostProcessingPlotter ──────────────────────\n",
    "plotter = PostProcessingPlotter(\n",
    "    clusters_df=filtered_clusters,\n",
    "    cellprops_df=filtered_cellprops,\n",
    "    ssit_df=SSITcellresults,\n",
    "    is_tpl=True\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> 100nM Dex TPL for P replicate\")\n",
    "plotter.plot_time_sweep(\n",
    "    dex_conc=100,\n",
    "    save_dir=img_dir,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aaac4a",
   "metadata": {},
   "source": [
    "## GR Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.Analysis_GR import AnalysisManager, GR_Confirmation\n",
    "\n",
    "loc = None\n",
    "log_location = r'/Volumes/share/Users/Eric/GR_DUSP1_reruns'\n",
    "mac = True\n",
    "\n",
    "\n",
    "def touching_border(df, image_height, image_width):\n",
    "    \"\"\"\n",
    "    Returns a boolean Series indicating which cells are touching any image border (no margin).\n",
    "\n",
    "    Expects df to have columns:\n",
    "      'cell_bbox-0' => min_row\n",
    "      'cell_bbox-1' => min_col\n",
    "      'cell_bbox-2' => max_row (exclusive)\n",
    "      'cell_bbox-3' => max_col (exclusive).\n",
    "    \"\"\"\n",
    "    min_row = df['cell_bbox-0']\n",
    "    min_col = df['cell_bbox-1']\n",
    "    max_row = df['cell_bbox-2']\n",
    "    max_col = df['cell_bbox-3']\n",
    "\n",
    "    return (\n",
    "        (min_row == 0)\n",
    "        | (min_col == 0)\n",
    "        | (max_row == image_height)\n",
    "        | (max_col == image_width)\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_GR(name, rep_prefix: int):\n",
    "    # 1) set up\n",
    "    am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "    am.select_analysis(name)\n",
    "    GR = GR_Confirmation(am)\n",
    "    GR.get_data()\n",
    "\n",
    "    # 2) get image dims for border-touching\n",
    "    image0 = GR.corrected_images[0]\n",
    "    h, w = image0.shape[-2:]\n",
    "\n",
    "    # 3) make a simple local index for each cell\n",
    "    local_ids = np.arange(len(GR.cellprops))\n",
    "\n",
    "    # 4) compute how many digits that index has\n",
    "    max_local = local_ids.max()\n",
    "    num_digits = len(str(max_local))\n",
    "\n",
    "    # 5) build the rep-specific offset\n",
    "    offset = rep_prefix * (10 ** num_digits)\n",
    "\n",
    "    # 6) assign your global unique_cell_id\n",
    "    GR.cellprops['unique_cell_id'] = local_ids + offset\n",
    "\n",
    "    # 7) remove border-touching cells as before\n",
    "    GR.cellprops['touching_border'] = touching_border(\n",
    "        GR.cellprops, image_height=h, image_width=w\n",
    "    )\n",
    "    GR.cellprops = GR.cellprops[~GR.cellprops['touching_border']].copy()  # <-- keep a copy\n",
    "\n",
    "    am.close()\n",
    "\n",
    "    # 8) return your SSIT-style frame\n",
    "    return measure_GR(GR.cellprops)\n",
    "\n",
    "\n",
    "def measure_GR(cellprops) -> pd.DataFrame:\n",
    "    results = pd.DataFrame(\n",
    "        columns=[\n",
    "            'unique_cell_id', 'nuc_area', 'nucGRint', 'cytoGRint', 'time',\n",
    "            'dex_conc', 'replica', 'fov', 'nas_location', 'h5_idx',\n",
    "            'background_p1', 'background_p3', 'background_p5', 'background_p10',\n",
    "            'background_otsu_thresh', 'background_mean_otsu',\n",
    "            'background_median_otsu', 'background_std_otsu'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Sort cellprops by unique_cell_id\n",
    "    props = cellprops.sort_values(by='unique_cell_id').copy()\n",
    "\n",
    "    # unique cell id\n",
    "    cell_ids = props['unique_cell_id']\n",
    "    # nuc area\n",
    "    nuc_area = props['nuc_area']\n",
    "    # avg int nuc\n",
    "    nucGRint = props['nuc_intensity_mean-0']\n",
    "    # avg int pseudocyto mask\n",
    "    cytoGRint = props['cyto_intensity_mean-0']\n",
    "    # time (experiment)\n",
    "    time = props['time']\n",
    "    # Dex conc\n",
    "    dex_conc = props['Dex_Conc']\n",
    "    # Replica\n",
    "    replica = props['replica']\n",
    "    # identifiers\n",
    "    fov = props['fov']\n",
    "    nas_location = props['NAS_location']\n",
    "    h5_idx = props['h5_idx']\n",
    "\n",
    "    results['unique_cell_id'] = cell_ids.values\n",
    "    results['nuc_area'] = nuc_area.values\n",
    "    results['nucGRint'] = nucGRint.values\n",
    "    results['cytoGRint'] = cytoGRint.values\n",
    "    results['time'] = time.values\n",
    "    results['dex_conc'] = dex_conc.values\n",
    "    results['replica'] = replica.values\n",
    "    results['fov'] = fov.values\n",
    "    results['nas_location'] = nas_location.values\n",
    "    results['h5_idx'] = h5_idx.values\n",
    "\n",
    "    # background\n",
    "    results['background_p1'] = props['background_p1'].values\n",
    "    results['background_p3'] = props['background_p3'].values\n",
    "    results['background_p5'] = props['background_p5'].values\n",
    "    results['background_p10'] = props['background_p10'].values\n",
    "    results['background_otsu_thresh'] = props['background_otsu_thresh'].values\n",
    "    results['background_mean_otsu'] = props['background_mean_otsu'].values\n",
    "    results['background_median_otsu'] = props['background_median_otsu'].values\n",
    "    results['background_std_otsu'] = props['background_std_otsu'].values\n",
    "\n",
    "    # Final QC: keep only rows that have BOTH nucGRint and cytoGRint\n",
    "    results = results.dropna(subset=['nucGRint', 'cytoGRint']).copy()\n",
    "\n",
    "    # keep unique_cell_id as both column and index\n",
    "    results['unique_cell_id'] = results['unique_cell_id'].astype('int64')\n",
    "    results.set_index('unique_cell_id', inplace=True, drop=False)  # <-- preserve column\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_RepA = filter_GR('GR_wIC_sig50_A_082125', rep_prefix=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ef82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_RepB = filter_GR('GR_wIC_sig50_B_082125', rep_prefix=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45352598",
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_RepC = filter_GR('GR_wIC_sig50_C_082125', rep_prefix=170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ff527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate without reindexing and without rebuilding unique_cell_id\n",
    "GR_ALL = pd.concat([GR_RepA, GR_RepB, GR_RepC], ignore_index=False, sort=False)\n",
    "GR_ALL.set_index('unique_cell_id', drop=False, inplace=True)\n",
    "\n",
    "# Save\n",
    "output_dir = save_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "base = f\"GR_mergedReps_noBorderCells_preGate_preNorm_{date_str}\"\n",
    "GR_ALL.to_csv(os.path.join(output_dir, f\"{base}_SSITcellresults.csv\"), index=False)\n",
    "print(\"Saved final GR results to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca6395",
   "metadata": {},
   "source": [
    "## Visualization and Correlation Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99039968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GR Results\n",
    "GR_ALL = pd.read_csv(r'/Volumes/share/Users/Eric/GR_DUSP1_AllData/FinalAnalysis_GR_DUSP1/GR_mergedReps_noBorderCells_preGate_preNorm_Aug25_SSITcellresults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb53555",
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_ALL.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Filter out timepoints with only one replica\n",
    "rep_counts = (\n",
    "    GR_ALL\n",
    "    .groupby(['dex_conc', 'time'])['replica']\n",
    "    .nunique()\n",
    "    .reset_index(name='n_replicas')\n",
    ")\n",
    "valid = rep_counts[rep_counts['n_replicas'] > 1][['dex_conc', 'time']]\n",
    "GR_valid = pd.merge(GR_ALL, valid, on=['dex_conc', 'time'], how='inner')\n",
    "\n",
    "# 2) Define your background metrics\n",
    "background_metrics = [\n",
    "    'background_p1',\n",
    "    'background_p3',\n",
    "    'background_p5',\n",
    "    'background_p10',\n",
    "    'background_otsu_thresh',\n",
    "    'background_mean_otsu',\n",
    "    'background_median_otsu',\n",
    "    'background_std_otsu'\n",
    "]\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "for bg in background_metrics:\n",
    "    # determine global x‐limits for this background metric\n",
    "    x_min, x_max = GR_valid[bg].min(), GR_valid[bg].max()\n",
    "    # determine global y‐limits for nucGRint and cytoGRint\n",
    "    y_min_n, y_max_n = GR_valid['nucGRint'].min(), GR_valid['nucGRint'].max()\n",
    "    y_min_c, y_max_c = GR_valid['cytoGRint'].min(), GR_valid['cytoGRint'].max()\n",
    "\n",
    "    # plot nucGRint\n",
    "    g = sns.FacetGrid(\n",
    "        GR_valid,\n",
    "        row='dex_conc', col='time',\n",
    "        hue='replica',\n",
    "        margin_titles=True,\n",
    "        sharex=True, sharey=True,  # share scales across all panels\n",
    "        height=3, aspect=1.2\n",
    "    )\n",
    "    g.map_dataframe(sns.scatterplot, x=bg, y='nucGRint', alpha=0.6)\n",
    "    g.set_axis_labels(bg, 'nucGRint')\n",
    "    g.set(xlim=(x_min, x_max), ylim=(y_min_n, y_max_n))\n",
    "    g.add_legend(title=\"Replica\")\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    g.figure.suptitle(f\"nucGRint vs {bg} (Faceted by Dex × Time)\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # plot cytoGRint\n",
    "    f = sns.FacetGrid(\n",
    "        GR_valid,\n",
    "        row='dex_conc', col='time',\n",
    "        hue='replica',\n",
    "        margin_titles=True,\n",
    "        sharex=True, sharey=True,\n",
    "        height=3, aspect=1.2\n",
    "    )\n",
    "    f.map_dataframe(sns.scatterplot, x=bg, y='cytoGRint', alpha=0.6)\n",
    "    f.set_axis_labels(bg, 'cytoGRint')\n",
    "    f.set(xlim=(x_min, x_max), ylim=(y_min_c, y_max_c))\n",
    "    f.add_legend(title=\"Replica\")\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    f.figure.suptitle(f\"cytoGRint vs {bg} (Faceted by Dex × Time)\", y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89a0d7",
   "metadata": {},
   "source": [
    "## Background Normalization\n",
    "\n",
    "\n",
    "Three intensity categories\n",
    "Ibg = Background intensity (backgound metric columns)\n",
    "Icyt = Pseudo-Cyto intensity (raw value in dataframe 'cytoGRint')\n",
    "Inuc = Nuclear intensity (raw value in dataframe 'nucGRint')\n",
    "\n",
    "CDF plots of intensities could identify bg, cyto, and nuclear intensity ranges. (Ibg is lowst intensity values, Ibg < Icyt ≤ Inuc ) \n",
    "\n",
    "Otsu threshold at 0min should identify actual Ibg value/treshold (However, cells with low nuc GR could be an issuse)... all other time/conc could have a mix of cyto and background.\n",
    "\n",
    "Collected per fov background metrics:\n",
    "    'background_p1', - 1st percentile intensity\n",
    "    'background_p3', - 3rd percentile\n",
    "    'background_p5', - 5th percentile\n",
    "    'background_p10', - 10th percentile\n",
    "    'background_otsu_thresh',\n",
    "    'background_mean_otsu',\n",
    "    'background_median_otsu',\n",
    "    'background_std_otsu'\n",
    "\n",
    "\n",
    "Mulplicitive (Microscope Related - Laser power, camera temp, etc)\n",
    " IcytNorm = Icty / Ibg\n",
    " InucNorm = Inuc / Igb\n",
    " IbgNorm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad76429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from time import sleep\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Today's date\n",
    "today = datetime.date.today()\n",
    "# Format date as 'Jun03' (for example)\n",
    "date_str = today.strftime(\"%b%d\")\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)\n",
    "\n",
    "matplotlib_logger = logging.getLogger('matplotlib')\n",
    "matplotlib_logger.setLevel(logging.WARNING)\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "print(src_path)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.Analysis_DUSP1 import DUSP1AnalysisManager, SNRAnalysis, DUSP1Measurement, DUSP1_filtering, DUSP1DisplayManager, PostProcessingPlotter\n",
    "from src.Analysis_GR import AnalysisManager, GR_Confirmation, GR_DisplayBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) load (as you already do) ---\n",
    "GR_ALL = pd.read_csv(\n",
    "    '/Volumes/share/Users/Eric/GR_DUSP1_AllData/GR_FinalAnalysis_073025/'\n",
    "    'GR_ALL_Jul30_gated_preNorm_SSITcellresults.csv'\n",
    ").copy()\n",
    "\n",
    "# --- 1) keep original measured background name short for convenience ---\n",
    "#     (do NOT overwrite your measured background)\n",
    "GR_ALL = GR_ALL.rename(columns={'background_p10': 'bg_p10'})\n",
    "\n",
    "# Remove timepoints with only one replica\n",
    "rep_counts = (\n",
    "    GR_ALL\n",
    "    .groupby(['dex_conc', 'time'])['replica']\n",
    "    .nunique()\n",
    "    .reset_index(name='n_replicas')\n",
    ")\n",
    "valid = rep_counts[rep_counts['n_replicas'] > 1][['dex_conc', 'time']]\n",
    "GR_ALL = pd.merge(GR_ALL, valid, on=['dex_conc', 'time'], how='inner')\n",
    "\n",
    "# --- 2) per-replica baseline at time=0 (robust) ---\n",
    "bg0_by_rep = (\n",
    "    GR_ALL.loc[GR_ALL['time'] == 0]\n",
    "          .groupby('replica')['bg_p10']\n",
    "          .median()\n",
    "          .rename('bg0_replica')            # replica’s baseline at 0 min\n",
    ")\n",
    "\n",
    "# attach to all rows\n",
    "GR_ALL = GR_ALL.merge(bg0_by_rep, on='replica', how='left')\n",
    "\n",
    "# sanity check\n",
    "missing = GR_ALL['bg0_replica'].isna().sum()\n",
    "if missing > 0:\n",
    "    print(f\"WARNING: {missing} rows lack a per-replica baseline (no time==0 for some replica).\")\n",
    "\n",
    "# --- 3) global reference (median across replica baselines) ---\n",
    "bg_ref = float(bg0_by_rep.median())\n",
    "GR_ALL['bg_ref'] = bg_ref  # store once for transparency\n",
    "\n",
    "# --- 4) convenience variables for coming methods ---\n",
    "# ratio of current-FOV background to that replica’s baseline\n",
    "GR_ALL['bg_ratio'] = GR_ALL['bg_p10'] / GR_ALL['bg0_replica']\n",
    "\n",
    "# delta from that replica’s baseline\n",
    "GR_ALL['bg_delta'] = GR_ALL['bg_p10'] - GR_ALL['bg0_replica']\n",
    "\n",
    "# delta relative to global reference (useful for “scale-to-reference” versions)\n",
    "GR_ALL['bg_delta_ref'] = GR_ALL['bg_p10'] - GR_ALL['bg_ref']\n",
    "GR_ALL['bg_ratio_ref'] = GR_ALL['bg_p10'] / GR_ALL['bg_ref']\n",
    "\n",
    "print(\n",
    "    \"Anchors ready:\",\n",
    "    {\n",
    "      \"bg0_by_rep\": bg0_by_rep.to_dict(),\n",
    "      \"bg_ref\": bg_ref\n",
    "    }\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG: choose background\n",
    "# =========================\n",
    "# Switch this to 'bg_p10' or any of your background metrics to compare:\n",
    "bg_col = 'bg_p10'    # <- your percentile background of choice\n",
    "eps = 1e-9                    # avoid divide-by-zero\n",
    "\n",
    "# Helper: ensure required columns exist\n",
    "required_cols = {'nucGRint','cytoGRint','replica','dex_conc','time','bg_ref','bg0_replica', bg_col}\n",
    "missing = required_cols - set(GR_ALL.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"GR_ALL missing columns: {missing}\")\n",
    "\n",
    "# ===========================================\n",
    "# 1) Multiplicative & Additive (all variants)\n",
    "# ===========================================\n",
    "norm_defs = {}\n",
    "\n",
    "# Multiplicative\n",
    "norm_defs['mul_unit'] = {\n",
    "    'nuc': GR_ALL['nucGRint']  / (GR_ALL[bg_col] + eps),\n",
    "    'cyto': GR_ALL['cytoGRint'] / (GR_ALL[bg_col] + eps),\n",
    "    'desc': f\"Multiplicative (bg→1) using {bg_col}\"\n",
    "}\n",
    "norm_defs['mul_ref'] = {\n",
    "    'nuc': GR_ALL['nucGRint']  * (GR_ALL['bg_ref'] / (GR_ALL[bg_col] + eps)),\n",
    "    'cyto': GR_ALL['cytoGRint'] * (GR_ALL['bg_ref'] / (GR_ALL[bg_col] + eps)),\n",
    "    'desc': f\"Multiplicative (bg→bg_ref) using {bg_col}\"\n",
    "}\n",
    "norm_defs['mul_rep0'] = {\n",
    "    'nuc': GR_ALL['nucGRint']  * (GR_ALL['bg0_replica'] / (GR_ALL[bg_col] + eps)),\n",
    "    'cyto': GR_ALL['cytoGRint'] * (GR_ALL['bg0_replica'] / (GR_ALL[bg_col] + eps)),\n",
    "    'desc': f\"Multiplicative (bg→replica 0-min baseline) using {bg_col}\"\n",
    "}\n",
    "\n",
    "# Additive\n",
    "norm_defs['add_unit'] = {\n",
    "    'nuc': GR_ALL['nucGRint']  - GR_ALL[bg_col],\n",
    "    'cyto': GR_ALL['cytoGRint'] - GR_ALL[bg_col],\n",
    "    'desc': f\"Additive (bg→0) using {bg_col}\"\n",
    "}\n",
    "norm_defs['add_ref'] = {\n",
    "    'nuc': GR_ALL['nucGRint']  + (GR_ALL['bg_ref'] - GR_ALL[bg_col]),\n",
    "    'cyto': GR_ALL['cytoGRint'] + (GR_ALL['bg_ref'] - GR_ALL[bg_col]),\n",
    "    'desc': f\"Additive (align to bg_ref) using {bg_col}\"\n",
    "}\n",
    "norm_defs['add_rep0'] = {\n",
    "    'nuc': GR_ALL['nucGRint']  + (GR_ALL['bg0_replica'] - GR_ALL[bg_col]),\n",
    "    'cyto': GR_ALL['cytoGRint'] + (GR_ALL['bg0_replica'] - GR_ALL[bg_col]),\n",
    "    'desc': f\"Additive (align to replica 0-min baseline) using {bg_col}\"\n",
    "}\n",
    "\n",
    "# Write columns\n",
    "for key, v in norm_defs.items():\n",
    "    GR_ALL[f'nuc_{key}']  = v['nuc']\n",
    "    GR_ALL[f'cyto_{key}'] = v['cyto']\n",
    "\n",
    "# ===========================================\n",
    "# 2) Linear (complex) method per (dex,time)\n",
    "#    I' = I - β1 * (bg - bg_ref)\n",
    "# ===========================================\n",
    "# Fit separate slopes for nuc and cyto for each (dex_conc, time)\n",
    "coefs = []\n",
    "for (dex, t), sub in GR_ALL.groupby(['dex_conc','time'], sort=False):\n",
    "    row = {'dex_conc': dex, 'time': t, 'nuc_b1': np.nan, 'cyto_b1': np.nan}\n",
    "    # Nucleus\n",
    "    sub_n = sub[['nucGRint', bg_col]].dropna()\n",
    "    if len(sub_n) >= 5 and sub_n[bg_col].var() > 0:\n",
    "        m = LinearRegression().fit(sub_n[[bg_col]], sub_n['nucGRint'])\n",
    "        row['nuc_b1'] = float(m.coef_[0])\n",
    "    # Cytoplasm\n",
    "    sub_c = sub[['cytoGRint', bg_col]].dropna()\n",
    "    if len(sub_c) >= 5 and sub_c[bg_col].var() > 0:\n",
    "        m = LinearRegression().fit(sub_c[[bg_col]], sub_c['cytoGRint'])\n",
    "        row['cyto_b1'] = float(m.coef_[0])\n",
    "    coefs.append(row)\n",
    "\n",
    "coefs_df = pd.DataFrame(coefs)\n",
    "\n",
    "# Merge back\n",
    "GR_ALL = GR_ALL.merge(coefs_df, on=['dex_conc','time'], how='left')\n",
    "\n",
    "# Apply linear adjustment to reference background\n",
    "GR_ALL['nuc_lin_ref']  = GR_ALL['nucGRint']  - GR_ALL['nuc_b1'].fillna(0.0)  * (GR_ALL[bg_col] - GR_ALL['bg_ref'])\n",
    "GR_ALL['cyto_lin_ref'] = GR_ALL['cytoGRint'] - GR_ALL['cyto_b1'].fillna(0.0) * (GR_ALL[bg_col] - GR_ALL['bg_ref'])\n",
    "\n",
    "# Apply linear adjustment to replica 0-min baseline\n",
    "GR_ALL['nuc_lin_rep0']  = GR_ALL['nucGRint'] - GR_ALL['nuc_b1'].fillna(0.0)  * (GR_ALL[bg_col] - GR_ALL['bg0_replica'])\n",
    "GR_ALL['cyto_lin_rep0'] = GR_ALL['cytoGRint'] - GR_ALL['cyto_b1'].fillna(0.0) * (GR_ALL[bg_col] - GR_ALL['bg0_replica'])\n",
    "\n",
    "# ================\n",
    "# 3) Plot helpers\n",
    "# ================\n",
    "def facet_hist_by_replica(df, x_col, *,\n",
    "                          title=None, hue='replica',\n",
    "                          row='dex_conc', col='time',\n",
    "                          row_order=None, col_order=None,\n",
    "                          height=3.0, aspect=1.1,\n",
    "                          bins=60, stat='density',\n",
    "                          xlim=None, outlined=True):\n",
    "    \"\"\"\n",
    "    Faceted histograms with replica as hue. If xlim is provided, use it\n",
    "    as both binrange and axis limits so no facet is empty.\n",
    "    \"\"\"\n",
    "    # decide bin range\n",
    "    if xlim is not None:\n",
    "        binrange = tuple(xlim)\n",
    "    else:\n",
    "        lo = float(np.nanquantile(df[x_col], 0.001))\n",
    "        hi = float(np.nanquantile(df[x_col], 0.999))\n",
    "        binrange = (lo, hi)\n",
    "\n",
    "    # style\n",
    "    kw = dict(bins=bins, binrange=binrange,\n",
    "              multiple='layer', stat=stat,\n",
    "              common_bins=True, common_norm=False)\n",
    "    if outlined:\n",
    "        kw.update(element='step', fill=False)\n",
    "    else:\n",
    "        kw.update(element=None, fill=True)\n",
    "\n",
    "    g = sns.displot(\n",
    "        data=df, x=x_col, hue=hue, kind='hist',\n",
    "        row=row, col=col,\n",
    "        row_order=row_order, col_order=col_order,\n",
    "        height=height, aspect=aspect,\n",
    "        **kw\n",
    "    )\n",
    "    if xlim is not None:\n",
    "        for ax in g.axes.flat:\n",
    "            ax.set_xlim(xlim)\n",
    "\n",
    "    g.set_axis_labels(x_col, stat.title())\n",
    "    if title:\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        g.fig.suptitle(title)\n",
    "    return g\n",
    "\n",
    "def robust_xlim(df, cols, q=(0.001, 0.999), pad=0.02):\n",
    "    vals = pd.concat([df[c].dropna() for c in cols], ignore_index=True)\n",
    "    lo = float(np.nanquantile(vals, q[0]))\n",
    "    hi = float(np.nanquantile(vals, q[1]))\n",
    "    span = hi - lo\n",
    "    return (lo - pad*span, hi + pad*span)\n",
    "\n",
    "def facet_violin_by_replica(df, y_col, *,\n",
    "                            title=None,\n",
    "                            x='replica',\n",
    "                            row='dex_conc', col='time',\n",
    "                            row_order=None, col_order=None,\n",
    "                            height=3.0, aspect=1.1,\n",
    "                            cut=0, inner='quart', scale='width'):\n",
    "    \"\"\"\n",
    "    Violin plot of y_col by replica, faceted by (row=row, col=col).\n",
    "    \"\"\"\n",
    "    g = sns.catplot(\n",
    "        data=df,\n",
    "        x=x, y=y_col,\n",
    "        kind='violin',\n",
    "        row=row, col=col,\n",
    "        row_order=row_order, col_order=col_order,\n",
    "        height=height, aspect=aspect,\n",
    "        cut=cut, inner=inner, scale=scale\n",
    "    )\n",
    "    g.set_axis_labels(x, y_col)\n",
    "    if title:\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        g.fig.suptitle(title)\n",
    "    return g\n",
    "\n",
    "# ======================================\n",
    "# 4) Generate plots for ALL comparisons\n",
    "# ======================================\n",
    "# Methods computed earlier\n",
    "methods = ['mul_unit','mul_ref','mul_rep0','add_unit','add_ref','add_rep0','lin_ref']\n",
    "nuc_cols  = [f'nuc_{m}'  for m in methods if f'nuc_{m}'  in GR_ALL.columns]\n",
    "cyto_cols = [f'cyto_{m}' for m in methods if f'cyto_{m}' in GR_ALL.columns]\n",
    "\n",
    "NUC_XLIM  = robust_xlim(GR_ALL, nuc_cols)   # use DF so we match the plotted subset\n",
    "CYTO_XLIM = robust_xlim(GR_ALL, cyto_cols)\n",
    "print(\"NUC_XLIM:\", NUC_XLIM, \"CYTO_XLIM:\", CYTO_XLIM)\n",
    "\n",
    "# ---------- plotting loop ----------\n",
    "PLOT_DF   = GR_ALL  # use the filtered data with A/B/C present\n",
    "dex_order = sorted(PLOT_DF['dex_conc'].unique())\n",
    "time_order= sorted(PLOT_DF['time'].unique())\n",
    "\n",
    "regions = [\n",
    "    ('nucGRint',  'nuc',  'Nucleus'),\n",
    "    ('cytoGRint', 'cyto', 'Cytoplasm')\n",
    "]\n",
    "\n",
    "for raw_col, prefix, region_name in regions:\n",
    "    xlim_fixed = NUC_XLIM if prefix == 'nuc' else CYTO_XLIM\n",
    "\n",
    "    for m in methods:\n",
    "        new_col = f'{prefix}_{m}'\n",
    "        if new_col not in PLOT_DF.columns:\n",
    "            print(f\"SKIP (not found): {new_col}\")\n",
    "            continue\n",
    "\n",
    "        # Faceted histograms (replica hue) with fixed limits\n",
    "        g1 = facet_hist_by_replica(\n",
    "            PLOT_DF,\n",
    "            x_col=new_col,\n",
    "            title=f'{region_name}: {m} | Histogram by Replica',\n",
    "            row='dex_conc', col='time',\n",
    "            row_order=dex_order, col_order=time_order,\n",
    "            height=2.6, aspect=1.0,\n",
    "            bins=60, stat='density',\n",
    "            xlim=xlim_fixed,       # <-- critical; drives binrange + axes\n",
    "            outlined=True\n",
    "        )\n",
    "\n",
    "        # Faceted violins (same subset)\n",
    "        g2 = sns.catplot(\n",
    "            data=PLOT_DF,\n",
    "            x='replica', y=new_col,\n",
    "            kind='violin',\n",
    "            row='dex_conc', col='time',\n",
    "            row_order=dex_order, col_order=time_order,\n",
    "            height=2.6, aspect=1.0,\n",
    "            cut=0, inner='quart', scale='width'\n",
    "        )\n",
    "        g2.set_axis_labels('replica', new_col)\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        g2.fig.suptitle(f'{region_name}: {m} | Distribution by Replica')\n",
    "\n",
    "# ==========================\n",
    "# 5) (Optional) Save output\n",
    "# ==========================\n",
    "out_csv = '/Volumes/share/Users/Eric/GR_DUSP1_AllData/FinalAnalysis_GR_DUSP1/GR_ALL_p10_bg_norms.csv'\n",
    "GR_ALL.to_csv(out_csv, index=False)\n",
    "print(f\"Saved with all normalization columns → {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 0) load (as you already do) ---\n",
    "# GR_ALL = pd.read_csv(\n",
    "#     '/Volumes/share/Users/Eric/GR_DUSP1_AllData/GR_FinalAnalysis_073025/'\n",
    "#     'GR_ALL_Jul30_gated_preNorm_SSITcellresults.csv'\n",
    "# ).copy()\n",
    "\n",
    "# # --- 1) keep original measured background name short for convenience ---\n",
    "# #     (do NOT overwrite your measured background)\n",
    "# GR_ALL = GR_ALL.rename(columns={'background_mean_otsu': 'bg_mean_otsu'})\n",
    "\n",
    "# # Remove timepoints with only one replica\n",
    "# rep_counts = (\n",
    "#     GR_ALL\n",
    "#     .groupby(['dex_conc', 'time'])['replica']\n",
    "#     .nunique()\n",
    "#     .reset_index(name='n_replicas')\n",
    "# )\n",
    "# valid = rep_counts[rep_counts['n_replicas'] > 1][['dex_conc', 'time']]\n",
    "# GR_ALL = pd.merge(GR_ALL, valid, on=['dex_conc', 'time'], how='inner')\n",
    "\n",
    "# # --- 2) per-replica baseline at time=0 (robust) ---\n",
    "# bg0_by_rep = (\n",
    "#     GR_ALL.loc[GR_ALL['time'] == 0]\n",
    "#           .groupby('replica')['bg_mean_otsu']\n",
    "#           .median()\n",
    "#           .rename('bg0_replica')            # replica’s baseline at 0 min\n",
    "# )\n",
    "\n",
    "# # attach to all rows\n",
    "# GR_ALL = GR_ALL.merge(bg0_by_rep, on='replica', how='left')\n",
    "\n",
    "# # sanity check\n",
    "# missing = GR_ALL['bg0_replica'].isna().sum()\n",
    "# if missing > 0:\n",
    "#     print(f\"WARNING: {missing} rows lack a per-replica baseline (no time==0 for some replica).\")\n",
    "\n",
    "# # --- 3) global reference (median across replica baselines) ---\n",
    "# bg_ref = float(bg0_by_rep.median())\n",
    "# GR_ALL['bg_ref'] = bg_ref  # store once for transparency\n",
    "\n",
    "# # --- 4) convenience variables for coming methods ---\n",
    "# # ratio of current-FOV background to that replica’s baseline\n",
    "# GR_ALL['bg_ratio'] = GR_ALL['bg_mean_otsu'] / GR_ALL['bg0_replica']\n",
    "\n",
    "# # delta from that replica’s baseline\n",
    "# GR_ALL['bg_delta'] = GR_ALL['bg_mean_otsu'] - GR_ALL['bg0_replica']\n",
    "\n",
    "# # delta relative to global reference (useful for “scale-to-reference” versions)\n",
    "# GR_ALL['bg_delta_ref'] = GR_ALL['bg_mean_otsu'] - GR_ALL['bg_ref']\n",
    "# GR_ALL['bg_ratio_ref'] = GR_ALL['bg_mean_otsu'] / GR_ALL['bg_ref']\n",
    "\n",
    "# print(\n",
    "#     \"Anchors ready:\",\n",
    "#     {\n",
    "#       \"bg0_by_rep\": bg0_by_rep.to_dict(),\n",
    "#       \"bg_ref\": bg_ref\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# # =========================\n",
    "# # CONFIG: choose background\n",
    "# # =========================\n",
    "# # Switch this to 'bg_mean_otsu' or any of your background metrics to compare:\n",
    "# bg_col = 'bg_mean_otsu'    # <- your percentile background of choice\n",
    "# eps = 1e-9                    # avoid divide-by-zero\n",
    "\n",
    "# # Helper: ensure required columns exist\n",
    "# required_cols = {'nucGRint','cytoGRint','replica','dex_conc','time','bg_ref','bg0_replica', bg_col}\n",
    "# missing = required_cols - set(GR_ALL.columns)\n",
    "# if missing:\n",
    "#     raise ValueError(f\"GR_ALL missing columns: {missing}\")\n",
    "\n",
    "# # ===========================================\n",
    "# # 1) Multiplicative & Additive (all variants)\n",
    "# # ===========================================\n",
    "# norm_defs = {}\n",
    "\n",
    "# # Multiplicative\n",
    "# norm_defs['mul_unit'] = {\n",
    "#     'nuc': GR_ALL['nucGRint']  / (GR_ALL[bg_col] + eps),\n",
    "#     'cyto': GR_ALL['cytoGRint'] / (GR_ALL[bg_col] + eps),\n",
    "#     'desc': f\"Multiplicative (bg→1) using {bg_col}\"\n",
    "# }\n",
    "# norm_defs['mul_ref'] = {\n",
    "#     'nuc': GR_ALL['nucGRint']  * (GR_ALL['bg_ref'] / (GR_ALL[bg_col] + eps)),\n",
    "#     'cyto': GR_ALL['cytoGRint'] * (GR_ALL['bg_ref'] / (GR_ALL[bg_col] + eps)),\n",
    "#     'desc': f\"Multiplicative (bg→bg_ref) using {bg_col}\"\n",
    "# }\n",
    "# norm_defs['mul_rep0'] = {\n",
    "#     'nuc': GR_ALL['nucGRint']  * (GR_ALL['bg0_replica'] / (GR_ALL[bg_col] + eps)),\n",
    "#     'cyto': GR_ALL['cytoGRint'] * (GR_ALL['bg0_replica'] / (GR_ALL[bg_col] + eps)),\n",
    "#     'desc': f\"Multiplicative (bg→replica 0-min baseline) using {bg_col}\"\n",
    "# }\n",
    "\n",
    "# # Additive\n",
    "# norm_defs['add_unit'] = {\n",
    "#     'nuc': GR_ALL['nucGRint']  - GR_ALL[bg_col],\n",
    "#     'cyto': GR_ALL['cytoGRint'] - GR_ALL[bg_col],\n",
    "#     'desc': f\"Additive (bg→0) using {bg_col}\"\n",
    "# }\n",
    "# norm_defs['add_ref'] = {\n",
    "#     'nuc': GR_ALL['nucGRint']  + (GR_ALL['bg_ref'] - GR_ALL[bg_col]),\n",
    "#     'cyto': GR_ALL['cytoGRint'] + (GR_ALL['bg_ref'] - GR_ALL[bg_col]),\n",
    "#     'desc': f\"Additive (align to bg_ref) using {bg_col}\"\n",
    "# }\n",
    "# norm_defs['add_rep0'] = {\n",
    "#     'nuc': GR_ALL['nucGRint']  + (GR_ALL['bg0_replica'] - GR_ALL[bg_col]),\n",
    "#     'cyto': GR_ALL['cytoGRint'] + (GR_ALL['bg0_replica'] - GR_ALL[bg_col]),\n",
    "#     'desc': f\"Additive (align to replica 0-min baseline) using {bg_col}\"\n",
    "# }\n",
    "\n",
    "# # Write columns\n",
    "# for key, v in norm_defs.items():\n",
    "#     GR_ALL[f'nuc_{key}']  = v['nuc']\n",
    "#     GR_ALL[f'cyto_{key}'] = v['cyto']\n",
    "\n",
    "# # ===========================================\n",
    "# # 2) Linear (complex) method per (dex,time)\n",
    "# #    I' = I - β1 * (bg - bg_ref)\n",
    "# # ===========================================\n",
    "# # Fit separate slopes for nuc and cyto for each (dex_conc, time)\n",
    "# coefs = []\n",
    "# for (dex, t), sub in GR_ALL.groupby(['dex_conc','time'], sort=False):\n",
    "#     row = {'dex_conc': dex, 'time': t, 'nuc_b1': np.nan, 'cyto_b1': np.nan}\n",
    "#     # Nucleus\n",
    "#     sub_n = sub[['nucGRint', bg_col]].dropna()\n",
    "#     if len(sub_n) >= 5 and sub_n[bg_col].var() > 0:\n",
    "#         m = LinearRegression().fit(sub_n[[bg_col]], sub_n['nucGRint'])\n",
    "#         row['nuc_b1'] = float(m.coef_[0])\n",
    "#     # Cytoplasm\n",
    "#     sub_c = sub[['cytoGRint', bg_col]].dropna()\n",
    "#     if len(sub_c) >= 5 and sub_c[bg_col].var() > 0:\n",
    "#         m = LinearRegression().fit(sub_c[[bg_col]], sub_c['cytoGRint'])\n",
    "#         row['cyto_b1'] = float(m.coef_[0])\n",
    "#     coefs.append(row)\n",
    "\n",
    "# coefs_df = pd.DataFrame(coefs)\n",
    "\n",
    "# # Merge back\n",
    "# GR_ALL = GR_ALL.merge(coefs_df, on=['dex_conc','time'], how='left')\n",
    "\n",
    "# # Apply linear adjustment to reference background\n",
    "# GR_ALL['nuc_lin_ref']  = GR_ALL['nucGRint']  - GR_ALL['nuc_b1'].fillna(0.0)  * (GR_ALL[bg_col] - GR_ALL['bg_ref'])\n",
    "# GR_ALL['cyto_lin_ref'] = GR_ALL['cytoGRint'] - GR_ALL['cyto_b1'].fillna(0.0) * (GR_ALL[bg_col] - GR_ALL['bg_ref'])\n",
    "\n",
    "# # Apply linear adjustment to replica 0-min baseline\n",
    "# GR_ALL['nuc_lin_rep0']  = GR_ALL['nucGRint'] - GR_ALL['nuc_b1'].fillna(0.0)  * (GR_ALL[bg_col] - GR_ALL['bg0_replica'])\n",
    "# GR_ALL['cyto_lin_rep0'] = GR_ALL['cytoGRint'] - GR_ALL['cyto_b1'].fillna(0.0) * (GR_ALL[bg_col] - GR_ALL['bg0_replica'])\n",
    "\n",
    "# # ================\n",
    "# # 3) Plot helpers\n",
    "# # ================\n",
    "# def facet_hist_by_replica(df, x_col, *,\n",
    "#                           title=None, hue='replica',\n",
    "#                           row='dex_conc', col='time',\n",
    "#                           row_order=None, col_order=None,\n",
    "#                           height=3.0, aspect=1.1,\n",
    "#                           bins=60, stat='density',\n",
    "#                           xlim=None, outlined=True):\n",
    "#     \"\"\"\n",
    "#     Faceted histograms with replica as hue. If xlim is provided, use it\n",
    "#     as both binrange and axis limits so no facet is empty.\n",
    "#     \"\"\"\n",
    "#     # decide bin range\n",
    "#     if xlim is not None:\n",
    "#         binrange = tuple(xlim)\n",
    "#     else:\n",
    "#         lo = float(np.nanquantile(df[x_col], 0.001))\n",
    "#         hi = float(np.nanquantile(df[x_col], 0.999))\n",
    "#         binrange = (lo, hi)\n",
    "\n",
    "#     # style\n",
    "#     kw = dict(bins=bins, binrange=binrange,\n",
    "#               multiple='layer', stat=stat,\n",
    "#               common_bins=True, common_norm=False)\n",
    "#     if outlined:\n",
    "#         kw.update(element='step', fill=False)\n",
    "#     else:\n",
    "#         kw.update(element=None, fill=True)\n",
    "\n",
    "#     g = sns.displot(\n",
    "#         data=df, x=x_col, hue=hue, kind='hist',\n",
    "#         row=row, col=col,\n",
    "#         row_order=row_order, col_order=col_order,\n",
    "#         height=height, aspect=aspect,\n",
    "#         **kw\n",
    "#     )\n",
    "#     if xlim is not None:\n",
    "#         for ax in g.axes.flat:\n",
    "#             ax.set_xlim(xlim)\n",
    "\n",
    "#     g.set_axis_labels(x_col, stat.title())\n",
    "#     if title:\n",
    "#         plt.subplots_adjust(top=0.9)\n",
    "#         g.fig.suptitle(title)\n",
    "#     return g\n",
    "\n",
    "# def robust_xlim(df, cols, q=(0.001, 0.999), pad=0.02):\n",
    "#     vals = pd.concat([df[c].dropna() for c in cols], ignore_index=True)\n",
    "#     lo = float(np.nanquantile(vals, q[0]))\n",
    "#     hi = float(np.nanquantile(vals, q[1]))\n",
    "#     span = hi - lo\n",
    "#     return (lo - pad*span, hi + pad*span)\n",
    "\n",
    "# def facet_violin_by_replica(df, y_col, *,\n",
    "#                             title=None,\n",
    "#                             x='replica',\n",
    "#                             row='dex_conc', col='time',\n",
    "#                             row_order=None, col_order=None,\n",
    "#                             height=3.0, aspect=1.1,\n",
    "#                             cut=0, inner='quart', scale='width'):\n",
    "#     \"\"\"\n",
    "#     Violin plot of y_col by replica, faceted by (row=row, col=col).\n",
    "#     \"\"\"\n",
    "#     g = sns.catplot(\n",
    "#         data=df,\n",
    "#         x=x, y=y_col,\n",
    "#         kind='violin',\n",
    "#         row=row, col=col,\n",
    "#         row_order=row_order, col_order=col_order,\n",
    "#         height=height, aspect=aspect,\n",
    "#         cut=cut, inner=inner, scale=scale\n",
    "#     )\n",
    "#     g.set_axis_labels(x, y_col)\n",
    "#     if title:\n",
    "#         plt.subplots_adjust(top=0.9)\n",
    "#         g.fig.suptitle(title)\n",
    "#     return g\n",
    "\n",
    "# # ======================================\n",
    "# # 4) Generate plots for ALL comparisons\n",
    "# # ======================================\n",
    "# # Methods computed earlier\n",
    "# methods = ['mul_unit','mul_ref','mul_rep0','add_unit','add_ref','add_rep0','lin_ref']\n",
    "# nuc_cols  = [f'nuc_{m}'  for m in methods if f'nuc_{m}'  in GR_ALL.columns]\n",
    "# cyto_cols = [f'cyto_{m}' for m in methods if f'cyto_{m}' in GR_ALL.columns]\n",
    "\n",
    "# NUC_XLIM  = robust_xlim(GR_ALL, nuc_cols)   # use DF so we match the plotted subset\n",
    "# CYTO_XLIM = robust_xlim(GR_ALL, cyto_cols)\n",
    "# print(\"NUC_XLIM:\", NUC_XLIM, \"CYTO_XLIM:\", CYTO_XLIM)\n",
    "\n",
    "# # ---------- plotting loop ----------\n",
    "# PLOT_DF   = GR_ALL  # use the filtered data with A/B/C present\n",
    "# dex_order = sorted(PLOT_DF['dex_conc'].unique())\n",
    "# time_order= sorted(PLOT_DF['time'].unique())\n",
    "\n",
    "# regions = [\n",
    "#     ('nucGRint',  'nuc',  'Nucleus'),\n",
    "#     ('cytoGRint', 'cyto', 'Cytoplasm')\n",
    "# ]\n",
    "\n",
    "# for raw_col, prefix, region_name in regions:\n",
    "#     xlim_fixed = NUC_XLIM if prefix == 'nuc' else CYTO_XLIM\n",
    "\n",
    "#     for m in methods:\n",
    "#         new_col = f'{prefix}_{m}'\n",
    "#         if new_col not in PLOT_DF.columns:\n",
    "#             print(f\"SKIP (not found): {new_col}\")\n",
    "#             continue\n",
    "\n",
    "#         # Faceted histograms (replica hue) with fixed limits\n",
    "#         g1 = facet_hist_by_replica(\n",
    "#             PLOT_DF,\n",
    "#             x_col=new_col,\n",
    "#             title=f'{region_name}: {m} | Histogram by Replica',\n",
    "#             row='dex_conc', col='time',\n",
    "#             row_order=dex_order, col_order=time_order,\n",
    "#             height=2.6, aspect=1.0,\n",
    "#             bins=60, stat='density',\n",
    "#             xlim=xlim_fixed,       # <-- critical; drives binrange + axes\n",
    "#             outlined=True\n",
    "#         )\n",
    "\n",
    "#         # Faceted violins (same subset)\n",
    "#         g2 = sns.catplot(\n",
    "#             data=PLOT_DF,\n",
    "#             x='replica', y=new_col,\n",
    "#             kind='violin',\n",
    "#             row='dex_conc', col='time',\n",
    "#             row_order=dex_order, col_order=time_order,\n",
    "#             height=2.6, aspect=1.0,\n",
    "#             cut=0, inner='quart', scale='width'\n",
    "#         )\n",
    "#         g2.set_axis_labels('replica', new_col)\n",
    "#         plt.subplots_adjust(top=0.9)\n",
    "#         g2.fig.suptitle(f'{region_name}: {m} | Distribution by Replica')\n",
    "\n",
    "# # # ==========================\n",
    "# # # 5) (Optional) Save output\n",
    "# # # ==========================\n",
    "# # out_csv = '/Volumes/share/Users/Eric/GR_DUSP1_AllData/GR_FinalAnalysis_073025/GR_ALL_percentile_bg_norms.csv'\n",
    "# # GR_ALL.to_csv(out_csv, index=False)\n",
    "# # print(f\"Saved with all normalization columns → {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d654575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
