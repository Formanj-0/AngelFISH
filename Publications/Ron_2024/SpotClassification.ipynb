{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8882958d",
   "metadata": {},
   "source": [
    "## DUSP1 smiFISH Classification Workflow\n",
    "\n",
    "This notebook processes experimental data from BigFish and CellProperties CSV files to classify DUSP1 smiFISH spots. Below is an outline of the workflow:\n",
    "\n",
    "### Input:\n",
    "- BigFish CSV files\n",
    "- CellProperties CSV files\n",
    "\n",
    "### Workflow Steps:\n",
    "1. **Merge Experimental Data**  \n",
    "    Combine data from the input CSV files for unified analysis.\n",
    "\n",
    "2. **Signal-to-Noise Ratio (SNR) Analysis**  \n",
    "    Perform SNR analysis to classify and filter spots based on signal quality.\n",
    "\n",
    "3. **Measurement Analysis**  \n",
    "    Conduct measurement analysis to extract relevant features from the data.\n",
    "\n",
    "4. **Data Merging**  \n",
    "    Merge all processed data into a single dataset for further analysis.\n",
    "\n",
    "5. **Create Training Spot Crops (11px x 11px)**  \n",
    "    Generate training data for machine learning models:\n",
    "    - Select 1000 spots from each `h5_idx`, ensuring variation across cells and fields of view (FOVs).\n",
    "    - Use the DUSP1 Display Manager to safely load spot channel images.\n",
    "    - Perform max projection of images along the z-axis to create 2D (x, y) representations.\n",
    "    - Extract 11px x 11px crops centered on each spot.\n",
    "    - Rescale intensity for visualization purposes.\n",
    "    - Display 100 sample spot crops for quality inspection before saving all crops to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# Today's date\n",
    "today = datetime.date.today()\n",
    "# Format date as 'Mar21' (for example)\n",
    "date_str = today.strftime(\"%b%d\")\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)\n",
    "\n",
    "matplotlib_logger = logging.getLogger('matplotlib')\n",
    "matplotlib_logger.setLevel(logging.WARNING)\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "print(src_path)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.Analysis_DUSP1_v2 import DUSP1AnalysisManager, SNRAnalysis, DUSP1Measurement, DUSP1DisplayManager, SpotCropSampler, DUSP1_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33388bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = None\n",
    "log_location = r'/Volumes/share/Users/Eric/GR_DUSP1_reruns'\n",
    "save_dir = r'/Volumes/share/Users/Eric/DUSP1_SpotCropsV2'\n",
    "df_outut = r'/Volumes/share/Users/Eric/DUSP1_SpotCropsV2/SNR_dataframes'\n",
    "if not os.path.exists(df_outut):\n",
    "    os.makedirs(df_outut)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define Thresholds\n",
    "abs_threshold = 4\n",
    "mg_threshold = 3\n",
    "\n",
    "# crop size\n",
    "pad = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a88c7",
   "metadata": {},
   "source": [
    "    # DUSP1 Replica D 3hr 100nM time-sweep R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a84491",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_D_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 10\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_D_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=1,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=2,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = \"DUSP1_D_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory where you saved the summary files\n",
    "# save_dir = \"/Volumes/share/Users/Eric/DUSP1_SpotCrops\"\n",
    "\n",
    "# # 1) Load the data\n",
    "# crops = np.load(os.path.join(save_dir, f\"{prefix}_all_crops.npy\"))  # shape: (N, 7, 7)\n",
    "# meta  = pd.read_csv(os.path.join(save_dir, f\"{prefix}_all_crop_metadata.csv\"))\n",
    "\n",
    "# # 2) Choose a few random examples (up to 5)\n",
    "# num_examples = min(5, len(crops))\n",
    "# indices = np.random.choice(len(crops), size=num_examples, replace=False)\n",
    "\n",
    "# # 3) Display them\n",
    "# fig, axes = plt.subplots(1, num_examples, figsize=(num_examples * 3, 3))\n",
    "# for ax, idx in zip(axes, indices):\n",
    "#     patch = crops[idx]\n",
    "#     info  = meta.iloc[idx]\n",
    "    \n",
    "#     ax.imshow(patch, cmap='gray')\n",
    "#     ax.set_title(\n",
    "#         f\"Cell {info.unique_cell_id}\\n\"\n",
    "#         f\"Spot {info.unique_spot_id}\\n\"\n",
    "#         f\"MG_SNR={info.MG_SNR:.1f}, SNR={info.snr:.1f}\\n\"\n",
    "#         f\"MG_pass={info.MG_pass}\"\n",
    "#     )\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8e5d6",
   "metadata": {},
   "source": [
    "    # DUSP1 Replica E 3hr 100nM time-sweep R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91985ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_E_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 20\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_E_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = \"DUSP1_E_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb75311",
   "metadata": {},
   "source": [
    "    # DUSP1 Replica F 3hr 100nM time-sweep R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_F_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 30\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_F_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_F_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234fd1e",
   "metadata": {},
   "source": [
    "    # DUSP1 Replica M 3hr 100nM time-sweep Partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a6907",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_M_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 40\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_M_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_M_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafe0e9",
   "metadata": {},
   "source": [
    "    # DUSP1 Replica N 3hr 100nM time-sweep Partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a614db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_N_Final2')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 50\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_N_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_N_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb73587",
   "metadata": {},
   "source": [
    "    DUSP1 75min Concentration-sweep Replica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_CS_R1_Final3')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 60\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_G_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_G_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e6253",
   "metadata": {},
   "source": [
    "    DUSP1 75min Concentration-sweep Replica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_H_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 70\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_H_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_H_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee3a73",
   "metadata": {},
   "source": [
    "    DUSP1 75min Concentration-sweep Replica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_I_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 80\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_I_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_I_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efb890",
   "metadata": {},
   "source": [
    "    DUSP1 0.3, 1, 10nM Dex 3hr Time-sweep Replica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dced7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_TCS_R1_Final3')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 90\n",
    "prefix = rep_prefix ** num_digits  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_J_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_J_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7afb5a3",
   "metadata": {},
   "source": [
    "    DUSP1 0.3, 1, 10nM Dex 3hr Time-sweep Replica 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_K_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 11\n",
    "prefix = rep_prefix * (10** num_digits)  # e.g., if max_id = 30245 → prefix = 1100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_k_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_K_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67524882",
   "metadata": {},
   "source": [
    "    DUSP1 0.3, 1, 10nM Dex 3hr Time-sweep Replica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_L_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 12\n",
    "prefix = rep_prefix * (10** num_digits)  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_L_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_L_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562b05b",
   "metadata": {},
   "source": [
    "    DUSP1 100nM Dex & 5µM TPL Time-sweep Replica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3be592",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_O_Final')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 13\n",
    "prefix = rep_prefix * (10** num_digits)  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_O_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_O_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800091ac",
   "metadata": {},
   "source": [
    "    DUSP1 100nM Dex & 5µM TPL Time-sweep Replica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ed789",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = DUSP1AnalysisManager(location=loc, log_location=log_location, mac=True) \n",
    "am.select_analysis('DUSP1_P_Final2')\n",
    "\n",
    "spots_df = am.select_datasets(\"spotresults\", dtype=\"dataframe\")\n",
    "clusters_df = am.select_datasets(\"clusterresults\", dtype=\"dataframe\")\n",
    "props_df = am.select_datasets(\"cell_properties\", dtype=\"dataframe\")\n",
    "\n",
    "snr_df = SNRAnalysis(spots_df, props_df, clusters_df, abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "merged_spots_df, merged_clusters_df, merged_cellprops_df = snr_df.get_results()\n",
    "\n",
    "# Create an instance of the DUSP1Measurement class.\n",
    "dusp = DUSP1Measurement(merged_spots_df, merged_clusters_df, merged_cellprops_df)\n",
    "\n",
    "# Process the data with a chosen threshold method\n",
    "cell_level_results = dusp.measure(abs_threshold=abs_threshold, mg_threshold=mg_threshold)\n",
    "\n",
    "# Add replica level unique IDs for 'unique_cell_id', 'unique_spot_id', and 'unique_cluster_id'\n",
    "# Get number of digits in the max unique_cell_id\n",
    "max_id = merged_cellprops_df['unique_cell_id'].max()\n",
    "num_digits = len(str(max_id))\n",
    "\n",
    "# Calculate multiplier to add a '1' followed by the right number of zeroes - prefix is specific for each experiment (e.g., repD:1, repE:2, etc.)\n",
    "rep_prefix = 14\n",
    "prefix = rep_prefix * (10** num_digits)  # e.g., if max_id = 30245 → prefix = 100000\n",
    "\n",
    "# Apply prefix to all related DataFrames\n",
    "merged_spots_df['unique_cell_id'] += prefix\n",
    "merged_clusters_df['unique_cell_id'] += prefix\n",
    "merged_cellprops_df['unique_cell_id'] += prefix\n",
    "cell_level_results['unique_cell_id'] += prefix\n",
    "\n",
    "# Repeat for unique_spot_id and unique_cluster_id\n",
    "max_spot_id = merged_spots_df['unique_spot_id'].max()\n",
    "spot_prefix = rep_prefix ** len(str(max_spot_id))\n",
    "merged_spots_df['unique_spot_id'] += spot_prefix\n",
    "\n",
    "max_cluster_id = merged_clusters_df['unique_cluster_id'].max()\n",
    "cluster_prefix = rep_prefix ** len(str(max_cluster_id))\n",
    "merged_clusters_df['unique_cluster_id'] += cluster_prefix\n",
    "\n",
    "# Save all results to CSV\n",
    "rep_string = 'DUSP1_P_Final'\n",
    "output_dir = df_outut\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cell_level_results.to_csv(os.path.join(output_dir, f\"{rep_string}_cell_level_results_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_spots_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_spots_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_clusters_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_clusters_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "merged_cellprops_df.to_csv(os.path.join(output_dir, f\"{rep_string}_merged_cellprops_df_MG{mg_threshold}_Abs{abs_threshold}_{date_str}.csv\"), index=False)\n",
    "\n",
    "# instantiate sampler\n",
    "sampler = SpotCropSampler(\n",
    "    spots_df=merged_spots_df,\n",
    "    clusters_df=merged_clusters_df,\n",
    "    cellprops_df=merged_cellprops_df,\n",
    "    mount_prefix=\"/Volumes/share\"\n",
    ")\n",
    "\n",
    "# # — TEST RUN: only 3 displays, no files written —\n",
    "# crops, meta = sampler.run(\n",
    "#     save_dir=save_dir,\n",
    "#     display=3,\n",
    "#     save_individual=False,\n",
    "#     save_summary=False,\n",
    "#     pad=pad,\n",
    "#     cells_per_quad=1,\n",
    "#     spots_per_cell=20,\n",
    "#     spotChannel=0\n",
    "# )\n",
    "\n",
    "# choose a prefix for this experiment\n",
    "prefix = f\"DUSP1_P_Final_MG3_Abs4\"\n",
    "\n",
    "# full run, no per-spot files, but summary files with prefix\n",
    "crops, meta = sampler.run(\n",
    "    save_dir=save_dir,\n",
    "    display=0,\n",
    "    save_individual=False,\n",
    "    save_summary=True,\n",
    "    file_prefix=prefix,\n",
    "    pad=pad,\n",
    "    cells_per_quad=1,\n",
    "    spots_per_cell=20,\n",
    "    spotChannel=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# — 1) Load your data —\n",
    "spots_df = pd.read_csv(\n",
    "    \"/Users/ericron/Desktop/AngelFISH/Publications/Ron_2024/Classification/\"\n",
    "    \"DUSP1_D_Final_merged_spots_df_MG3_Abs4_Apr29.csv\"\n",
    ")\n",
    "clusters_df = pd.read_csv(\n",
    "    \"/Users/ericron/Desktop/AngelFISH/Publications/Ron_2024/Classification/\"\n",
    "    \"DUSP1_D_Final_merged_clusters_df_MG3_Abs4_Apr29.csv\"\n",
    ")\n",
    "cellprops_df = pd.read_csv(\n",
    "    \"/Users/ericron/Desktop/AngelFISH/Publications/Ron_2024/Classification/\"\n",
    "    \"DUSP1_D_Final_merged_cellprops_df_MG3_Abs4_Apr29.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info on spots that do not pass MG_pass and mg_lt_snr =1\n",
    "spots_df.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed996b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get every unique_cell_id as strings\n",
    "all_ids = cellprops_df['unique_cell_id'].astype(str).unique().tolist()\n",
    "\n",
    "# 2) Find those that start with \"10\" and end with \"619\" or \"1269\"\n",
    "matching = [\n",
    "    cid for cid in all_ids\n",
    "    if cid.startswith('10') and (cid.endswith('619') or cid.endswith('1269'))\n",
    "]\n",
    "\n",
    "print(\"Found these matching IDs:\", matching)\n",
    "\n",
    "# 3) If 'matching' is empty, broaden the search to any containing \"619\" or \"1269\":\n",
    "if not matching:\n",
    "    broader = [cid for cid in all_ids if '619' in cid or '1269' in cid]\n",
    "    print(\"Broader matches (contain '619' or '1269'):\", broader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509a497",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfacd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) Imports & Paths ─────────────────────────────────────────────────────────\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "\n",
    "# make sure your Analysis_DUSP1_v2.py sits under a 'src/' folder in this ROOT\n",
    "ROOT = '/Users/ericron/Desktop/AngelFISH'  \n",
    "sys.path.append(ROOT)\n",
    "\n",
    "# adjust these to wherever your files really live:\n",
    "spots_path     = os.path.join(ROOT, 'Publications', 'Ron_2024', 'Classification',\n",
    "                              'DUSP1_D_Final_merged_spots_df_MG3_Abs4_Apr29.csv')\n",
    "clusters_path  = os.path.join(ROOT, 'Publications', 'Ron_2024', 'Classification',\n",
    "                              'DUSP1_D_Final_merged_clusters_df_MG3_Abs4_Apr29.csv')\n",
    "cellprops_path = os.path.join(ROOT, 'Publications', 'Ron_2024', 'Classification',\n",
    "                              'DUSP1_D_Final_merged_cellprops_df_MG3_Abs4_Apr29.csv')\n",
    "model_path     = '/Volumes/share/Users/Eric/DUSP1_SpotCropsV2/global_random_forest.pkl'\n",
    "\n",
    "# ─── 2) Load DataFrames ────────────────────────────────────────────────────────\n",
    "merged_spots_df     = pd.read_csv(spots_path)\n",
    "merged_clusters_df  = pd.read_csv(clusters_path)\n",
    "merged_cellprops_df = pd.read_csv(cellprops_path)\n",
    "\n",
    "# ─── 3) Import your classes and load RF model ─────────────────────────────────\n",
    "from src.Analysis_DUSP1_v2 import DUSP1Measurement, DUSP1_filtering\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    clf = joblib.load(model_path)\n",
    "    all_feats = list(clf.feature_names_in_)\n",
    "    # drop the three spot-crop features\n",
    "    features = [f for f in all_feats \n",
    "                if f not in ('num_spots_in_crop', 'num_TS_in_crop', 'num_foci_in_crop')]\n",
    "    print(f\"Loaded RF model ({len(all_feats)} total features).\")\n",
    "    print(f\"Using {len(features)} features (crop counts dropped):\\n\", features)\n",
    "else:\n",
    "    clf = None\n",
    "    features = []\n",
    "    print(\"RF model not found; skipping RF-based steps.\")\n",
    "\n",
    "# ─── 4) Run measurement pipeline ───────────────────────────────────────────────\n",
    "dusp = DUSP1Measurement(\n",
    "    spots=merged_spots_df,\n",
    "    clusters=merged_clusters_df,\n",
    "    cellprops=merged_cellprops_df,\n",
    "    model=model_path\n",
    ")\n",
    "\n",
    "# set your thresholds here:\n",
    "ABS_THRESHOLD = 4.0\n",
    "MG_THRESHOLD  = 3.0\n",
    "\n",
    "results_df = dusp.measure(\n",
    "    abs_threshold=ABS_THRESHOLD,\n",
    "    mg_threshold=MG_THRESHOLD\n",
    ")\n",
    "\n",
    "# ─── 5) RF filter spots ────────────────────────────────────────────────────────\n",
    "rf_filter = DUSP1_filtering(method='all')\n",
    "filtered_spots, removed_spots = rf_filter.apply_spots(\n",
    "    spots=dusp.spots,\n",
    "    results=results_df,\n",
    "    method='all'\n",
    ")\n",
    "\n",
    "# ─── 6) Show exactly which columns fed the RF ─────────────────────────────────\n",
    "if clf:\n",
    "    print(\"Features used by RF:\", list(clf.feature_names_in_))\n",
    "\n",
    "# 7) Prepare for PCA & prediction by auto‐mapping model features to df columns\n",
    "all_feats = list(clf.feature_names_in_) if clf else []\n",
    "\n",
    "def normalize(name):\n",
    "    \"\"\"Lowercase, turn non‐alphanumerics to '_', strip trailing '_<digits>', collapse underscores.\"\"\"\n",
    "    s = re.sub(r'[^0-9a-z]+', '_', name.lower())\n",
    "    s = re.sub(r'_[0-9]+$', '', s)\n",
    "    return s.strip('_')\n",
    "\n",
    "# build map: normalized column name -> actual column name\n",
    "norm2col = { normalize(c): c for c in merged_spots_df.columns }\n",
    "\n",
    "mapped = {}\n",
    "missing = []\n",
    "for feat in all_feats:\n",
    "    n = normalize(feat)\n",
    "    if n in norm2col:\n",
    "        mapped[feat] = norm2col[n]\n",
    "    else:\n",
    "        missing.append(feat)\n",
    "\n",
    "if missing:\n",
    "    print(\"Warning: RF expects features not in DataFrame, filling zeros for:\", missing)\n",
    "\n",
    "# now build X with exactly all_feats (missing → 0)\n",
    "X = pd.DataFrame(index=merged_spots_df.index)\n",
    "for feat in all_feats:\n",
    "    if feat in mapped:\n",
    "        X[feat] = merged_spots_df[mapped[feat]]\n",
    "    else:\n",
    "        X[feat] = 0\n",
    "\n",
    "# ensure numeric\n",
    "X = X.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "# annotate original spots with RF output\n",
    "if clf and not X.empty:\n",
    "    merged_spots_df['rf_probability'] = clf.predict_proba(X)[:, 1]\n",
    "    merged_spots_df['rf_prediction']  = (merged_spots_df['rf_probability'] >= 0.5).astype(int)\n",
    "else:\n",
    "    merged_spots_df['rf_prediction'] = 1\n",
    "\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# 8) PCA projection\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pc = pca.fit_transform(X)\n",
    "merged_spots_df['PC1'], merged_spots_df['PC2'] = pc[:,0], pc[:,1]\n",
    "\n",
    "# 9) Plot kept vs removed\n",
    "plt.figure(figsize=(6,6))\n",
    "mask = merged_spots_df['rf_prediction'] == 1\n",
    "plt.scatter(merged_spots_df.loc[mask,'PC1'],   merged_spots_df.loc[mask,'PC2'],\n",
    "            c='green', label='Kept', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(merged_spots_df.loc[~mask,'PC1'], merged_spots_df.loc[~mask,'PC2'],\n",
    "            c='red',   label='Removed', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.legend(); plt.title('PCA of Spots: Kept vs Removed')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ─── 1) Point this at your merged spots CSV ─────────────────────────────────\n",
    "spots_path = '/Users/ericron/Desktop/AngelFISH/Publications/Ron_2024/Classification/DUSP1_D_Final_merged_spots_df_MG3_Abs4_Apr29.csv'  \n",
    "\n",
    "# Load the DataFrame\n",
    "merged_spots_df = pd.read_csv(spots_path)\n",
    "\n",
    "# 2) select all numeric columns and fill NaNs\n",
    "numeric_cols = merged_spots_df.select_dtypes(include=[np.number]).columns\n",
    "X = merged_spots_df[numeric_cols].fillna(0)\n",
    "drop = ['x_px','y_px','z_px']\n",
    "X = merged_spots_df[numeric_cols.drop(drop)].fillna(0)\n",
    "\n",
    "# 3) standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4) PCA on the scaled data\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(X_scaled)\n",
    "merged_spots_df['PC1'] = coords[:,0]\n",
    "merged_spots_df['PC2'] = coords[:,1]\n",
    "\n",
    "# 5) optionally look at how much variance PC1/PC2 capture:\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# 6) plot\n",
    "plt.figure(figsize=(7,7))\n",
    "mask = merged_spots_df['MG_pass']\n",
    "plt.scatter(merged_spots_df.loc[mask,  'PC1'],\n",
    "            merged_spots_df.loc[mask,  'PC2'],\n",
    "            c='green', label='MG_pass', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(merged_spots_df.loc[~mask, 'PC1'],\n",
    "            merged_spots_df.loc[~mask, 'PC2'],\n",
    "            c='red',   label='MG_fail', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.title('PCA (standardized features): MG_pass vs MG_fail')\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
