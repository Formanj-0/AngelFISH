{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUSP1 Confirmation Notebook\n",
    "The purpose of this notebook is to:\n",
    "1. Confirm successful segmentation.\n",
    "2. Confirm successful BigFISH spot and cluster detection.\n",
    "3. Refine spots and clusters through additional filtering (SNR) for gating and final dataframe preparation:  \n",
    "    a. Test predefined SNR thresholds.  \n",
    "    b. Test weighted SNR tresholding    \n",
    "    c. Filter `df_spots` with snr threshold if needed.  \n",
    "    d. Create final dataframes ('df_cellspots').    \n",
    "    e. Save the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)\n",
    "\n",
    "matplotlib_logger = logging.getLogger('matplotlib')\n",
    "matplotlib_logger.setLevel(logging.WARNING)\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "print(src_path)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.Analysis import AnalysisManager, Analysis, SpotDetection_SNRConfirmation, Spot_Cluster_Analysis_WeightedSNR, GR_Confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the log file to search for analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = None \n",
    "log_location = r'/Volumes/share/Users/Eric/AngelFISH_data'  #  r'/Volumes/share/Users/Jack/All_Analysis'\n",
    "am1 = AnalysisManager(location=loc, log_location=log_location, mac=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all analysis done \n",
    "all_analysis_names = am1.list_analysis_names()\n",
    "print(\"All discovered analyses:\", all_analysis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUSP1 100nM Dex 3hr Time-sweep Replica 1\n",
    "am1.select_analysis('DUSP1_D_Jan2125')\n",
    "am1.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis/confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class\n",
    "SD1 = Spot_Cluster_Analysis_WeightedSNR(am1)\n",
    "# Load the data\n",
    "SD1.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Segmentation, BF_spotdetection, SNR thresholding (basic and weighted), Summary Stats and plots\n",
    "SD1.display(newFOV=True, newCell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this multiple times to see a new randomly selected cell\n",
    "SD.display(newFOV=True, newCell=True) # num_fovs_to_display=2,num_cells_to_display=2, num_spots_to_display=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spots = pd.DataFrame(SD.spots)\n",
    "df_clusters = pd.DataFrame(SD.clusters)\n",
    "df_cellspots = pd.DataFrame(SD.cellspots)\n",
    "df_cellprops = pd.DataFrame(SD.cellprops)\n",
    "\n",
    "# Print columns for each dataframe\n",
    "print(\"df_spots columns:\")\n",
    "print(\", \".join(df_spots.columns))\n",
    "\n",
    "print(\"\\ndf_clusters columns:\")\n",
    "print(\", \".join(df_clusters.columns))\n",
    "\n",
    "print(\"\\ndf_cellspots columns:\")\n",
    "print(\", \".join(df_cellspots.columns))\n",
    "\n",
    "print(\"\\ndf_cellprops columns:\")\n",
    "print(\", \".join(df_cellprops.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of timepoints in the dataset\n",
    "print(df_spots['time'].unique())\n",
    "print(df_spots['h5_idx'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for all times\n",
    "stats = df_spots.groupby('time')['snr'].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Dynamic SNR thresholding based on mean Â± 2*std\n",
    "thresholds = {}\n",
    "for time, row in stats.iterrows():\n",
    "    thresholds[time] = (row['mean'] - 2 * row['std'], row['mean'] + 2 * row['std'])\n",
    "\n",
    "# Apply dynamic thresholding\n",
    "df_spots['threshold_pass'] = df_spots.apply(\n",
    "    lambda row: thresholds[row['time']][0] <= row['snr'] <= thresholds[row['time']][1],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filtered DataFrame\n",
    "df_spots_filtered = df_spots[df_spots['threshold_pass']]\n",
    "\n",
    "# Categorize noise levels based on the paper's description\n",
    "def categorize_noise(snr):\n",
    "    if snr < 2:\n",
    "        return 'very_high_noise'\n",
    "    if 2 <= snr < 5:\n",
    "        return 'snr:2-5'\n",
    "    elif 8 <= snr <= 26:\n",
    "        return 'snr:8-26'\n",
    "    elif snr > 26:\n",
    "        return 'snr>26'\n",
    "    else:\n",
    "        return 'did this work?'\n",
    "\n",
    "df_spots['noise_level'] = df_spots['snr'].apply(categorize_noise)\n",
    "\n",
    "# Plot histograms of SNR for each time\n",
    "for time, group in df_spots.groupby('time'):\n",
    "    mean_snr = group['snr'].mean()\n",
    "    median_snr = group['snr'].median()\n",
    "    std_snr = group['snr'].std()\n",
    "    \n",
    "    plt.hist(group['snr'], bins=50, alpha=0.7, label=f'time: {time}')\n",
    "    plt.axvline(mean_snr, color='b', linestyle='dashed', linewidth=1, label=f'Mean: {mean_snr:.2f}')\n",
    "    plt.axvline(median_snr, color='g', linestyle='dashed', linewidth=1, label=f'Median: {median_snr:.2f}')\n",
    "    plt.title(f'SNR Histogram for time {time}min')\n",
    "    plt.xlabel('SNR')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Scatter Plot of Intensity vs SNR\n",
    "plt.scatter(df_spots['signal'], df_spots['snr'], s=1, alpha=0.7)\n",
    "plt.xlabel('Intensity')\n",
    "plt.ylabel('SNR')\n",
    "plt.title('Intensity vs SNR (All Spots)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# Noise Level Distribution\n",
    "noise_level_counts = df_spots['noise_level'].value_counts()\n",
    "plt.bar(noise_level_counts.index, noise_level_counts.values, alpha=0.7)\n",
    "plt.title('Noise Level Distribution')\n",
    "plt.xlabel('Noise Level')\n",
    "plt.ylabel('Spot Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spots_filtered = df_spots[(df_spots['cell_label'] > 0)] # & (df_spots['cluster_index'] > -1)]\n",
    "print(len(df_spots_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame(SD.clusters)\n",
    "df_clusters.columns\n",
    "df_clusters_filtered = df_clusters[(df_clusters['is_nuc'] > 0)]\n",
    "print(len(df_clusters_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters['is_nuc'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.cellprops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(SD.cellprops) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cellspots = pd.DataFrame(SD.cellspots)\n",
    "df_cellspots.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_TS = df_cellspots[(df_cellspots['nb_transcription_site'] > 0)]\n",
    "print(len(num_TS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cells that have props but arent in the cell spots\n",
    "allcells = SD.cellprops\n",
    "cells_wSpots = SD.cellspots\n",
    "# Find cells that are in allcells but not in cells_wSpots\n",
    "merged = allcells.merge(cells_wSpots, how='left', left_on=['nuc_label', 'fov', 'NAS_location'], right_on=['cell_id', 'fov', 'NAS_location'], indicator=True)\n",
    "print(merged.shape)\n",
    "same_entries = merged[merged['_merge'] == 'both'].drop(columns=['cell_id', '_merge'])\n",
    "different_entries = merged[merged['_merge'] == 'left_only'].drop(columns=['cell_id', '_merge'])\n",
    "\n",
    "print(\"Same entries:\")\n",
    "print(same_entries.shape)\n",
    "print(\"\\nDifferent entries:\")\n",
    "print(different_entries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import dask.array as da\n",
    "print(f'There are {allcells.shape[0]} cells in this data set')\n",
    "print(f'There are {cells_wSpots.shape[0]} cells with spots')\n",
    "\n",
    "# how many are have bounded boxes touching the border\n",
    "print(f'{different_entries['touching_border'].sum()} cells are touching the border and are not counted')\n",
    "\n",
    "# Select a random row from the different_entries dataframe\n",
    "for _ in range(2):\n",
    "    random_row = different_entries[~different_entries['touching_border']].sample(n=1).iloc[0]\n",
    "\n",
    "    # Read the h5 file\n",
    "    h5_file = random_row['NAS_location']\n",
    "    h5_file = os.path.join(r'\\\\munsky-nas.engr.colostate.edu\\share', h5_file) # TODO this will need to be updated so you dont have to find it to get it to work\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # Grab the mask and raw_image\n",
    "        masks = da.from_array(f['/masks'])\n",
    "        raw_images = da.from_array(f['/raw_images'])\n",
    "\n",
    "        # Extract the bounding box coordinates\n",
    "        bbox = [random_row['cell_bbox-0'], random_row['cell_bbox-1'], random_row['cell_bbox-2'], random_row['cell_bbox-3']]\n",
    "\n",
    "        img = raw_images[random_row['fov'], random_row['timepoint_x']].squeeze()\n",
    "        for c in range(img.shape[0]):\n",
    "            # Display the raw image with the selected cell highlighted\n",
    "            t = np.max(img[c, :, :,:], axis=0)\n",
    "            t.compute()\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "            ax.imshow(t, cmap='gray')\n",
    "            rect = plt.Rectangle((bbox[1], bbox[0]), bbox[3] - bbox[1], bbox[2] - bbox[0], edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of spots\n",
    "keys_to_plot = ['signal', 'snr']\n",
    "for k in SD.spots.keys():\n",
    "    if k in keys_to_plot:\n",
    "        # Plot histogram for 'area'\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(SD.spots[k], bins=200, density=True)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Histogram of {k}')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.cellspots.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot counts as a function of time and dex\n",
    "keys_to_plot = ['nb_rna', 'nb_rna_in_nuc']\n",
    "\n",
    "\n",
    "tp_set = sorted(set(SD.cellspots['time']))\n",
    "dex_set = sorted(set(SD.cellspots['Dex_Conc']))\n",
    "for k in keys_to_plot:\n",
    "    fig, axs = plt.subplots(len(tp_set), len(dex_set), figsize=(15, 15))\n",
    "    fig.suptitle(f'{k} as a function of time and dex', fontsize=16)\n",
    "    for i_d, d in enumerate(dex_set):\n",
    "        data = SD.cellspots[SD.cellspots['Dex_Conc'] == d]\n",
    "        for i_t, t in enumerate(tp_set):\n",
    "            temp = data[data['time'] == t]\n",
    "            mean_val = temp[k].mean()\n",
    "            std_val = temp[k].std()\n",
    "            if d == 0 and t == 0:\n",
    "                for ax in axs[i_t, :]:\n",
    "                    ax.hist(temp[k], bins=200, density=True)\n",
    "                    ax.axvline(mean_val, color='r', linestyle='solid', linewidth=2)\n",
    "                    ax.axvline(mean_val + std_val, color='g', linestyle='dashed', linewidth=1)\n",
    "                    ax.axvline(mean_val - std_val, color='g', linestyle='dashed', linewidth=1)\n",
    "                    ax.set_xlim([0, SD.cellspots[k].max()])\n",
    "                    ax.grid(True)  # Turn on grid lines\n",
    "                    if i_t != len(tp_set) - 1:\n",
    "                        axs[i_t, i_d].set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "                axs[i_t, 0].set_ylabel(f'Time: {t}')\n",
    "            else:\n",
    "                axs[i_t, i_d].hist(temp[k], bins=200, density=True)\n",
    "                axs[i_t, i_d].axvline(mean_val, color='r', linestyle='solid', linewidth=2)\n",
    "                axs[i_t, i_d].axvline(mean_val + std_val, color='g', linestyle='dashed', linewidth=1)\n",
    "                axs[i_t, i_d].axvline(mean_val - std_val, color='g', linestyle='dashed', linewidth=1)\n",
    "                axs[i_t, i_d].set_xlim([0, SD.cellspots[k].max()])\n",
    "                axs[i_t, i_d].grid(True)  # Turn on grid lines\n",
    "                if i_t != len(tp_set) - 1:\n",
    "                    axs[i_t, i_d].set_xticks([])\n",
    "                axs[i_t, i_d].set_yticks([])\n",
    "                axs[i_t, 0].set_ylabel(f'Time: {t}')\n",
    "                axs[0, i_d].set_title(f'Dex: {d}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Approach\n",
    "\n",
    "In this approach, every spot is retained but given a weight reflecting its estimated reliability. Higher weights indicate higher trust in that spot.\n",
    "\n",
    "### Example Weight Scheme\n",
    "Based on the paperâs reported errors:\n",
    "\n",
    "- **SNR < 2**: Discard (weight = 0.0)\n",
    "- **2 â¤ SNR < 5**: High noise, ~24% error â reliability ~ 76% â weight = 0.76\n",
    "- **5 â¤ SNR < 8**: Not explicitly given, but presumably better than 24% error â weight = 0.85\n",
    "- **8 â¤ SNR â¤ 26**: Medium/low noise, ~1.4â5.5% error â reliability ~ 94â98.6% â weight = 0.95\n",
    "- **SNR > 26**: Very low noise, ~1.4% (or lower) error â ~98.6% reliability â weight = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted snr analysis\n",
    "\n",
    "# 1. Assign Weights Based on SNR Ranges\n",
    "def assign_weight(snr):\n",
    "    \"\"\"Return a weight based on known or estimated detection reliability.\"\"\"\n",
    "    if snr < 2:\n",
    "        # Very noisy => discard\n",
    "        return 0.0\n",
    "    elif 2 <= snr < 5:\n",
    "        # High noise => ~24% error => ~76% reliable\n",
    "        return 0.76\n",
    "    elif 5 <= snr < 8:\n",
    "        # Intermediate between high & medium => guess ~85% reliability\n",
    "        return 0.85\n",
    "    elif 8 <= snr <= 26:\n",
    "        # Medium/low noise => 1.4â5.5% error => pick ~95% reliability\n",
    "        return 0.95\n",
    "    else:  # snr > 26\n",
    "        return 1\n",
    "\n",
    "df_spots['weight'] = df_spots['snr'].apply(assign_weight)\n",
    "\n",
    "# 2. Weighted Histogram of SNR\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(\n",
    "    df_spots['snr'], \n",
    "    bins=50, \n",
    "    weights=df_spots['weight'], \n",
    "    alpha=0.7,\n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('Weighted Histogram of SNR')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Weighted Count')\n",
    "plt.show()\n",
    "\n",
    "# 3. Compare Weighted vs. Unweighted Histograms\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Unweighted\n",
    "plt.hist(\n",
    "    df_spots['snr'], \n",
    "    bins=50, \n",
    "    alpha=0.5, \n",
    "    label='Unweighted', \n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Weighted\n",
    "plt.hist(\n",
    "    df_spots['snr'], \n",
    "    bins=50, \n",
    "    weights=df_spots['weight'], \n",
    "    alpha=0.7, \n",
    "    label='Weighted', \n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('SNR Distribution: Unweighted vs. Weighted')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Count (or Weighted Count)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Weighted Scatter Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(\n",
    "    df_spots['snr'], \n",
    "    df_spots['signal'], \n",
    "    s=10 * df_spots['weight'],  # scale dot size by weight\n",
    "    c=df_spots['weight'],       # color also by weight (optional)\n",
    "    cmap='viridis', \n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('Scatter Plot of SNR vs. Signal (Marker Size Weighted)')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Signal')\n",
    "plt.colorbar(label='Weight')\n",
    "plt.yscale('log')  # if you want log scale for signal\n",
    "plt.show()\n",
    "\n",
    "# 5. Basic Statistics Showing Mean Weight per SNR Range\n",
    "bins = [0, 2, 5, 8, 26, np.inf]\n",
    "labels = ['<2', '2â5', '5â8', '8â26', '>26']\n",
    "df_spots['snr_bin'] = pd.cut(df_spots['snr'], bins=bins, labels=labels)\n",
    "\n",
    "stats_weight = df_spots.groupby('snr_bin')['weight'].agg(['count'])\n",
    "print(\"weighted Count by SNR Bin:\")\n",
    "print(stats_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Approach\n",
    "\n",
    "Here, instead of giving each spot a continuous weight, we use a Bernoulli process to decide per spot whether itâs kept or discarded, based on an estimated probability of being a true positive (TP).\n",
    "\n",
    "### Estimate Probability of True Positive \\(P(TP|SNR)\\)\n",
    "Sample from a Bernoulli distribution:\n",
    "\n",
    "### Example Probability Scheme\n",
    "Using the same error logic:\n",
    "\n",
    "- **SNR < 2**: \\(P(TP) = 0\\) (discard)\n",
    "- **2 â¤ SNR < 5**: error \\(\\approx 24\\%\\) â \\(P(TP) = 0.76\\)\n",
    "- **5 â¤ SNR < 8**: intermediate guess \\(P(TP) = 0.85\\)\n",
    "- **8 â¤ SNR â¤ 26**: error \\(\\approx 1.4\\% - 5.5\\%\\) â \\(P(TP) = 0.95\\)\n",
    "- **SNR > 26**: error \\(\\approx 1.4\\%\\) â \\(P(TP) \\approx 0.986\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Probability of True Positive\n",
    "def prob_true_positive(snr):\n",
    "    \"\"\"Return P(TP) based on known or estimated detection error.\"\"\"\n",
    "    if snr < 2:\n",
    "        return 0.0\n",
    "    elif 2 <= snr < 5:\n",
    "        return 0.76\n",
    "    elif 5 <= snr < 8:\n",
    "        return 0.85\n",
    "    elif 8 <= snr <= 26:\n",
    "        return 0.95\n",
    "    else:  # snr > 26\n",
    "        return 0.986\n",
    "\n",
    "df_spots['pTP'] = df_spots['snr'].apply(prob_true_positive)\n",
    "\n",
    "# 2. Single Realization (One Draw per Spot)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "df_spots['keep_prob_approach'] = np.random.rand(len(df_spots)) < df_spots['pTP']\n",
    "\n",
    "# 3. Visualize Kept vs. Discarded\n",
    "df_kept = df_spots[df_spots['keep_prob_approach'] == True]\n",
    "df_disc = df_spots[df_spots['keep_prob_approach'] == False]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(\n",
    "    df_spots['snr'], \n",
    "    bins=50, \n",
    "    alpha=0.4, \n",
    "    label='All Spots', \n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.hist(\n",
    "    df_kept['snr'], \n",
    "    bins=50, \n",
    "    alpha=0.7, \n",
    "    label='Kept (Prob. Approach)', \n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('SNR Distribution: Probabilistic Keeping')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Compare Number of Spots Kept vs. Discarded\n",
    "kept_count = len(df_kept)\n",
    "discarded_count = len(df_disc)\n",
    "print(f\"Kept Spots: {kept_count}, Discarded Spots: {discarded_count}\")\n",
    "\n",
    "# 5. (Optional) Multiple Realizations\n",
    "# If you want to see how stable the approach is over multiple runs\n",
    "n_runs = 5\n",
    "kept_fractions = []\n",
    "for i in range(n_runs):\n",
    "    # Draw Bernoulli for each spot\n",
    "    keep_vec = np.random.rand(len(df_spots)) < df_spots['pTP']\n",
    "    # Fraction of spots kept\n",
    "    kept_fraction = keep_vec.mean()\n",
    "    kept_fractions.append(kept_fraction)\n",
    "\n",
    "print(\"Kept fractions over multiple runs:\", kept_fractions)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(range(n_runs), kept_fractions, marker='o', linestyle='--')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Fraction of Spots Kept Over Multiple Probabilistic Runs')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Fraction Kept')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.cellprops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique cell id for every cells\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(spots, clusters, props) -> pd.DataFrame:\n",
    "    results = pd.DataFrame(columns=['cell_id', 'num_ts', 'num_spots_ts', 'num_foci', 'num_spots_foci', 'num_spots', 'num_nuc_spots', 'num_cyto_spots', \n",
    "                                    'nuc_area_px', 'cyto_area_px', 'avg_nuc_int', 'avg_cyto_int', 'time', 'Dex_conc'])\n",
    "    \n",
    "    # Sort spots, clusters, and props by unique_cell_id\n",
    "    spots = spots.sort_values(by='unique_cell_id')\n",
    "    clusters = clusters.sort_values(by='unique_cell_id')\n",
    "    props = props.sort_values(by='unique_cell_id')\n",
    "\n",
    "    # unique cell id\n",
    "    cell_ids = props['unique_cell_id']\n",
    "\n",
    "    # num of ts\n",
    "    num_ts = clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of foci\n",
    "    num_foci = clusters[clusters['is_nuc'] == 0].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of ts spots\n",
    "    num_spots_ts = clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id')['nb_spots'].sum().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of foci spots\n",
    "    num_spots_foci = clusters[clusters['is_nuc'] == 0].groupby('unique_cell_id')['nb_spots'].sum().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spots\n",
    "    num_spots = spots.groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spot in nuc\n",
    "    num_nuc_spots = spots[spots['is_nuc'] == 1].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spot in cyto \n",
    "    num_cyto_spots = spots[spots['is_nuc'] == 0].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # nuc area\n",
    "    nuc_area = props['nuc_area']\n",
    "\n",
    "    # cyto area\n",
    "    cyto_area = props['cyto_area']\n",
    "\n",
    "    # avg int nuc\n",
    "    avg_nuc_int = props['nuc_intensity_mean-0']\n",
    "    \n",
    "    # avg int cyto\n",
    "    avg_cyto_int = props['cyto_intensity_mean-0']\n",
    "\n",
    "    # time (experiment)\n",
    "    time = props['time'] \n",
    "\n",
    "    # Dex conc\n",
    "    dex_conc = props['Dex_Conc']\n",
    "\n",
    "    results['cell_id'] = cell_ids\n",
    "    results['num_ts'] = num_ts.values\n",
    "    results['num_foci'] = num_foci.values\n",
    "    results['num_spots_ts'] = num_spots_ts.values\n",
    "    results['num_spots_foci'] = num_spots_foci.values\n",
    "    results['num_spots'] = num_spots.values\n",
    "    results['num_nuc_spots'] = num_nuc_spots.values\n",
    "    results['num_cyto_spots'] = num_cyto_spots.values\n",
    "    results['nuc_area_px'] = nuc_area.values\n",
    "    results['cyto_area_px'] = cyto_area.values\n",
    "    results['avg_nuc_int'] = avg_nuc_int.values\n",
    "    results['avg_cyto_int'] = avg_cyto_int.values\n",
    "    results['time'] = time.values\n",
    "    results['Dex_conc'] = dex_conc.values\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_spots = measure(SD.spots, SD.clusters, SD.cellprops)\n",
    "cell_spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure num_spots = num_nuc_spots + num_cyto_spots for all rows\n",
    "assert (cell_spots['num_spots'] == cell_spots['num_nuc_spots'] + cell_spots['num_cyto_spots']).all(), \"Mismatch in spot counts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.cellspots.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.cellspots = SD.cellspots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                                    left_on=['NAS_location', 'cell_id', 'fov'], \n",
    "                                    right_on=['NAS_location', 'cell_label', 'fov'], \n",
    "                                    how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align indices before performing the assertion\n",
    "if 'cell_id' in cell_spots.keys():\n",
    "    cell_spots = cell_spots.set_index('cell_id')\n",
    "\n",
    "if 'unique_cell_id' in SD.cellspots.keys():\n",
    "    SD.cellspots = SD.cellspots.set_index('unique_cell_id')\n",
    "\n",
    "aligned_nb_rna = SD.cellspots['nb_rna']\n",
    "\n",
    "aligned_num_spots = cell_spots['num_spots']\n",
    "aligned_num_spots = aligned_num_spots - cell_spots['num_spots_ts']\n",
    "\n",
    "# Ensure aligned_num_spots only contains indices present in aligned_nb_rna\n",
    "aligned_num_spots = aligned_num_spots.loc[SD.cellspots.index]\n",
    "\n",
    "not_close_indices = np.where(~np.isclose(aligned_nb_rna, aligned_num_spots, rtol = 0.05))[0]\n",
    "print(\"Indices where nb_rna and num_spots are not close:\", len(not_close_indices))\n",
    "\n",
    "assert len(not_close_indices) == 0, \"Mismatch in nb_rna and num_spots counts\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'cell_id':<10} {'my counting':<30} {'bigfish counting':<30} {'corrected':<30} {'ts spots':<30} {'foci spots':<30}\")\n",
    "for cell_id, my, bf, cr, ts, foci in zip(aligned_nb_rna.iloc[not_close_indices].index, cell_spots.loc[aligned_nb_rna.index]['num_spots'].iloc[not_close_indices], \n",
    "                                         aligned_nb_rna.iloc[not_close_indices], aligned_num_spots.iloc[not_close_indices], \n",
    "                                         cell_spots.loc[aligned_nb_rna.index]['num_spots_ts'].iloc[not_close_indices], cell_spots.loc[aligned_nb_rna.index]['num_spots_foci'].iloc[not_close_indices] ):\n",
    "    print(f\"{cell_id:<10} {my:<30} {bf:<30} {cr:<30} {ts:<30} {foci:<30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "am1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
