{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  GR DUSP1 Gating Notebook\n",
    "\n",
    "The Purpose of this notebook is:\n",
    "1) Load in all analyisis for final dataframe preparation \n",
    "2) Filter DUSP1 data to remove partial cells and low SNR spots\n",
    "3) Filter GR data to remove partial cells\n",
    "4) Estimate GR cytoplasmic area from DUPS1 data\n",
    "5) GR intensity to molecular counts \n",
    "6) Concatonate final GR and DUSP1 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ericron/Desktop/AngelFISH\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import seaborn as sns\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)\n",
    "\n",
    "matplotlib_logger = logging.getLogger('matplotlib')\n",
    "matplotlib_logger.setLevel(logging.WARNING)\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "print(src_path)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.Analysis import AnalysisManager, Analysis, SpotDetection_SNRConfirmation, Spot_Cluster_Analysis_WeightedSNR, GR_Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure function to create final DUSP1 dataframe for SSIT\n",
    "# Function to get the second largest value or default to 0\n",
    "def second_largest(series):\n",
    "    unique_vals = series.dropna().unique()  # Remove NaN and get unique values\n",
    "    if len(unique_vals) < 2:\n",
    "        return 0  # Return 0 if there's no second-largest value\n",
    "    return np.sort(unique_vals)[-2]  # Return the second-largest value\n",
    "\n",
    "\n",
    "def measure_DUSP1(spots, clusters, props) -> pd.DataFrame:\n",
    "    results = pd.DataFrame(columns=['cell_id', 'num_ts', 'num_spots_ts', 'largest_ts', 'second_largest_ts', 'num_foci', 'num_spots_foci', 'num_spots', 'num_nuc_spots', 'num_cyto_spots', \n",
    "                                    'nuc_area_px', 'cyto_area_px', 'avg_nuc_int', 'avg_cyto_int', 'time', 'Dex_conc', 'replica'])\n",
    "    \n",
    "    # Sort spots, clusters, and props by unique_cell_id\n",
    "    spots = spots.sort_values(by='unique_cell_id')\n",
    "    clusters = clusters.sort_values(by='unique_cell_id')\n",
    "    props = props.sort_values(by='unique_cell_id')\n",
    "\n",
    "    # unique cell id\n",
    "    cell_ids = props['unique_cell_id']\n",
    "\n",
    "    # num of ts\n",
    "    num_ts = clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of foci\n",
    "    num_foci = clusters[clusters['is_nuc'] == 0].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of ts spots\n",
    "    num_spots_ts = clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id')['nb_spots'].sum().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # largest TS size\n",
    "    largest_ts = clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id')['nb_spots'].max().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # Compute second-largest TS size per cell\n",
    "    second_largest_ts = (clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id')['nb_spots'].apply(second_largest).reindex(cell_ids, fill_value=0))    \n",
    "\n",
    "    # num of foci spots\n",
    "    num_spots_foci = clusters[clusters['is_nuc'] == 0].groupby('unique_cell_id')['nb_spots'].sum().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spots\n",
    "    num_spots = spots.groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spot in nuc\n",
    "    num_nuc_spots = spots[spots['is_nuc'] == 1].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spot in cyto \n",
    "    num_cyto_spots = spots[spots['is_nuc'] == 0].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # nuc area\n",
    "    nuc_area = props['nuc_area']\n",
    "\n",
    "    # cyto area\n",
    "    cyto_area = props['cyto_area']\n",
    "\n",
    "    # avg int nuc\n",
    "    avg_nuc_int = props['nuc_intensity_mean-0']\n",
    "    \n",
    "    # avg int cyto\n",
    "    avg_cyto_int = props['cyto_intensity_mean-0']\n",
    "\n",
    "    # time (experiment)\n",
    "    time = props['time'] \n",
    "\n",
    "    # Dex conc\n",
    "    dex_conc = props['Dex_conc']\n",
    "\n",
    "    # Replica\n",
    "    replica = spots.groupby('unique_cell_id')['replica'].first().reindex(cell_ids, fill_value=np.nan)\n",
    "\n",
    "    results['cell_id'] = cell_ids\n",
    "    results['num_ts'] = num_ts.values\n",
    "    results['largest_ts'] = largest_ts.values\n",
    "    results['second_largest_ts'] = second_largest_ts.values\n",
    "    results['num_foci'] = num_foci.values\n",
    "    results['num_spots_ts'] = num_spots_ts.values\n",
    "    results['num_spots_foci'] = num_spots_foci.values\n",
    "    results['num_spots'] = num_spots.values\n",
    "    results['num_nuc_spots'] = num_nuc_spots.values\n",
    "    results['num_cyto_spots'] = num_cyto_spots.values\n",
    "    results['nuc_area_px'] = nuc_area.values\n",
    "    results['cyto_area_px'] = cyto_area.values\n",
    "    results['avg_nuc_int'] = avg_nuc_int.values\n",
    "    results['avg_cyto_int'] = avg_cyto_int.values\n",
    "    results['time'] = time.values\n",
    "    results['Dex_conc'] = dex_conc.values\n",
    "    results['replica'] = replica.values\n",
    "\n",
    "    return results\n",
    "\n",
    "def measure_DUSP1_TPL(spots, clusters, props) -> pd.DataFrame:\n",
    "    results = pd.DataFrame(columns=['cell_id', 'num_ts', 'num_spots_ts', 'largest_ts', 'second_largest_ts', 'num_foci', 'num_spots_foci', 'num_spots', 'num_nuc_spots', 'num_cyto_spots', \n",
    "                                    'nuc_area_px', 'cyto_area_px', 'avg_nuc_int', 'avg_cyto_int', 'time', 'time_tpl' 'Dex_conc', 'replica',\n",
    "                                    'tryptCond1', 'tryptCond2', 'tryptCond3', 'tryptCond4', 'tryptCond5'])\n",
    "    \n",
    "    # Sort spots, clusters, and props by unique_cell_id\n",
    "    spots = spots.sort_values(by='unique_cell_id')\n",
    "    clusters = clusters.sort_values(by='unique_cell_id')\n",
    "    props = props.sort_values(by='unique_cell_id')\n",
    "\n",
    "    # unique cell id\n",
    "    cell_ids = props['unique_cell_id']\n",
    "\n",
    "    # num of ts\n",
    "    num_ts = clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of foci\n",
    "    num_foci = clusters[clusters['is_nuc'] == 0].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of ts spots\n",
    "    num_spots_ts = clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id')['nb_spots'].sum().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # largest TS size\n",
    "    largest_ts = clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id')['nb_spots'].max().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # Compute second-largest TS size per cell\n",
    "    second_largest_ts = (clusters[clusters['is_nuc'] == 1].groupby('unique_cell_id')['nb_spots'].apply(second_largest).reindex(cell_ids, fill_value=0))    \n",
    "\n",
    "    # num of foci spots\n",
    "    num_spots_foci = clusters[clusters['is_nuc'] == 0].groupby('unique_cell_id')['nb_spots'].sum().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spots\n",
    "    num_spots = spots.groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spot in nuc\n",
    "    num_nuc_spots = spots[spots['is_nuc'] == 1].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # num of spot in cyto \n",
    "    num_cyto_spots = spots[spots['is_nuc'] == 0].groupby('unique_cell_id').size().reindex(cell_ids, fill_value=0)\n",
    "\n",
    "    # nuc area\n",
    "    nuc_area = props['nuc_area']\n",
    "\n",
    "    # cyto area\n",
    "    cyto_area = props['cyto_area']\n",
    "\n",
    "    # avg int nuc\n",
    "    avg_nuc_int = props['nuc_intensity_mean-0']\n",
    "    \n",
    "    # avg int cyto\n",
    "    avg_cyto_int = props['cyto_intensity_mean-0']\n",
    "\n",
    "    # time of Dex(experiment)\n",
    "    time = props['time'] \n",
    "\n",
    "    # time of tpl\n",
    "    time_tpl = props['time_tpl']\n",
    "\n",
    "    # Dex conc\n",
    "    dex_conc = props['Dex_conc']\n",
    "\n",
    "    # Replica\n",
    "    replica = spots.groupby('unique_cell_id')['replica'].first().reindex(cell_ids, fill_value=np.nan)\n",
    "\n",
    "    # Extract unique TPL times\n",
    "    trptTimes = np.sort(props['time_tpl'].dropna().unique())\n",
    "    \n",
    "    # Initialize tryptCond columns\n",
    "    for k, t_TPL in enumerate(trptTimes, start=1):\n",
    "        JJ = (\n",
    "            ((dex_conc == 100) | (time == 0)) &\n",
    "            ((time == 0) | ((time <= t_TPL) & time_tpl.isna()) | ((time >= t_TPL) & (time_tpl == t_TPL)))\n",
    "        )\n",
    "        results[f'tryptCond{k}'] = JJ.astype(int) * k\n",
    "    \n",
    "    # Populate DataFrame\n",
    "    results['cell_id'] = cell_ids\n",
    "    results['num_ts'] = num_ts.values\n",
    "    results['largest_ts'] = largest_ts.values\n",
    "    results['second_largest_ts'] = second_largest_ts.values\n",
    "    results['num_foci'] = num_foci.values\n",
    "    results['num_spots_ts'] = num_spots_ts.values\n",
    "    results['num_spots_foci'] = num_spots_foci.values\n",
    "    results['num_spots'] = num_spots.values\n",
    "    results['num_nuc_spots'] = num_nuc_spots.values\n",
    "    results['num_cyto_spots'] = num_cyto_spots.values\n",
    "    results['nuc_area_px'] = nuc_area.values\n",
    "    results['cyto_area_px'] = cyto_area.values\n",
    "    results['avg_nuc_int'] = avg_nuc_int.values\n",
    "    results['avg_cyto_int'] = avg_cyto_int.values\n",
    "    results['time'] = time.values\n",
    "    results['time_tpl'] = time_tpl.values\n",
    "    results['Dex_conc'] = dex_conc.values\n",
    "    results['replica'] = replica.values\n",
    "\n",
    "    return results\n",
    "\n",
    "# Measure function for GR pre-SSIT dataframe \n",
    "def measure_GR(cellprops) -> pd.DataFrame:\n",
    "    results = pd.DataFrame(columns=['cell_id', 'nuc_area', 'nucGRint', 'cytoGRint', 'time', 'Dex_conc', 'replica'])\n",
    "    \n",
    "    # Sort cellprops by unique_cell_id\n",
    "    props = cellprops.sort_values(by='unique_cell_id')\n",
    "\n",
    "    # unique cell id\n",
    "    cell_ids = props['unique_cell_id']\n",
    "\n",
    "    # nuc area\n",
    "    nuc_area = props['nuc_area']\n",
    "\n",
    "    # avg int nuc\n",
    "    nucGRint = props['nuc_intensity_mean-0']\n",
    "    \n",
    "    # avg int pseudocyto mask\n",
    "    cytoGRint = props['cyto_intensity_mean-0']\n",
    "\n",
    "    # time (experiment)\n",
    "    time = props['time'] \n",
    "\n",
    "    # Dex conc\n",
    "    dex_conc = props['Dex_conc']\n",
    "\n",
    "    # Replica\n",
    "    replica = props['replica']\n",
    "\n",
    "    results['cell_id'] = cell_ids\n",
    "    results['nuc_area'] = nuc_area.values\n",
    "    results['nucGRint'] = nucGRint.values\n",
    "    results['cytoGRint'] = cytoGRint.values\n",
    "    results['time'] = time.values\n",
    "    results['Dex_conc'] = dex_conc.values\n",
    "    results['replica'] = replica.values\n",
    "\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the log file to search for analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = None \n",
    "log_location = r'/Volumes/share/Users/Eric/GR_DUSP1_2025'  # r'/Volumes/share/Users/Jack/All_Analysis' \n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "BlockingIOError",
     "evalue": "[Errno 35] Unable to open file (unable to lock file, errno = 35, error message = 'Resource temporarily unavailable')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# list all analysis done \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m all_analysis_names \u001b[38;5;241m=\u001b[39m \u001b[43mam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_analysis_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AngelFISH/src/Analysis.py:72\u001b[0m, in \u001b[0;36mAnalysisManager.list_analysis_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_analysis_names\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_analysis_names()\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalysis_names:\n",
      "File \u001b[0;32m~/Desktop/AngelFISH/src/Analysis.py:190\u001b[0m, in \u001b[0;36mAnalysisManager.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [h\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_files]:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_files\u001b[38;5;241m.\u001b[39mappend(\u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/AngelFISH/.venv/lib/python3.11/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/Desktop/AngelFISH/.venv/lib/python3.11/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mBlockingIOError\u001b[0m: [Errno 35] Unable to open file (unable to lock file, errno = 35, error message = 'Resource temporarily unavailable')"
     ]
    }
   ],
   "source": [
    "# list all analysis done \n",
    "all_analysis_names = am.list_analysis_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUSP1 Experiment Analysis List\n",
    "\n",
    "### DUSP1 100nM Dex 3hr Time-sweep\n",
    "- Replica D: `Analysis_DUSP1_D_JacksRunAll_2025-02-05`\n",
    "- Replica E: `Analysis_DUSP1_E_ERonRunAll_2025-02-06`\n",
    "- Replica F: `Analysis_DUSP1_F_ERonReRun_2025-02-08`\n",
    "- Replica M: `Analysis_DUSP1_M_ERonRunAll_2025-02-06`\n",
    "- Replica N: `Analysis_DUSP1_N_JacksRunAll_2025-02-06`\n",
    "\n",
    "### DUSP1 75min Concentration-sweep\n",
    "- Replica G: `Analysis_DUSP1_G_ERonReRun_2025-02-08`\n",
    "- Replica H: `Analysis_DUSP1_H_ERonRunAll_2025-02-06`\n",
    "- Replica I: `Analysis_DUSP1_I_JacksRunAll_2025-02-06`\n",
    "\n",
    "### DUSP1 0.3, 1, 10nM Dex 3hr Time-sweep\n",
    "- Replica J: `Analysis_DUSP1_J_ERonRunAll_2025-02-06`\n",
    "- Replica K: `Analysis_DUSP1_K_ERonReRun_2025-02-08`\n",
    "- Replica L: `Analysis_DUSP1_L_JacksRunAll_2025-02-06`\n",
    "\n",
    "### DUSP1 TPL\n",
    "- Replica O `Analysis_DUSP1_O_JacksRunAll_2025-02-06`\n",
    "- Replica P `Analysis_DUSP1_P_ERonReRun_2025-02-08`\n",
    "\n",
    "# GR Experiment Analyis List\n",
    "\n",
    "### GR 1, 10, 100nM Dex 3hr Time-Sweep\n",
    "- Replica A: `Analysis_GR_IC_A_ER020725_2025-02-07`\n",
    "- Replica B: `Analysis_GR_IC_B_ReRun021025_2025-02-10`\n",
    "- Replica C: `Analysis_GR_IC_C_ER020725_2025-02-08`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "# Select the specific analysis - ex. DUSP1 100nM Dex 3hr Time-sweep Replica 1\n",
    "am.select_analysis('DUSP1_D_JacksRunAll')\n",
    "print('locations with this dataset:', am.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter DUPS1 data to remove partial cells and low SNR spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepD = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 100nM Dex 3hr Time-sweep Replica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset - DUSP1 100nM Dex 3hr Time-sweep Replica 2\n",
    "am.select_analysis('DUSP1_E_ERonRunAll')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepE = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 100nM Dex 3hr Time-sweep Replica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_F_ERonReRun')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepF = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 100nM Dex 3hr Time-sweep Replica 4 (partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_M_ERonRunAll')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepM = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 100nM Dex 3hr Time-sweep Replica 5 (partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_N_JacksRunAll')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepN = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 75min Concentration-sweep Replica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset - DUSP1 100nM Dex 3hr Time-sweep Replica 2\n",
    "am.select_analysis('DUSP1_G_ERonReRun')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepG = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 75min Concentration-sweep Replica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_H_ERonRunAll')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepH = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 75min Concentration-sweep Replica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_I_JacksRunAll')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepI = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 0.3, 1, 10nM Dex 3hr Time-sweep Replica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_J_ERonRunAll')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepJ = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 0.3, 1, 10nM Dex 3hr Time-sweep Replica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_K_ERonReRun')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepK = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 0.3, 1, 10nM Dex 3hr Time-sweep Replica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_L_JacksRunAll')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepL = measure_DUSP1(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 100nM Dex & 5µM TPL Time-sweep Replica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_O_JacksRunAll')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepO_TPL = measure_DUSP1_TPL(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DUSP1 100nM Dex & 5µM TPL Time-sweep Replica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('DUSP1_P_ERonReRun')\n",
    "# Initiate the Weighted SNR analysis class\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)\n",
    "# Load the data\n",
    "SD.get_data()\n",
    "# Assign revise weighted threshold\n",
    "SD.assign_revised_weighted_threshold()\n",
    "# Create unique cell id for every cell\n",
    "SD.cellprops['unique_cell_id'] = np.arange(len(SD.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "SD.cellprops = SD.cellprops[SD.cellprops['touching_border'] == 0]\n",
    "# Remove spots that are less than the weighted snr threshold\n",
    "SD.spots = SD.spots[SD.spots['keep_wsnr']]\n",
    "# Merge the spots and clusters dataframes by the unique cell ID\n",
    "SD.spots = SD.spots.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "SD.clusters = SD.clusters.merge(SD.cellprops[['NAS_location', 'cell_label', 'fov', 'unique_cell_id']], \n",
    "                            on=['NAS_location', 'cell_label', 'fov'], \n",
    "                            how='left')\n",
    "# Use measure_DUSP1 to create SSIT compatible dataframe\n",
    "DUSP1_RepP_TPL = measure_DUSP1_TPL(SD.spots, SD.clusters, SD.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUSP1_ALL.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# label Dex-only frames as TPL = False\n",
    "for df in [DUSP1_RepD, DUSP1_RepE, DUSP1_RepF, DUSP1_RepM, DUSP1_RepN,\n",
    "           DUSP1_RepG, DUSP1_RepH, DUSP1_RepI, DUSP1_RepJ, DUSP1_RepK, DUSP1_RepL]:\n",
    "    df['is_TPL'] = False\n",
    "\n",
    "# Label TPL frames as TPL = True\n",
    "for df in [DUSP1_RepO_TPL, DUSP1_RepP_TPL]:\n",
    "    df['is_TPL'] = True\n",
    "\n",
    "# 1) Concatenate Dex-only DUSP1 data\n",
    "DUSP1_ALL = pd.concat(\n",
    "    [DUSP1_RepD, DUSP1_RepE, DUSP1_RepF, DUSP1_RepM, DUSP1_RepN,\n",
    "     DUSP1_RepG, DUSP1_RepH, DUSP1_RepI, DUSP1_RepJ, DUSP1_RepK, DUSP1_RepL],\n",
    "    ignore_index=True\n",
    ")\n",
    "DUSP1_ALL['unique_cell_id'] = np.arange(len(DUSP1_ALL))\n",
    "\n",
    "# 2) Concatenate TPL data\n",
    "DUSP1_TPL = pd.concat(\n",
    "    [DUSP1_RepO_TPL, DUSP1_RepP_TPL],\n",
    "    ignore_index=True\n",
    ")\n",
    "DUSP1_TPL['unique_cell_id'] = np.arange(len(DUSP1_TPL))\n",
    "\n",
    "# 3) Create a combined DataFrame (Dex + TPL), preserving the is_TPL flag\n",
    "DUSP1_Dex_TPL_ALL = pd.concat([DUSP1_ALL, DUSP1_TPL], \n",
    "                           ignore_index=True, \n",
    "                           sort=False)\n",
    "# Assign a new unique_cell_id across the combined data\n",
    "DUSP1_Dex_TPL_ALL['unique_cell_id'] = np.arange(len(DUSP1_Dex_TPL_ALL))\n",
    "\n",
    "# (Optional) save each DataFrame to disk\n",
    "DUSP1_ALL.to_csv('DUSP1_ALL.csv', index=False)\n",
    "DUSP1_TPL.to_csv('DUSP1_TPL.csv', index=False)\n",
    "DUSP1_Dex_TPL_ALL.to_csv('DUSP1_Dex_TPL_ALL.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter GR data:\n",
    "1. Remove partial cells\n",
    "2. Use `measure_GR` to prepare pre-SSIT dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    GR 1, 10, 100nM Dex 3hr Time-sweep Replica 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('GR_IC_A_ER020725')\n",
    "# select GR conformation analysis \n",
    "GR = GR_Confirmation(am)\n",
    "# Load the data\n",
    "GR.get_data()\n",
    "# Create unique cell id for every cell\n",
    "GR.cellprops['unique_cell_id'] = np.arange(len(GR.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "GR.cellprops = GR.cellprops[GR.cellprops['touching_border'] == 0]\n",
    "# Use measure_GR to create SSIT compatible dataframe\n",
    "GR_RepA = measure_GR(GR.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    GR 1, 10, 100nM Dex 3hr Time-sweep Replica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('GR_IC_B_ReRun021025')\n",
    "# select GR conformation analysis \n",
    "GR = GR_Confirmation(am)\n",
    "# Load the data\n",
    "GR.get_data()\n",
    "# Create unique cell id for every cell\n",
    "GR.cellprops['unique_cell_id'] = np.arange(len(GR.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "GR.cellprops = GR.cellprops[GR.cellprops['touching_border'] == 0]\n",
    "# Use measure_GR to create SSIT compatible dataframe\n",
    "GR_RepB = measure_GR(GR.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    GR 1, 10, 100nM Dex 3hr Time-sweep Replica 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the class and find analysis at log_location\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True)\n",
    "# Select the dataset\n",
    "am.select_analysis('GR_IC_C_ER020725')\n",
    "# select GR conformation analysis \n",
    "GR = GR_Confirmation(am)\n",
    "# Load the data\n",
    "GR.get_data()\n",
    "# Create unique cell id for every cell\n",
    "GR.cellprops['unique_cell_id'] = np.arange(len(GR.cellprops))\n",
    "# Remove partial cells from dataset\n",
    "GR.cellprops = GR.cellprops[GR.cellprops['touching_border'] == 0]\n",
    "# Use measure_GR to create SSIT compatible dataframe\n",
    "GR_RepC = measure_GR(GR.cellprops)\n",
    "am.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate all DUSP1 Dex experiments & reset 'unique_cell_id'\n",
    "GR_ALL = pd.concat(\n",
    "[GR_RepA, GR_RepB, GR_RepC], ignore_index=True) # DUSP1_RepN, DUSP1_RepJ\n",
    "GR_ALL['unique_cell_id'] = np.arange(len(GR_ALL))\n",
    "\n",
    "# Save the dataframes to a csv file\n",
    "GR_ALL.to_csv('GR_ALL.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GR_ALL & DUSP1_All final dataframe preperation for SSIT\n",
    "\n",
    "1) Fit a Polynomial (2nd-degree) using (nuc_area_px, cyto_area_px) from DUSP1_ALL.\n",
    "\n",
    "2) Estimate Cytoplasm Area in GR_ALL:\n",
    "3) Creates `CalcCytoArea` by evaluating the fitted polynomial at each row’s `nuc_area`.\n",
    "\n",
    "4) Gate both data sets on the 25%–75% range of nuclear area.\n",
    "\n",
    "5) Compute “Normalized” GR (`normGRnuc`, `normGRcyt`) in GR_ALL:\n",
    "- Scales nuclear/cyt intensities (5%→95% range) into integer bins [0,20].\n",
    "\n",
    "6) Plot Histograms for the normalized nuclear/cyt GR (using custom colors).\n",
    "\n",
    "7) Save the updated, gated data sets to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) READ INPUT DATA\n",
    "# =========================\n",
    "# DUSP1_ALL from above:\n",
    "df_dusp = DUSP1_ALL\n",
    "\n",
    "# GR_ALL from above:\n",
    "df_gr = GR_ALL\n",
    "\n",
    "# 2) FIT POLYNOMIAL TO (NUC, CYTO) FROM DUSP1_ALL\n",
    "# =========================\n",
    "# Find rows that have valid nuc_area_px and cyto_area_px.\n",
    "df_dusp_nonmissing = df_dusp.dropna(subset=['nuc_area_px', 'cyto_area_px']).copy()\n",
    "\n",
    "x_nuc = df_dusp_nonmissing['nuc_area_px'].values\n",
    "y_cyto = df_dusp_nonmissing['cyto_area_px'].values\n",
    "\n",
    "# Fit a 2nd-degree polynomial: cyto_area_px = a*(nuc_area_px)^2 + b*(nuc_area_px) + c\n",
    "poly_coeffs = np.polyfit(x_nuc, y_cyto, deg=2)\n",
    "\n",
    "# optional: for debugging/inspection\n",
    "print(\"Fitted polynomial coefficients (a, b, c):\", poly_coeffs)\n",
    "# plot the fitted polynomial on the data\n",
    "plt.scatter(x_nuc, y_cyto, label='data')\n",
    "x_fit = np.linspace(x_nuc.min(), x_nuc.max(), 100)\n",
    "y_fit = np.polyval(poly_coeffs, x_fit)\n",
    "plt.plot(x_fit, y_fit, label='fitted polynomial', color='red')\n",
    "plt.xlabel('Nuclear area (px)')\n",
    "plt.ylabel('Cytoplasm area (px)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 3) ESTIMATE CYTO AREA IN GR_ALL\n",
    "# =========================\n",
    "# We'll store the computed cytoplasm area in a new column: 'CalcCytoArea'\n",
    "# Evaluate the polynomial at GR_ALL['nuc_area'].\n",
    "df_gr['CalcCytoArea'] = np.polyval(poly_coeffs, df_gr['nuc_area'])\n",
    "\n",
    "\n",
    "# 4) GATE BOTH DATAFRAMES ON [25%, 75%] NUCLEAR AREA\n",
    "# =========================\n",
    "# We'll define a helper function for gating.\n",
    "def gate_on_nuc_area(df, nuc_col):\n",
    "    \"\"\"Return a copy of df gated to [25th, 75th percentile] of nuc_col.\"\"\"\n",
    "    lower = df[nuc_col].quantile(0.25)\n",
    "    upper = df[nuc_col].quantile(0.75)\n",
    "    return df[(df[nuc_col] >= lower) & (df[nuc_col] <= upper)].copy()\n",
    "\n",
    "# Gate DUSP1_ALL on nuc_area_px\n",
    "df_dusp_gated = gate_on_nuc_area(df_dusp, 'nuc_area_px')\n",
    "\n",
    "# Gate GR_ALL on nuc_area\n",
    "df_gr_gated = gate_on_nuc_area(df_gr, 'nuc_area')\n",
    "\n",
    "print(f\"DUSP1_ALL original: {len(df_dusp)} rows -> gated: {len(df_dusp_gated)} rows\")\n",
    "print(f\"GR_ALL original:    {len(df_gr)} rows -> gated: {len(df_gr_gated)} rows\")\n",
    "\n",
    "# # (Optional) Save a copy of the gated GR data\n",
    "# df_gr_gated.to_csv(\"GR_ALL_Gated_On_Nuc.csv\", index=False)\n",
    "\n",
    "nuc_mean = df_dusp_gated['nuc_area_px'].mean()\n",
    "cyto_mean = df_dusp_gated['cyto_area_px'].mean()\n",
    "ratio = nuc_mean / cyto_mean\n",
    "print(\"Estimated (Nuc : Cyto) area ratio =\", ratio)\n",
    "\n",
    "# # Remove timepoints 20, 40, 60, 90, 150 without replicas\n",
    "# timepoints_to_remove = [20, 40, 60, 90, 150]\n",
    "# df_gr_gated = df_gr_gated[~df_gr_gated['time'].isin(timepoints_to_remove)]\n",
    "\n",
    "# # (Optional) Randomly assign Dex=0 rows to 1, 10, 100 nM\n",
    "# # Find rows where Dex_Conc == 0\n",
    "# mask_zero = (df_gr_gated['Dex_Conc'] == 0)\n",
    "# n_zero = mask_zero.sum()\n",
    "# if n_zero > 0:\n",
    "#     # Generate random indices in [0, 2] for each zero row\n",
    "#     rand_indices = np.random.randint(0, 3, size=n_zero)\n",
    "#     dex_values = [1, 10, 100]\n",
    "#     df_gr_gated.loc[mask_zero, 'Dex_Conc'] = [dex_values[i] for i in rand_indices]\n",
    "\n",
    "\n",
    "# 5) COMPUTE \"NORMALIZED\" GR FOR NUC & CYTO IN GR_ALL\n",
    "# =========================\n",
    "#  Define bins & threshold:\n",
    "binsCyt = 15\n",
    "threshold = 0.01\n",
    "\n",
    "#  Sort cytoplasmic intensities:\n",
    "sortGRcyt = df_gr_gated['cytoGRint'].sort_values().values\n",
    "\n",
    "#  Identify the indices for the 1% and 99% cutoffs\n",
    "low_idx  = int(np.ceil(threshold * len(sortGRcyt))) - 1\n",
    "high_idx = int(np.ceil((1 - threshold) * len(sortGRcyt))) - 1\n",
    "\n",
    "#  Gather the min/max thresholds\n",
    "minThreshold = sortGRcyt[low_idx]\n",
    "maxCytGR     = sortGRcyt[high_idx]\n",
    "\n",
    "def apply_cyt_norm(values):\n",
    "    # Scale to [0,1] based on [minThreshold, maxCytGR], then clip\n",
    "    scaled = (values - minThreshold) / (maxCytGR - minThreshold)\n",
    "    clipped = np.clip(scaled, 0, 1)\n",
    "    return np.round(clipped * binsCyt).astype(int)\n",
    "\n",
    "# Compute normGRcyt:\n",
    "df_gr_gated['normGRcyt'] = apply_cyt_norm(df_gr_gated['cytoGRint'])\n",
    "\n",
    "# Brian's Matlab logic for the nuclear intensity:\n",
    "#   'nucGRLevel = ratio * Nuc_GR_avg_int', then scale by the same [minThreshold, maxCytGR].\n",
    "df_gr_gated['nucGR_Level'] = ratio * df_gr_gated['nucGRint']\n",
    "df_gr_gated['normGRnuc']   = apply_cyt_norm(df_gr_gated['nucGR_Level'])\n",
    "\n",
    "# Clean up if you don't want that intermediate column:\n",
    "# df_gr_gated.drop(columns=['nucGR_Level'], inplace=True)\n",
    "\n",
    "# Debug: check a few rows\n",
    "print(df_gr_gated[['nucGRint','cytoGRint','normGRnuc','normGRcyt']].head())\n",
    "\n",
    "\n",
    "# 6) HISTOGRAMS\n",
    "# =========================\n",
    "# Set the SNS theme\n",
    "sns.set_theme(style=\"ticks\", palette=\"colorblind\", context=\"poster\", font='times new roman')\n",
    "\n",
    "# Define the concentrations and specific timepoints you want to plot\n",
    "concentrations = [1, 10, 100]  # nM concentrations\n",
    "desired_timepoints = [0, 10, 30, 50, 75, 120, 180]\n",
    "\n",
    "# Metrics to plot and their corresponding labels\n",
    "metrics = ['nucGRint', 'cytoGRint', 'normGRnuc', 'normGRcyt']\n",
    "labels = {\n",
    "    'nucGRint': 'Nuclear GR Intensity',\n",
    "    'cytoGRint': 'Cytoplasmic GR Intensity',\n",
    "    'normGRnuc': 'Normalized Nuclear GR Counts',\n",
    "    'normGRcyt': 'Normalized Cytoplasmic GR Counts',\n",
    "}\n",
    "\n",
    "# # Define axis limits for each metric\n",
    "# axis_limits = {\n",
    "#     'nucGRint': (1000, 6000),\n",
    "#     'cytoGRint': (100, 4000),\n",
    "#     'normGRnuc': (0, 30),\n",
    "#     'normGRcyt': (0, 30)\n",
    "# }\n",
    "\n",
    "# Generate colors from the Seaborn colorblind palette\n",
    "colors = sns.color_palette(\"colorblind\", len(concentrations) + 1)  # Including color for 0 concentration\n",
    "concentration_color_map = dict(zip([0] + concentrations, colors))\n",
    "\n",
    "# Plotting\n",
    "for metric in metrics:\n",
    "    fig, axes = plt.subplots(len(concentrations), len(desired_timepoints), figsize=(20, 10), sharey='row')\n",
    "    for i, conc in enumerate(concentrations):\n",
    "        for j, time in enumerate(desired_timepoints):\n",
    "            ax = axes[i, j]\n",
    "            # Filter data for specific concentration and timepoint\n",
    "            if time == 0:\n",
    "                data_subset = df_gr_gated[(df_gr_gated['time'] == 0)]\n",
    "                color = concentration_color_map[0]\n",
    "                title = \"Common 0min\"\n",
    "            else:\n",
    "                data_subset = df_gr_gated[(df_gr_gated['Dex_conc'] == conc) & (df_gr_gated['time'] == time)]\n",
    "                color = concentration_color_map[conc]\n",
    "                title = f\"{time}min\"\n",
    "\n",
    "            sns.histplot(data=data_subset, x=metric, kde=True, color=color, linewidth=1, bins=20, ax=ax, element='bars', fill=True, line_kws={'linewidth': 5}, legend=False, alpha=0.9)\n",
    "            # ax.set_xlim(axis_limits[metric])\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel(None)  # Only show x-axis label on the bottom plots\n",
    "            ax.set_ylabel('Density' if j == 0 else '')  # Only show y-axis label on the first column\n",
    "\n",
    "    plt.suptitle(f\"{labels[metric]} by Dexamethasone Concentration and Time\", fontsize=24, fontweight='bold')\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to make room for the main title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GR Split-Voilin Plots\n",
    "\n",
    "# Set the Seaborn style\n",
    "sns.set_theme(style=\"ticks\", palette=\"colorblind\", context=\"poster\", font='times new roman')\n",
    "\n",
    "# Define the concentrations and specific timepoints you want to plot\n",
    "concentrations = [0, 1, 10, 100]  # Including 0 concentration for baseline\n",
    "desired_timepoints = [0, 10, 30, 50, 75, 120, 180]\n",
    "\n",
    "# Prepare a DataFrame to store baseline values for each metric at time 0\n",
    "baseline_data = df_gr_gated[df_gr_gated['Dex_conc'] == 0].groupby('time').mean(numeric_only=True)\n",
    "\n",
    "# Metrics to plot\n",
    "metrics = ['nucGRint', 'cytoGRint'] # , 'normGRnuc', 'normGRcyt', 'nuc_cyto_ratio'\n",
    "\n",
    "# Calculate Log2 fold changes\n",
    "for metric in metrics:\n",
    "    baseline_column = f'baseline_{metric}'\n",
    "    baseline_data[baseline_column] = baseline_data[metric]\n",
    "    df_gr_gated[f'log2fc_{metric}'] = df_gr_gated.apply(\n",
    "    lambda row: np.log2(row[metric] / baseline_data.at[0, metric]) if row['time'] != 0 else 0, axis=1)\n",
    "\n",
    "# Melt the DataFrame to have 'metric' and 'log2fc_value' columns\n",
    "melted_GR_data = df_gr_gated.melt(id_vars=['Dex_conc', 'time'], value_vars=[f'log2fc_{m}' for m in metrics],\n",
    "                              var_name='metric', value_name='log2fc_value')\n",
    "\n",
    "# Update the 'metric' column to have nicer labels\n",
    "melted_GR_data['metric'] = melted_GR_data['metric'].str.replace('log2fc_', '').str.replace('_', ' ').str.title()\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, len(desired_timepoints), figsize=(15, 5), sharey=True)\n",
    "fig.suptitle('GR Intensities', fontsize=20, fontweight='bold', y=0.9)\n",
    "\n",
    "for j, time in enumerate(desired_timepoints):\n",
    "    ax = axes[j]\n",
    "    data_subset = melted_GR_data[melted_GR_data['time'] == time]\n",
    "    sns.violinplot(x='Dex_conc', y='log2fc_value', hue='metric', data=data_subset, linewidth=2.0, ax=ax, scale=\"width\", split=True)\n",
    "    ax.set_title(f\"{time} min\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(None)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='x', labelsize=10, width=2)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "    if j > 0:\n",
    "        ax.set_ylabel('')  # Remove the y-label for all but the first subplot\n",
    "    else:\n",
    "        ax.set_ylabel('Log2 Fold Change', fontsize=16, fontweight='bold')\n",
    "    ax.get_legend().remove()  # Remove the legend\n",
    "\n",
    "# Set x-label for all subplots\n",
    "fig.text(0.5, 0.004, 'Dexamethasone Concentration (nM)', ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GR Intensity Over Time Separate concentration plots\n",
    "\n",
    "# Make a copy of the GR data\n",
    "GR_data = df_gr_gated.copy()\n",
    "\n",
    "# Calculate means for each replica\n",
    "replica_means = GR_data.groupby(['Dex_conc', 'time', 'replica']).agg({\n",
    "    'nucGRint': 'mean',\n",
    "    'cytoGRint': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate the mean and standard deviation of the replica means\n",
    "summary_stats = replica_means.groupby(['Dex_conc', 'time']).agg({\n",
    "    'nucGRint': ['mean', 'std'],\n",
    "    'cytoGRint': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for easier access\n",
    "summary_stats.columns = ['Dex_conc', 'time', 'mean_nuc_int', 'std_nuc_int', 'mean_cyto_int', 'std_cyto_int']\n",
    "\n",
    "# Calculate overall mean and standard deviation for each concentration and time point\n",
    "overall_stats = GR_data.groupby(['Dex_conc', 'time']).agg({\n",
    "    'nucGRint': ['mean', 'std'],\n",
    "    'cytoGRint': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for easier access\n",
    "overall_stats.columns = ['Dex_conc', 'time', 'overall_mean_nuc', 'overall_std_nuc', 'overall_mean_cyto', 'overall_std_cyto']\n",
    "\n",
    "# Set Style\n",
    "sns.set_theme(style=\"ticks\", palette=\"colorblind\", context=\"poster\", font='times new roman')\n",
    "\n",
    "# Define the color palette for Nuclear and Cytoplasmic intensities\n",
    "colors_nuc_cyto = sns.color_palette(\"colorblind\", 2)  # Two colors: one for Nuclear, one for Cytoplasmic\n",
    "\n",
    "# Define the desired time points\n",
    "desired_timepoints = [0, 10, 30, 50, 75, 120, 180]\n",
    "\n",
    "for conc in [1, 10, 100]:\n",
    "    # Filter data for plotting\n",
    "    subset_summary = summary_stats[(summary_stats['Dex_conc'] == conc) & (summary_stats['time'].isin(desired_timepoints))]\n",
    "    subset_overall = overall_stats[(overall_stats['Dex_conc'] == conc) & (overall_stats['time'].isin(desired_timepoints))]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plotting Nuclear Intensity Mean with Error Bars\n",
    "    plt.errorbar(subset_summary['time'], subset_summary['mean_nuc_int'],\n",
    "                 yerr=subset_summary['std_nuc_int'], fmt='-o', color=colors_nuc_cyto[0], capsize=5,\n",
    "                 label='Nuclear Intensity')\n",
    "\n",
    "    # Filling between std deviations for overall data - Nuclear\n",
    "    plt.fill_between(subset_overall['time'],\n",
    "                     subset_overall['overall_mean_nuc'] - subset_overall['overall_std_nuc'],\n",
    "                     subset_overall['overall_mean_nuc'] + subset_overall['overall_std_nuc'],\n",
    "                     color=colors_nuc_cyto[0], alpha=0.2, label='Total Data Spread - Nuclear')\n",
    "\n",
    "    # Plotting Cytoplasmic Intensity Mean with Error Bars\n",
    "    plt.errorbar(subset_summary['time'], subset_summary['mean_cyto_int'],\n",
    "                 yerr=subset_summary['std_cyto_int'], fmt='--o', color=colors_nuc_cyto[1], capsize=5,\n",
    "                 label='Cytoplasmic Intensity')\n",
    "\n",
    "    # Filling between std deviations for overall data - Cytoplasmic\n",
    "    plt.fill_between(subset_overall['time'],\n",
    "                     subset_overall['overall_mean_cyto'] - subset_overall['overall_std_cyto'],\n",
    "                     subset_overall['overall_mean_cyto'] + subset_overall['overall_std_cyto'],\n",
    "                     color=colors_nuc_cyto[1], alpha=0.2, label='Total Data Spread - Cytoplasmic')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'{conc} nM Dex', fontsize=20, fontweight='bold')\n",
    "    plt.xlabel('Time (min)', fontsize=14)\n",
    "    plt.ylabel('Intensity (AU)', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    #plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7) SAVE THE UPDATED GATED DATAFRAMES\n",
    "# # =========================\n",
    "# # Gated DUSP1 (unchanged except row filtering)\n",
    "# df_dusp_gated.to_csv(\"DUSP1_ALL_gated_V01.csv\", index=False)\n",
    "\n",
    "# # Gated GR with new columns: CalcCytoArea, normGRnuc, normGRcyt\n",
    "# df_gr_gated.to_csv(\"GR_ALL_gated_with_CytoArea_and_normGR_V01.csv\", index=False)\n",
    "\n",
    "# print(\"Saved gated DUSP1 to 'DUSP1_ALL_gated.csv'\")\n",
    "# print(\"Saved gated GR to 'GR_ALL_gated_with_CytoArea_and_normGR.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
