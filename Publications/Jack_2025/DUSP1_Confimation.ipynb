{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUSP1 Confirmation Notebook\n",
    "The purpose of this notebook is to:\n",
    "1. Confirm successful segmentation.\n",
    "2. Confirm successful BigFISH spot and cluster detection.\n",
    "3. Refine spots and clusters through additional filtering (SNR) for gating and final dataframe preparation (determine if best before or after total concatenation):  \n",
    "    a. Find SNR threshold.  \n",
    "    b. Filter `df_spots`.  \n",
    "    c. (Optional) Check to see if removed spot was in a cluster (very unlikely due to how clusters are defined).  \n",
    "    d. Create final dataframes (`df_spots`, `df_clusters`, `df_cellspots`, `df_cellprops`).  \n",
    "    e. Save the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)\n",
    "\n",
    "matplotlib_logger = logging.getLogger('matplotlib')\n",
    "matplotlib_logger.setLevel(logging.WARNING)\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "print(src_path)\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src.Analysis import AnalysisManager, Analysis, SpotDetection_SNRConfirmation, Spot_Cluster_Analysis_WeightedSNR, GR_Confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads in the data from specified location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = None \n",
    "log_location = r'/Volumes/share/Users/Eric/AngelFISH_data'  #  r'/Volumes/share/Users/Jack/All_Analysis'\n",
    "am = AnalysisManager(location=loc, log_location=log_location, mac=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all analysis done \n",
    "am.list_analysis_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can filter on naDe and dates\n",
    "am.select_analysis('DUSP1_D_Jan2125')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(am.analysis_names)\n",
    "am.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does analysis/confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select DUSP1 spot detection\n",
    "# SD = SpotDetection_Confirmation(am)\n",
    "SD = Spot_Cluster_Analysis_WeightedSNR(am)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads the data into memory \n",
    "SD.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this multiple times to see a new randomly selected cell\n",
    "SD.display(newFOV=True, newCell=True) # num_fovs_to_display=2,num_cells_to_display=2, num_spots_to_display=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spots = pd.DataFrame(SD.spots)\n",
    "df_clusters = pd.DataFrame(SD.clusters)\n",
    "df_cellspots = pd.DataFrame(SD.cellspots)\n",
    "df_cellprops = pd.DataFrame(SD.cellprops)\n",
    "\n",
    "# Print columns for each dataframe\n",
    "print(\"df_spots columns:\")\n",
    "print(\", \".join(df_spots.columns))\n",
    "\n",
    "print(\"\\ndf_clusters columns:\")\n",
    "print(\", \".join(df_clusters.columns))\n",
    "\n",
    "print(\"\\ndf_cellspots columns:\")\n",
    "print(\", \".join(df_cellspots.columns))\n",
    "\n",
    "print(\"\\ndf_cellprops columns:\")\n",
    "print(\", \".join(df_cellprops.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of timepoints in the dataset\n",
    "print(df_spots['time'].unique())\n",
    "print(df_spots['h5_idx'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for all times\n",
    "stats = df_spots.groupby('time')['snr'].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Dynamic SNR thresholding based on mean ± 2*std\n",
    "thresholds = {}\n",
    "for time, row in stats.iterrows():\n",
    "    thresholds[time] = (row['mean'] - 2 * row['std'], row['mean'] + 2 * row['std'])\n",
    "\n",
    "# Apply dynamic thresholding\n",
    "df_spots['threshold_pass'] = df_spots.apply(\n",
    "    lambda row: thresholds[row['time']][0] <= row['snr'] <= thresholds[row['time']][1],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filtered DataFrame\n",
    "df_spots_filtered = df_spots[df_spots['threshold_pass']]\n",
    "\n",
    "# Categorize noise levels based on the paper's description\n",
    "def categorize_noise(snr):\n",
    "    if snr < 2:\n",
    "        return 'very_high_noise'\n",
    "    if 2 <= snr < 5:\n",
    "        return 'snr:2-5'\n",
    "    elif 8 <= snr <= 26:\n",
    "        return 'snr:8-26'\n",
    "    elif snr > 26:\n",
    "        return 'snr>26'\n",
    "    else:\n",
    "        return 'did this work?'\n",
    "\n",
    "df_spots['noise_level'] = df_spots['snr'].apply(categorize_noise)\n",
    "\n",
    "# Plot histograms of SNR for each time\n",
    "for time, group in df_spots.groupby('time'):\n",
    "    mean_snr = group['snr'].mean()\n",
    "    median_snr = group['snr'].median()\n",
    "    std_snr = group['snr'].std()\n",
    "    \n",
    "    plt.hist(group['snr'], bins=50, alpha=0.7, label=f'time: {time}')\n",
    "    plt.axvline(mean_snr, color='b', linestyle='dashed', linewidth=1, label=f'Mean: {mean_snr:.2f}')\n",
    "    plt.axvline(median_snr, color='g', linestyle='dashed', linewidth=1, label=f'Median: {median_snr:.2f}')\n",
    "    plt.title(f'SNR Histogram for time {time}min')\n",
    "    plt.xlabel('SNR')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Scatter Plot of Intensity vs SNR\n",
    "plt.scatter(df_spots['signal'], df_spots['snr'], s=1, alpha=0.7)\n",
    "plt.xlabel('Intensity')\n",
    "plt.ylabel('SNR')\n",
    "plt.title('Intensity vs SNR (All Spots)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# Noise Level Distribution\n",
    "noise_level_counts = df_spots['noise_level'].value_counts()\n",
    "plt.bar(noise_level_counts.index, noise_level_counts.values, alpha=0.7)\n",
    "plt.title('Noise Level Distribution')\n",
    "plt.xlabel('Noise Level')\n",
    "plt.ylabel('Spot Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spots_filtered = df_spots[(df_spots['cell_label'] > 0)] # & (df_spots['cluster_index'] > -1)]\n",
    "print(len(df_spots_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame(SD.clusters)\n",
    "df_clusters.columns\n",
    "df_clusters_filtered = df_clusters[(df_clusters['is_nuc'] > 0)]\n",
    "print(len(df_clusters_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters['is_nuc'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.cellprops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(SD.cellprops) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cellspots = pd.DataFrame(SD.cellspots)\n",
    "df_cellspots.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_TS = df_cellspots[(df_cellspots['nb_transcription_site'] > 0)]\n",
    "print(len(num_TS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cells that have props but arent in the cell spots\n",
    "allcells = SD.cellprops\n",
    "cells_wSpots = SD.cellspots\n",
    "# Find cells that are in allcells but not in cells_wSpots\n",
    "merged = allcells.merge(cells_wSpots, how='left', left_on=['nuc_label', 'fov', 'NAS_location'], right_on=['cell_id', 'fov', 'NAS_location'], indicator=True)\n",
    "print(merged.shape)\n",
    "same_entries = merged[merged['_merge'] == 'both'].drop(columns=['cell_id', '_merge'])\n",
    "different_entries = merged[merged['_merge'] == 'left_only'].drop(columns=['cell_id', '_merge'])\n",
    "\n",
    "print(\"Same entries:\")\n",
    "print(same_entries.shape)\n",
    "print(\"\\nDifferent entries:\")\n",
    "print(different_entries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import dask.array as da\n",
    "print(f'There are {allcells.shape[0]} cells in this data set')\n",
    "print(f'There are {cells_wSpots.shape[0]} cells with spots')\n",
    "\n",
    "# how many are have bounded boxes touching the border\n",
    "print(f'{different_entries['touching_border'].sum()} cells are touching the border and are not counted')\n",
    "\n",
    "# Select a random row from the different_entries dataframe\n",
    "for _ in range(2):\n",
    "    random_row = different_entries[~different_entries['touching_border']].sample(n=1).iloc[0]\n",
    "\n",
    "    # Read the h5 file\n",
    "    h5_file = random_row['NAS_location']\n",
    "    h5_file = os.path.join(r'\\\\munsky-nas.engr.colostate.edu\\share', h5_file) # TODO this will need to be updated so you dont have to find it to get it to work\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # Grab the mask and raw_image\n",
    "        masks = da.from_array(f['/masks'])\n",
    "        raw_images = da.from_array(f['/raw_images'])\n",
    "\n",
    "        # Extract the bounding box coordinates\n",
    "        bbox = [random_row['cell_bbox-0'], random_row['cell_bbox-1'], random_row['cell_bbox-2'], random_row['cell_bbox-3']]\n",
    "\n",
    "        img = raw_images[random_row['fov'], random_row['timepoint_x']].squeeze()\n",
    "        for c in range(img.shape[0]):\n",
    "            # Display the raw image with the selected cell highlighted\n",
    "            t = np.max(img[c, :, :,:], axis=0)\n",
    "            t.compute()\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "            ax.imshow(t, cmap='gray')\n",
    "            rect = plt.Rectangle((bbox[1], bbox[0]), bbox[3] - bbox[1], bbox[2] - bbox[0], edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of spots\n",
    "keys_to_plot = ['signal', 'snr']\n",
    "for k in SD.spots.keys():\n",
    "    if k in keys_to_plot:\n",
    "        # Plot histogram for 'area'\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(SD.spots[k], bins=200, density=True)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Histogram of {k}')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD.cellspots.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot counts as a function of time and dex\n",
    "keys_to_plot = ['nb_rna', 'nb_rna_in_nuc']\n",
    "\n",
    "\n",
    "tp_set = sorted(set(SD.cellspots['time']))\n",
    "dex_set = sorted(set(SD.cellspots['Dex_Conc']))\n",
    "for k in keys_to_plot:\n",
    "    fig, axs = plt.subplots(len(tp_set), len(dex_set), figsize=(15, 15))\n",
    "    fig.suptitle(f'{k} as a function of time and dex', fontsize=16)\n",
    "    for i_d, d in enumerate(dex_set):\n",
    "        data = SD.cellspots[SD.cellspots['Dex_Conc'] == d]\n",
    "        for i_t, t in enumerate(tp_set):\n",
    "            temp = data[data['time'] == t]\n",
    "            mean_val = temp[k].mean()\n",
    "            std_val = temp[k].std()\n",
    "            if d == 0 and t == 0:\n",
    "                for ax in axs[i_t, :]:\n",
    "                    ax.hist(temp[k], bins=200, density=True)\n",
    "                    ax.axvline(mean_val, color='r', linestyle='solid', linewidth=2)\n",
    "                    ax.axvline(mean_val + std_val, color='g', linestyle='dashed', linewidth=1)\n",
    "                    ax.axvline(mean_val - std_val, color='g', linestyle='dashed', linewidth=1)\n",
    "                    ax.set_xlim([0, SD.cellspots[k].max()])\n",
    "                    ax.grid(True)  # Turn on grid lines\n",
    "                    if i_t != len(tp_set) - 1:\n",
    "                        axs[i_t, i_d].set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "                axs[i_t, 0].set_ylabel(f'Time: {t}')\n",
    "            else:\n",
    "                axs[i_t, i_d].hist(temp[k], bins=200, density=True)\n",
    "                axs[i_t, i_d].axvline(mean_val, color='r', linestyle='solid', linewidth=2)\n",
    "                axs[i_t, i_d].axvline(mean_val + std_val, color='g', linestyle='dashed', linewidth=1)\n",
    "                axs[i_t, i_d].axvline(mean_val - std_val, color='g', linestyle='dashed', linewidth=1)\n",
    "                axs[i_t, i_d].set_xlim([0, SD.cellspots[k].max()])\n",
    "                axs[i_t, i_d].grid(True)  # Turn on grid lines\n",
    "                if i_t != len(tp_set) - 1:\n",
    "                    axs[i_t, i_d].set_xticks([])\n",
    "                axs[i_t, i_d].set_yticks([])\n",
    "                axs[i_t, 0].set_ylabel(f'Time: {t}')\n",
    "                axs[0, i_d].set_title(f'Dex: {d}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGFISH Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigfish\n",
    "import bigfish.stack as stack\n",
    "import bigfish.multistack as multistack\n",
    "import bigfish.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Approach\n",
    "\n",
    "In this approach, every spot is retained but given a weight reflecting its estimated reliability. Higher weights indicate higher trust in that spot.\n",
    "\n",
    "### Example Weight Scheme\n",
    "Based on the paper’s reported errors:\n",
    "\n",
    "- **SNR < 2**: Discard (weight = 0.0)\n",
    "- **2 ≤ SNR < 5**: High noise, ~24% error ⇒ reliability ~ 76% ⇒ weight = 0.76\n",
    "- **5 ≤ SNR < 8**: Not explicitly given, but presumably better than 24% error ⇒ weight = 0.85\n",
    "- **8 ≤ SNR ≤ 26**: Medium/low noise, ~1.4–5.5% error ⇒ reliability ~ 94–98.6% ⇒ weight = 0.95\n",
    "- **SNR > 26**: Very low noise, ~1.4% (or lower) error ⇒ ~98.6% reliability ⇒ weight = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted snr analysis\n",
    "\n",
    "# 1. Assign Weights Based on SNR Ranges\n",
    "def assign_weight(snr):\n",
    "    \"\"\"Return a weight based on known or estimated detection reliability.\"\"\"\n",
    "    if snr < 2:\n",
    "        # Very noisy => discard\n",
    "        return 0.0\n",
    "    elif 2 <= snr < 5:\n",
    "        # High noise => ~24% error => ~76% reliable\n",
    "        return 0.76\n",
    "    elif 5 <= snr < 8:\n",
    "        # Intermediate between high & medium => guess ~85% reliability\n",
    "        return 0.85\n",
    "    elif 8 <= snr <= 26:\n",
    "        # Medium/low noise => 1.4–5.5% error => pick ~95% reliability\n",
    "        return 0.95\n",
    "    else:  # snr > 26\n",
    "        return 1\n",
    "\n",
    "df_spots['weight'] = df_spots['snr'].apply(assign_weight)\n",
    "\n",
    "# 2. Weighted Histogram of SNR\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(\n",
    "    df_spots['snr'], \n",
    "    bins=50, \n",
    "    weights=df_spots['weight'], \n",
    "    alpha=0.7,\n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('Weighted Histogram of SNR')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Weighted Count')\n",
    "plt.show()\n",
    "\n",
    "# 3. Compare Weighted vs. Unweighted Histograms\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Unweighted\n",
    "plt.hist(\n",
    "    df_spots['snr'], \n",
    "    bins=50, \n",
    "    alpha=0.5, \n",
    "    label='Unweighted', \n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Weighted\n",
    "plt.hist(\n",
    "    df_spots['snr'], \n",
    "    bins=50, \n",
    "    weights=df_spots['weight'], \n",
    "    alpha=0.7, \n",
    "    label='Weighted', \n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('SNR Distribution: Unweighted vs. Weighted')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Count (or Weighted Count)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Weighted Scatter Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(\n",
    "    df_spots['snr'], \n",
    "    df_spots['signal'], \n",
    "    s=10 * df_spots['weight'],  # scale dot size by weight\n",
    "    c=df_spots['weight'],       # color also by weight (optional)\n",
    "    cmap='viridis', \n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('Scatter Plot of SNR vs. Signal (Marker Size Weighted)')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Signal')\n",
    "plt.colorbar(label='Weight')\n",
    "plt.yscale('log')  # if you want log scale for signal\n",
    "plt.show()\n",
    "\n",
    "# 5. Basic Statistics Showing Mean Weight per SNR Range\n",
    "bins = [0, 2, 5, 8, 26, np.inf]\n",
    "labels = ['<2', '2–5', '5–8', '8–26', '>26']\n",
    "df_spots['snr_bin'] = pd.cut(df_spots['snr'], bins=bins, labels=labels)\n",
    "\n",
    "stats_weight = df_spots.groupby('snr_bin')['weight'].agg(['count'])\n",
    "print(\"weighted Count by SNR Bin:\")\n",
    "print(stats_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Approach\n",
    "\n",
    "Here, instead of giving each spot a continuous weight, we use a Bernoulli process to decide per spot whether it’s kept or discarded, based on an estimated probability of being a true positive (TP).\n",
    "\n",
    "### Estimate Probability of True Positive \\(P(TP|SNR)\\)\n",
    "Sample from a Bernoulli distribution:\n",
    "\n",
    "### Example Probability Scheme\n",
    "Using the same error logic:\n",
    "\n",
    "- **SNR < 2**: \\(P(TP) = 0\\) (discard)\n",
    "- **2 ≤ SNR < 5**: error \\(\\approx 24\\%\\) ⇒ \\(P(TP) = 0.76\\)\n",
    "- **5 ≤ SNR < 8**: intermediate guess \\(P(TP) = 0.85\\)\n",
    "- **8 ≤ SNR ≤ 26**: error \\(\\approx 1.4\\% - 5.5\\%\\) ⇒ \\(P(TP) = 0.95\\)\n",
    "- **SNR > 26**: error \\(\\approx 1.4\\%\\) ⇒ \\(P(TP) \\approx 0.986\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Probability of True Positive\n",
    "def prob_true_positive(snr):\n",
    "    \"\"\"Return P(TP) based on known or estimated detection error.\"\"\"\n",
    "    if snr < 2:\n",
    "        return 0.0\n",
    "    elif 2 <= snr < 5:\n",
    "        return 0.76\n",
    "    elif 5 <= snr < 8:\n",
    "        return 0.85\n",
    "    elif 8 <= snr <= 26:\n",
    "        return 0.95\n",
    "    else:  # snr > 26\n",
    "        return 0.986\n",
    "\n",
    "df_spots['pTP'] = df_spots['snr'].apply(prob_true_positive)\n",
    "\n",
    "# 2. Single Realization (One Draw per Spot)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "df_spots['keep_prob_approach'] = np.random.rand(len(df_spots)) < df_spots['pTP']\n",
    "\n",
    "# 3. Visualize Kept vs. Discarded\n",
    "df_kept = df_spots[df_spots['keep_prob_approach'] == True]\n",
    "df_disc = df_spots[df_spots['keep_prob_approach'] == False]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(\n",
    "    df_spots['snr'], \n",
    "    bins=50, \n",
    "    alpha=0.4, \n",
    "    label='All Spots', \n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.hist(\n",
    "    df_kept['snr'], \n",
    "    bins=50, \n",
    "    alpha=0.7, \n",
    "    label='Kept (Prob. Approach)', \n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('SNR Distribution: Probabilistic Keeping')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Compare Number of Spots Kept vs. Discarded\n",
    "kept_count = len(df_kept)\n",
    "discarded_count = len(df_disc)\n",
    "print(f\"Kept Spots: {kept_count}, Discarded Spots: {discarded_count}\")\n",
    "\n",
    "# 5. (Optional) Multiple Realizations\n",
    "# If you want to see how stable the approach is over multiple runs\n",
    "n_runs = 5\n",
    "kept_fractions = []\n",
    "for i in range(n_runs):\n",
    "    # Draw Bernoulli for each spot\n",
    "    keep_vec = np.random.rand(len(df_spots)) < df_spots['pTP']\n",
    "    # Fraction of spots kept\n",
    "    kept_fraction = keep_vec.mean()\n",
    "    kept_fractions.append(kept_fraction)\n",
    "\n",
    "print(\"Kept fractions over multiple runs:\", kept_fractions)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(range(n_runs), kept_fractions, marker='o', linestyle='--')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Fraction of Spots Kept Over Multiple Probabilistic Runs')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Fraction Kept')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
