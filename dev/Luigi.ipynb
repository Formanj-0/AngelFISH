{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator, Extra\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from numpydantic import NDArray\n",
    "import dask.array as da\n",
    "from pycromanager import Dataset\n",
    "import numpy as np\n",
    "import luigi\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import json\n",
    "\n",
    "def convert_to_pydantic_safe(value):\n",
    "    if isinstance(value, (np.integer, np.int32, np.int64)):\n",
    "        return int(value)\n",
    "    elif isinstance(value, (np.floating, np.float32, np.float64)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return [convert_to_pydantic_safe(v) for v in value]\n",
    "    elif isinstance(value, dict):\n",
    "        return {k: convert_to_pydantic_safe(v) for k, v in value.items()}\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "class Parameters(BaseModel, extra=Extra.allow):\n",
    "    voxel_size_yx: float = 130\n",
    "    voxel_size_z: float = 500\n",
    "    spot_z: float = 100\n",
    "    spot_yx: float = 360\n",
    "    index_dict: Optional[Dict[str, Any]] = None\n",
    "    nucChannel: Optional[int] = None\n",
    "    cytoChannel: Optional[int] = None\n",
    "    FISHChannel: Optional[int] = None\n",
    "    independent_params: List[Dict[str, Any]] = [{}]\n",
    "    timestep_s: Optional[float] = None\n",
    "    local_dataset_location: Optional[Union[str, List]] = None\n",
    "    nas_location: Optional[Union[str, List]] = None\n",
    "    num_images: Optional[int] = None\n",
    "    images: Optional[NDArray] = da.ones((0, 0, 0))\n",
    "    masks: Optional[NDArray] = da.ones((0, 0, 0))\n",
    "    image_path: Optional[str] = None\n",
    "    mask_path: Optional[str] = None\n",
    "    clear_after_error: bool = True\n",
    "    name: Optional[str] = None\n",
    "    NUMBER_OF_CORES: int = 4\n",
    "    num_images_to_run: int = 100000000\n",
    "    connection_config_location: str = 'c:\\\\Users\\\\formanj\\\\GitHub\\\\AngelFISH\\\\config_nas.yml'\n",
    "    display_plots: bool = True\n",
    "    load_in_mask: bool = False\n",
    "    mask_structure: Optional[Any] = None\n",
    "    order: str = 'pt'\n",
    "    data_dir: str = None\n",
    "    share_name: str = 'share'\n",
    "\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "        use_enum_values = True\n",
    "\n",
    "\n",
    "class Data(BaseModel, ABC, extra=Extra.allow):\n",
    "    nas_location: str\n",
    "    history: Optional[List[str]] = []\n",
    "    image_path: Optional[str] = None\n",
    "    mask_path: Optional[str] = None\n",
    "    images: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    masks: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    independent_params: List[Dict[str, Any]] = [{}]\n",
    "\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "        use_enum_values = True\n",
    "\n",
    "    def append_dict(self, data: dict):\n",
    "        for k, v in data.items():\n",
    "            setattr(self, k, convert_to_pydantic_safe(v))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataSingle(Data, extra=Extra.allow):\n",
    "    p: int\n",
    "    t: int\n",
    "    image: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    mask: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    independent_params: List[Dict[str, Any]] = [{}]\n",
    "\n",
    "\n",
    "class DataBulk(Data, extra=Extra.allow):\n",
    "    images: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    masks: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    \n",
    "    def split(p, t) -> DataSingle:\n",
    "        pass\n",
    "\n",
    "    def merge(data: list):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor(luigi.Task, ABC):\n",
    "    param_path = luigi.Parameter()\n",
    "    step_name: str = \"base\"\n",
    "    modify_images = False\n",
    "    modify_masks = False\n",
    "    previous_task = luigi.Parameter()\n",
    "    nas_location = luigi.Parameter()\n",
    "\n",
    "    def requires(self):\n",
    "        \"\"\"Allow for the chaining of tasks. Returns dependencies if needed.\"\"\"\n",
    "        return self.previous_task() if self.previous_task is not None else None# Return other tasks that need to run first\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"Define the output of this task.\"\"\"\n",
    "        return luigi.LocalTarget(f\"{self.step_name}_output.json\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self, **kwargs):\n",
    "        \"\"\"Define the actual processing logic.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the processing task.\"\"\"\n",
    "        print(f'Processing {os.path.basename(self.nas_location)} with {self.step_name}')\n",
    "\n",
    "        if self.output().exists():\n",
    "            print('Results already exist')\n",
    "    \n",
    "        with open(self.param_path, 'r') as json_file: # load in params\n",
    "            params = Parameters.model_validate_json(json.load(json_file))\n",
    "\n",
    "        with open(self.input().path, 'r') as json_file: # load in data\n",
    "            data = Data.model_validate_json(json.load(json_file))\n",
    "\n",
    "        if data is not None:\n",
    "            kwargs = {**params.model_dump(), **data.model_dump()}\n",
    "        else:\n",
    "            kwargs = {**params.model_dump()}\n",
    "\n",
    "        results = self.eval(**kwargs)\n",
    "\n",
    "        data.append_dict(results)\n",
    "    \n",
    "        with self.output().open('w') as f:\n",
    "            json.dump(data.model_dump_json(round_trip=True), f)\n",
    "\n",
    "class SingleImageProcessingTask(ImageProcessor):\n",
    "    p = luigi.IntParameter()\n",
    "    t = luigi.IntParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        \"\"\"Allow for the chaining of tasks. Returns dependencies if needed.\"\"\"\n",
    "        if type(self.previous_task) is dict:\n",
    "            return self.previous_task[self.p][self.t]\n",
    "        elif self.previous_task is not None:\n",
    "            return None\n",
    "        else:\n",
    "            return self.previous_task # Return other tasks that need to run first\n",
    "\n",
    "    def output(self):\n",
    "        dataName = os.path.basename(self.nas_location)\n",
    "        cache_key = f'{self.step_name}-p_{self.p}-t_{self.t}-{dataName}.json'\n",
    "        return luigi.LocalTarget(cache_key)\n",
    "\n",
    "    def run(self):\n",
    "        print(f'Processing {os.path.basename(self.nas_location)}, {self.p}, {self.t} with {self.step_name}')\n",
    "\n",
    "        if self.output().exists():\n",
    "            raise 'Result already exist'\n",
    "\n",
    "        if self.output().exists():\n",
    "            print('Results already exist')\n",
    "    \n",
    "        with open(self.param_path, 'r') as json_file: # load in params\n",
    "            params = Parameters.model_validate_json(json.load(json_file))\n",
    "\n",
    "        with open(self.input().path, 'r') as json_file: # load in data\n",
    "            data = DataSingle.model_validate_json(json.load(json_file))\n",
    "    \n",
    "        if data is not None:\n",
    "            kwargs = {**params.model_dump(), **data.model_dump()}\n",
    "        else:\n",
    "            kwargs = {**params.model_dump()}\n",
    "\n",
    "        results = self.eval(**kwargs)\n",
    "\n",
    "        data.append_dict(results)\n",
    "\n",
    "        with self.output().open('w') as f:\n",
    "            if not self.modify_images and not self.modify_masks:\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['image', 'images', 'masks', 'mask']), f)\n",
    "            elif not self.modify_images and self.modify_masks :\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['masks', 'mask']), f)\n",
    "            elif self.modify_images and not self.modify_masks :\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['image', 'images']), f)\n",
    "            else:\n",
    "                json.dump(data.model_dump_json(round_trip=True), f)\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BulkImageProcessingTask(ImageProcessor):\n",
    "    def output(self):\n",
    "        dataName = os.path.basename(self.nas_location)\n",
    "        cache_key = f'{self.step_name}-{dataName}.json'\n",
    "        return luigi.LocalTarget(cache_key)\n",
    "\n",
    "    def run(self):\n",
    "        print(f'Processing {os.path.basename(self.nas_location)} with {self.step_name}')\n",
    "\n",
    "        if self.output().exists():\n",
    "            print('Results already exist')\n",
    "    \n",
    "        with open(self.param_path, 'r') as json_file: # load in params\n",
    "            params = Parameters.model_validate_json(json.load(json_file))\n",
    "\n",
    "        if self.previous_task is not None:\n",
    "            with open(self.input().path, 'r') as json_file: # load in data\n",
    "                data = DataBulk.model_validate_json(json.load(json_file))\n",
    "        else: \n",
    "            data = None\n",
    "    \n",
    "        if data is not None:\n",
    "            kwargs = {**params.model_dump(), **data.model_dump()}\n",
    "        else:\n",
    "            kwargs = {**params.model_dump()}\n",
    "\n",
    "        results = self.eval(**kwargs)\n",
    "\n",
    "        if data is not None:\n",
    "            data.append_dict(results)\n",
    "        else:\n",
    "            data = DataBulk(nas_location=self.nas_location).append_dict(results)\n",
    "    \n",
    "        with self.output().open('w') as f:\n",
    "            if not self.modify_images and not self.modify_masks:\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['image', 'images', 'masks', 'mask']), f)\n",
    "            elif not self.modify_images and self.modify_masks :\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['masks', 'mask']), f)\n",
    "            elif self.modify_images and not self.modify_masks :\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['image', 'images']), f)\n",
    "            else:\n",
    "                json.dump(data.model_dump_json(round_trip=True), f)\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(luigi.Task):\n",
    "    nas_location = luigi.Parameter()\n",
    "    previous_task = luigi.Parameter()\n",
    "    positions = luigi.IntParameter()\n",
    "    time_points = luigi.IntParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        return self.previous_task()\n",
    "    \n",
    "    def output(self):\n",
    "        dataName = os.path.basename(self.nas_location)\n",
    "        return {p: {t: luigi.LocalTarget(f'Split-p_{p}-t_{t}-{dataName}.json') for t in self.time_points} for p in self.positions}\n",
    "\n",
    "    def run(self): # convert DataBulk -> {DataSingles}\n",
    "        with open(self.input().path, 'r') as json_file: # load in params\n",
    "            bulkData = DataBulk.model_validate_json(json.load(json_file))\n",
    "        \n",
    "        for p in self.positions:\n",
    "            for t in self.time_points:\n",
    "                singleData = bulkData.split(p=p, t=t)\n",
    "                with self.output()[p][t].open('w') as f:\n",
    "                    json.dump(singleData.model_dump_json(round_trip=True), f)\n",
    "\n",
    "class Merger(luigi.Task):\n",
    "    nas_location = luigi.Parameter()\n",
    "    previous_task = luigi.Parameter()\n",
    "    positions = luigi.IntParameter()\n",
    "    time_points = luigi.IntParameter()\n",
    "    \n",
    "    def requires(self):\n",
    "        return {p: {t: self.previous_task(p=p, t=t) for t in self.time_points} for p in self.positions}\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"Define the output for the merged result.\"\"\"\n",
    "        data_name = os.path.basename(self.nas_location)\n",
    "        return luigi.LocalTarget(f'Merged-{data_name}.json')\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Merge all the split data.\"\"\"\n",
    "        merged_data = []\n",
    "        for p in self.positions:\n",
    "            for t in self.time_points:\n",
    "                with self.input()[p][t].open('r') as f:\n",
    "                    single_data = DataSingle.model_validate_json(json.load(f))\n",
    "                    merged_data.append(single_data)\n",
    "        \n",
    "        bulk_data = DataBulk.merge(merged_data)\n",
    "        \n",
    "        with self.output().open('w') as f:\n",
    "            json.dump(bulk_data.model_dump_json(round_trip=True), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessSingles(luigi.WrapperTask):\n",
    "    steps: List[Any]\n",
    "    positions: List[int]\n",
    "    time_points: List[int]\n",
    "    \n",
    "    def requires(self):\n",
    "        \"\"\"For each position and time point, run the processing steps.\"\"\"\n",
    "        tasks = []\n",
    "        for step in self.steps:\n",
    "            for p in self.positions:\n",
    "                for t in self.time_points:\n",
    "                    # Each time-point/position combination will require specific tasks\n",
    "                    task = step(p=p, t=t, previous_task=Splitter, nas_location=self.nas_location)\n",
    "                    tasks.append(task)\n",
    "        return tasks\n",
    "\n",
    "class ProcessBulk(luigi.WrapperTask):\n",
    "    steps: List[Any]\n",
    "    nas_location: str\n",
    "\n",
    "    def requires(self):\n",
    "        tasks = []\n",
    "        for step in self.steps:\n",
    "            task = step(param_path=self.param_path, nas_location=self.nas_location, previous_task=Merger)\n",
    "            tasks.append(task)\n",
    "        return tasks\n",
    "\n",
    "class Workflow(luigi.WrapperTask):\n",
    "    params: Parameters\n",
    "    steps: List[Any]\n",
    "    positions: List[int]\n",
    "    time_points: List[int]\n",
    "    nas_location: str\n",
    "    param_path: str\n",
    "    \n",
    "    def requires(self):\n",
    "        return [\n",
    "            ProcessSingles(steps=self.steps, positions=self.positions, time_points=self.time_points, nas_location=self.nas_location),\n",
    "            ProcessBulk(steps=self.steps, nas_location=self.nas_location, param_path=self.param_path)\n",
    "        ]\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"The final output of the pipeline.\"\"\"\n",
    "        return luigi.LocalTarget(\"final_output.json\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"This is where the final result is collected and written to disk.\"\"\"\n",
    "        with self.output().open('w') as f:\n",
    "            f.write(\"Final result of configurable pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# logger = logging.getLogger('luigi-interface')\n",
    "# logger.setLevel(logging.ERROR)\n",
    "# logger.propagate = False\n",
    "\n",
    "\n",
    "# Simple Example\n",
    "# Create some fake data using dask arrays\n",
    "import dask.array as da\n",
    "\n",
    "# Create a fake 3D image dataset with dimensions (time, height, width)\n",
    "fake_images = da.random.random((10, 100, 100), chunks=(1, 100, 100))\n",
    "\n",
    "# Create a fake 3D mask dataset with the same dimensions\n",
    "fake_masks = da.random.randint(0, 2, size=(10, 100, 100), chunks=(1, 100, 100))\n",
    "\n",
    "# Save the fake data to disk\n",
    "# Save the fake data to disk as JSON\n",
    "fake_images_list = fake_images.compute().tolist()\n",
    "fake_masks_list = fake_masks.compute().tolist()\n",
    "\n",
    "with open('fake_images.json', 'w') as f:\n",
    "    json.dump(fake_images_list, f)\n",
    "\n",
    "with open('fake_masks.json', 'w') as f:\n",
    "    json.dump(fake_masks_list, f)\n",
    "\n",
    "data = DataBulk(images=fake_images, masks=fake_masks, nas_location='somewhere', history=['step1', 'step2'], p=0, t=0)\n",
    "\n",
    "# Define parameters\n",
    "params = Parameters(\n",
    "    voxel_size_yx=130,\n",
    "    voxel_size_z=500,\n",
    "    spot_z=100,\n",
    "    spot_yx=360,\n",
    "    num_images=10,\n",
    "    images=fake_images,\n",
    "    masks=fake_masks,\n",
    "    image_path='fake_images',\n",
    "    mask_path='fake_masks',\n",
    "    data_dir='data'\n",
    ")\n",
    "\n",
    "with open('params.json', 'w') as f:\n",
    "    json.dump(params.model_dump_json(round_trip=True), f)\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data.model_dump_json(round_trip=True), f)\n",
    "\n",
    "# Instantiate and run the task\n",
    "class ExampleSingleImageProcessingTask(BulkImageProcessingTask):\n",
    "    param_path = 'params.json'\n",
    "    nas_location = 'dataset1'\n",
    "    p = 0\n",
    "    t = 0\n",
    "    previous_task = None\n",
    "\n",
    "    def eval(self, **kwargs):\n",
    "        # Example processing: just return the sum of the image and mask\n",
    "        image_sum = kwargs['images'].sum().compute()\n",
    "        mask_sum = kwargs['masks'].sum().compute()\n",
    "        return {'image_sum': image_sum, 'mask_sum': mask_sum}\n",
    "\n",
    "# Instantiate and run the task\n",
    "task = ExampleSingleImageProcessingTask()\n",
    "luigi.build([task], local_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complicated Example\n",
    "# Create some fake data using dask arrays\n",
    "\n",
    "# Create a fake 3D image dataset with dimensions (time, height, width)\n",
    "images = da.random.random((5, 7, 3, 10, 100, 100)) # p, t, c, z, y, x\n",
    "\n",
    "# Save the fake data to disk as JSON\n",
    "fake_images_list = fake_images.compute().tolist()\n",
    "fake_masks_list = fake_masks.compute().tolist()\n",
    "\n",
    "with open('fake_images.json', 'w') as f:\n",
    "    json.dump(fake_images_list, f)\n",
    "\n",
    "with open('fake_masks.json', 'w') as f:\n",
    "    json.dump(fake_masks_list, f)\n",
    "\n",
    "data = DataBulk(images=fake_images, masks=fake_masks, nas_location='somewhere', history=['step1', 'step2'], p=0, t=0)\n",
    "\n",
    "# Define parameters\n",
    "params = Parameters(\n",
    "    voxel_size_yx=130,\n",
    "    voxel_size_z=500,\n",
    "    spot_z=100,\n",
    "    spot_yx=360,\n",
    "    num_images=10,\n",
    "    images=fake_images,\n",
    "    masks=fake_masks,\n",
    "    image_path='fake_images',\n",
    "    mask_path='fake_masks',\n",
    "    data_dir='data'\n",
    ")\n",
    "\n",
    "with open('params.json', 'w') as f:\n",
    "    json.dump(params.model_dump_json(round_trip=True), f)\n",
    "\n",
    "\n",
    "class LoadInData(BulkImageProcessingTask):\n",
    "    def eval(self, image_path, **kwargs):\n",
    "        return {'load_in_data': image_path}\n",
    "\n",
    "class GenerateMasks(SingleImageProcessingTask):\n",
    "    def eval(self, images, **kwargs):\n",
    "        # Create a fake 3D mask dataset with the same dimensions\n",
    "        nuc_mask = da.random.randint(0, 2, size=(10, 100, 100)) # p, t, z, y, x\n",
    "        return {'nuc_mask': nuc_mask}\n",
    "    \n",
    "# Instantiate and run the task\n",
    "class MeasureSomething(SingleImageProcessingTask):\n",
    "    def eval(self, images, nuc_masks, **kwargs):\n",
    "        # Example processing: just return the sum of the image and mask\n",
    "        image_sum = images.sum().compute()\n",
    "        mask_sum = nuc_masks.sum().compute()\n",
    "        return {'image_sum': image_sum, 'mask_sum': mask_sum}\n",
    "\n",
    "# Instantiate and run the task\n",
    "class SaveSomething(BulkImageProcessingTask):\n",
    "    def eval(self, **kwargs):\n",
    "        print('something happened')\n",
    "        return None\n",
    "\n",
    "# Instantiate and run the task\n",
    "task = ExampleSingleImageProcessingTask()\n",
    "luigi.build([LoadInData(nas_location='dataset2', param_path='params.json', previous_task=None), \n",
    "             Splitter(nas_location='dataset2', previous_task=LoadInData(nas_location='dataset2', param_path='params.json', previous_task=None), positions=list(np.arange(5)), time_points=list(np.arange(7))), \n",
    "             *[MeasureSomething(nas_location='dataset2', param_path='params.json', previous_task=Splitter(nas_location='dataset2', previous_task=LoadInData(nas_location='dataset2', param_path='params.json', previous_task=None), positions=list(np.arange(5)), time_points=list(np.arange(7))), p=p, t=t) for p in range(5) for t in range(7)], \n",
    "             Merger(nas_location='dataset2', previous_task=MeasureSomething, positions=list(np.arange(5)), time_points=list(np.arange(7))),\n",
    "             SaveSomething(nas_location='dataset2', param_path='params.json', previous_task=Merger(nas_location='dataset2', previous_task=MeasureSomething, positions=list(np.arange(5)), time_points=list(np.arange(7))))], \n",
    "             local_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
