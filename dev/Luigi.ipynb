{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:luigi-interface:Loaded []\n",
      "c:\\temp\\ipykernel_17156\\341192665.py:24: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  class Parameters(BaseModel, extra=Extra.allow):\n",
      "c:\\temp\\ipykernel_17156\\341192665.py:59: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  class Data(BaseModel, ABC, extra=Extra.allow):\n",
      "c:\\temp\\ipykernel_17156\\341192665.py:79: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  class DataSingle(Data, extra=Extra.allow):\n",
      "c:\\temp\\ipykernel_17156\\341192665.py:87: PydanticDeprecatedSince20: `pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  class DataBulk(Data, extra=Extra.allow):\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, field_validator, Extra\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from numpydantic import NDArray\n",
    "import dask.array as da\n",
    "from pycromanager import Dataset\n",
    "import numpy as np\n",
    "import luigi\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import json\n",
    "\n",
    "def convert_to_pydantic_safe(value):\n",
    "    if isinstance(value, (np.integer, np.int32, np.int64)):\n",
    "        return int(value)\n",
    "    elif isinstance(value, (np.floating, np.float32, np.float64)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return [convert_to_pydantic_safe(v) for v in value]\n",
    "    elif isinstance(value, dict):\n",
    "        return {k: convert_to_pydantic_safe(v) for k, v in value.items()}\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "class Parameters(BaseModel, extra=Extra.allow):\n",
    "    voxel_size_yx: float = 130\n",
    "    voxel_size_z: float = 500\n",
    "    spot_z: float = 100\n",
    "    spot_yx: float = 360\n",
    "    index_dict: Optional[Dict[str, Any]] = None\n",
    "    nucChannel: Optional[int] = None\n",
    "    cytoChannel: Optional[int] = None\n",
    "    FISHChannel: Optional[int] = None\n",
    "    independent_params: List[Dict[str, Any]] = [{}]\n",
    "    timestep_s: Optional[float] = None\n",
    "    local_dataset_location: Optional[Union[str, List]] = None\n",
    "    nas_location: Optional[Union[str, List]] = None\n",
    "    num_images: Optional[int] = None\n",
    "    images: Optional[NDArray] = da.ones((0, 0, 0))\n",
    "    masks: Optional[NDArray] = da.ones((0, 0, 0))\n",
    "    image_path: Optional[str] = None\n",
    "    mask_path: Optional[str] = None\n",
    "    clear_after_error: bool = True\n",
    "    name: Optional[str] = None\n",
    "    NUMBER_OF_CORES: int = 4\n",
    "    num_images_to_run: int = 100000000\n",
    "    connection_config_location: str = 'c:\\\\Users\\\\formanj\\\\GitHub\\\\AngelFISH\\\\config_nas.yml'\n",
    "    display_plots: bool = True\n",
    "    load_in_mask: bool = False\n",
    "    mask_structure: Optional[Any] = None\n",
    "    order: str = 'pt'\n",
    "    data_dir: str = None\n",
    "    share_name: str = 'share'\n",
    "\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "        use_enum_values = True\n",
    "\n",
    "\n",
    "class Data(BaseModel, ABC, extra=Extra.allow):\n",
    "    nas_location: str\n",
    "    history: Optional[List[str]] = []\n",
    "    image_path: Optional[str] = None\n",
    "    mask_path: Optional[str] = None\n",
    "    images: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    masks: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    independent_params: List[Dict[str, Any]] = [{}]\n",
    "\n",
    "    class Config:\n",
    "        extra = 'allow'\n",
    "        use_enum_values = True\n",
    "\n",
    "    def append_dict(self, data: dict):\n",
    "        for k, v in data.items():\n",
    "            setattr(self, k, convert_to_pydantic_safe(v))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class DataSingle(Data, extra=Extra.allow):\n",
    "    p: int\n",
    "    t: int\n",
    "    image: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    mask: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    independent_params: List[Dict[str, Any]] = [{}]\n",
    "\n",
    "\n",
    "class DataBulk(Data, extra=Extra.allow):\n",
    "    images: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    masks: Optional[NDArray] = da.empty((0, 0, 0))\n",
    "    \n",
    "    def split(self, p, t) -> DataSingle:\n",
    "        return DataSingle(nas_location=self.nas_location,\n",
    "                        history=self.history,\n",
    "                        image_path=self.image_path,\n",
    "                        mask_path=self.mask_path,\n",
    "                        images=self.images,\n",
    "                        masks=self.masks,\n",
    "                        independent_params=self.independent_params,\n",
    "                        p=p\n",
    "                        t=t\n",
    "                        image=self.images[p, t]\n",
    "                        mask==self.images[p, t]\n",
    "                        \n",
    "\n",
    "    def merge(self, data: list):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor(luigi.Task, ABC):\n",
    "    param_path = luigi.Parameter()\n",
    "    step_name: str = \"base\"\n",
    "    modify_images = False\n",
    "    modify_masks = False\n",
    "    previous_task = luigi.Parameter()\n",
    "    nas_location = luigi.Parameter()\n",
    "\n",
    "    def requires(self):\n",
    "        \"\"\"Allow for the chaining of tasks. Returns dependencies if needed.\"\"\"\n",
    "        return self.previous_task if self.previous_task is not None else None# Return other tasks that need to run first\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"Define the output of this task.\"\"\"\n",
    "        return luigi.LocalTarget(f\"{self.step_name}_output.json\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self, **kwargs):\n",
    "        \"\"\"Define the actual processing logic.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the processing task.\"\"\"\n",
    "        print(f'Processing {os.path.basename(self.nas_location)} with {self.step_name}')\n",
    "\n",
    "        if self.output().exists():\n",
    "            print('Results already exist')\n",
    "    \n",
    "        with open(self.param_path, 'r') as json_file: # load in params\n",
    "            params = Parameters.model_validate_json(json.load(json_file))\n",
    "\n",
    "        with open(self.input().path, 'r') as json_file: # load in data\n",
    "            data = Data.model_validate_json(json.load(json_file))\n",
    "\n",
    "        if data is not None:\n",
    "            kwargs = {**params.model_dump(), **data.model_dump()}\n",
    "        else:\n",
    "            kwargs = {**params.model_dump()}\n",
    "\n",
    "        results = self.eval(**kwargs)\n",
    "\n",
    "        data.append_dict(results)\n",
    "    \n",
    "        with self.output().open('w') as f:\n",
    "            json.dump(data.model_dump_json(round_trip=True), f)\n",
    "\n",
    "class SingleImageProcessingTask(ImageProcessor):\n",
    "    p = luigi.IntParameter()\n",
    "    t = luigi.IntParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        \"\"\"Allow for the chaining of tasks. Returns dependencies if needed.\"\"\"\n",
    "        if type(self.previous_task) is dict:\n",
    "            return self.previous_task[self.p][self.t]\n",
    "        elif self.previous_task is not None:\n",
    "            return None\n",
    "        else:\n",
    "            return self.previous_task # Return other tasks that need to run first\n",
    "\n",
    "    def output(self):\n",
    "        dataName = os.path.basename(self.nas_location)\n",
    "        cache_key = f'{self.step_name}-p_{self.p}-t_{self.t}-{dataName}.json'\n",
    "        return luigi.LocalTarget(cache_key)\n",
    "\n",
    "    def run(self):\n",
    "        print(f'Processing {os.path.basename(self.nas_location)}, {self.p}, {self.t} with {self.step_name}')\n",
    "\n",
    "        if self.output().exists():\n",
    "            raise 'Result already exist'\n",
    "\n",
    "        if self.output().exists():\n",
    "            print('Results already exist')\n",
    "    \n",
    "        with open(self.param_path, 'r') as json_file: # load in params\n",
    "            params = Parameters.model_validate_json(json.load(json_file))\n",
    "\n",
    "        with open(self.input().path, 'r') as json_file: # load in data\n",
    "            data = DataSingle.model_validate_json(json.load(json_file))\n",
    "    \n",
    "        if data is not None:\n",
    "            kwargs = {**params.model_dump(), **data.model_dump()}\n",
    "        else:\n",
    "            kwargs = {**params.model_dump()}\n",
    "\n",
    "        results = self.eval(**kwargs)\n",
    "\n",
    "        data.append_dict(results)\n",
    "\n",
    "        with self.output().open('w') as f:\n",
    "            if not self.modify_images and not self.modify_masks:\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['image', 'images', 'masks', 'mask']), f)\n",
    "            elif not self.modify_images and self.modify_masks :\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['masks', 'mask']), f)\n",
    "            elif self.modify_images and not self.modify_masks :\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['image', 'images']), f)\n",
    "            else:\n",
    "                json.dump(data.model_dump_json(round_trip=True), f)\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BulkImageProcessingTask(ImageProcessor):\n",
    "    def output(self):\n",
    "        dataName = os.path.basename(self.nas_location)\n",
    "        cache_key = f'{self.step_name}-{dataName}.json'\n",
    "        return luigi.LocalTarget(cache_key)\n",
    "\n",
    "    def run(self):\n",
    "        print(f'Processing {os.path.basename(self.nas_location)} with {self.step_name}')\n",
    "\n",
    "        if self.output().exists():\n",
    "            print('Results already exist')\n",
    "    \n",
    "        with open(self.param_path, 'r') as json_file: # load in params\n",
    "            params = Parameters.model_validate_json(json.load(json_file))\n",
    "\n",
    "        if self.previous_task is not None:\n",
    "            with open(self.input().path, 'r') as json_file: # load in data\n",
    "                data = DataBulk.model_validate_json(json.load(json_file))\n",
    "        else: \n",
    "            data = None\n",
    "    \n",
    "        if data is not None:\n",
    "            kwargs = {**params.model_dump(), **data.model_dump()}\n",
    "        else:\n",
    "            kwargs = {**params.model_dump()}\n",
    "\n",
    "        results = self.eval(**kwargs)\n",
    "\n",
    "        if data is not None:\n",
    "            data.append_dict(results)\n",
    "        else:\n",
    "            data = DataBulk(nas_location=self.nas_location).append_dict(results)\n",
    "    \n",
    "        with self.output().open('w') as f:\n",
    "            if not self.modify_images and not self.modify_masks:\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['image', 'images', 'masks', 'mask']), f)\n",
    "            elif not self.modify_images and self.modify_masks :\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['masks', 'mask']), f)\n",
    "            elif self.modify_images and not self.modify_masks :\n",
    "                json.dump(data.model_dump_json(round_trip=True, exclude=['image', 'images']), f)\n",
    "            else:\n",
    "                json.dump(data.model_dump_json(round_trip=True), f)\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(luigi.Task):\n",
    "    nas_location = luigi.Parameter()\n",
    "    previous_task = luigi.Parameter()\n",
    "    positions = luigi.IntParameter()\n",
    "    time_points = luigi.IntParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        return self.previous_task if self.previous_task is not None else None# Return other tasks that need to run first\n",
    "\n",
    "    \n",
    "    def output(self):\n",
    "        dataName = os.path.basename(self.nas_location)\n",
    "        return {p: {t: luigi.LocalTarget(f'Split-p_{p}-t_{t}-{dataName}.json') for t in self.time_points} for p in self.positions}\n",
    "\n",
    "    def run(self): # convert DataBulk -> {DataSingles}\n",
    "        with open(self.input().path, 'r') as json_file: # load in params\n",
    "            bulkData = DataBulk.model_validate_json(json.load(json_file))\n",
    "        \n",
    "        for p in self.positions:\n",
    "            for t in self.time_points:\n",
    "                singleData = bulkData.split(p=p, t=t)\n",
    "                with self.output()[p][t].open('w') as f:\n",
    "                    json.dump(singleData.model_dump_json(round_trip=True), f)\n",
    "\n",
    "class Merger(luigi.Task):\n",
    "    nas_location = luigi.Parameter()\n",
    "    previous_task = luigi.Parameter()\n",
    "    positions = luigi.IntParameter()\n",
    "    time_points = luigi.IntParameter()\n",
    "    \n",
    "    def requires(self):\n",
    "        return {p: {t: self.previous_task(p=p, t=t) for t in self.time_points} for p in self.positions}\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"Define the output for the merged result.\"\"\"\n",
    "        data_name = os.path.basename(self.nas_location)\n",
    "        return luigi.LocalTarget(f'Merged-{data_name}.json')\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Merge all the split data.\"\"\"\n",
    "        merged_data = []\n",
    "        for p in self.positions:\n",
    "            for t in self.time_points:\n",
    "                with self.input()[p][t].open('r') as f:\n",
    "                    single_data = DataSingle.model_validate_json(json.load(f))\n",
    "                    merged_data.append(single_data)\n",
    "        \n",
    "        bulk_data = DataBulk.merge(merged_data)\n",
    "        \n",
    "        with self.output().open('w') as f:\n",
    "            json.dump(bulk_data.model_dump_json(round_trip=True), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessSingles(luigi.WrapperTask):\n",
    "    steps: List[Any]\n",
    "    positions: List[int]\n",
    "    time_points: List[int]\n",
    "    \n",
    "    def requires(self):\n",
    "        \"\"\"For each position and time point, run the processing steps.\"\"\"\n",
    "        tasks = []\n",
    "        for step in self.steps:\n",
    "            for p in self.positions:\n",
    "                for t in self.time_points:\n",
    "                    # Each time-point/position combination will require specific tasks\n",
    "                    task = step(p=p, t=t, previous_task=Splitter, nas_location=self.nas_location)\n",
    "                    tasks.append(task)\n",
    "        return tasks\n",
    "\n",
    "class ProcessBulk(luigi.WrapperTask):\n",
    "    steps: List[Any]\n",
    "    nas_location: str\n",
    "\n",
    "    def requires(self):\n",
    "        tasks = []\n",
    "        for step in self.steps:\n",
    "            task = step(param_path=self.param_path, nas_location=self.nas_location, previous_task=Merger)\n",
    "            tasks.append(task)\n",
    "        return tasks\n",
    "\n",
    "class Workflow(luigi.WrapperTask):\n",
    "    params: Parameters\n",
    "    steps: List[Any]\n",
    "    positions: List[int]\n",
    "    time_points: List[int]\n",
    "    nas_location: str\n",
    "    param_path: str\n",
    "    \n",
    "    def requires(self):\n",
    "        return [\n",
    "            ProcessSingles(steps=self.steps, positions=self.positions, time_points=self.time_points, nas_location=self.nas_location),\n",
    "            ProcessBulk(steps=self.steps, nas_location=self.nas_location, param_path=self.param_path)\n",
    "        ]\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"The final output of the pipeline.\"\"\"\n",
    "        return luigi.LocalTarget(\"final_output.json\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"This is where the final result is collected and written to disk.\"\"\"\n",
    "        with self.output().open('w') as f:\n",
    "            f.write(\"Final result of configurable pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:luigi:logging already configured\n",
      "DEBUG: Checking if ExampleSingleImageProcessingTask() is complete\n",
      "DEBUG:luigi-interface:Checking if ExampleSingleImageProcessingTask() is complete\n",
      "INFO: Informed scheduler that task   ExampleSingleImageProcessingTask__99914b932b   has status   PENDING\n",
      "INFO:luigi-interface:Informed scheduler that task   ExampleSingleImageProcessingTask__99914b932b   has status   PENDING\n",
      "INFO: Done scheduling tasks\n",
      "INFO:luigi-interface:Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "INFO:luigi-interface:Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG:luigi-interface:Asking scheduler for work...\n",
      "DEBUG:luigi.scheduler:Starting pruning of task graph\n",
      "DEBUG:luigi.scheduler:Done pruning task graph\n",
      "DEBUG: Pending tasks: 1\n",
      "DEBUG:luigi-interface:Pending tasks: 1\n",
      "INFO: [pid 17156] Worker Worker(salt=6887437780, workers=1, host=ens13487, username=formanj, pid=17156) running   ExampleSingleImageProcessingTask()\n",
      "INFO:luigi-interface:[pid 17156] Worker Worker(salt=6887437780, workers=1, host=ens13487, username=formanj, pid=17156) running   ExampleSingleImageProcessingTask()\n",
      "INFO: [pid 17156] Worker Worker(salt=6887437780, workers=1, host=ens13487, username=formanj, pid=17156) done      ExampleSingleImageProcessingTask()\n",
      "INFO:luigi-interface:[pid 17156] Worker Worker(salt=6887437780, workers=1, host=ens13487, username=formanj, pid=17156) done      ExampleSingleImageProcessingTask()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "DEBUG:luigi-interface:1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   ExampleSingleImageProcessingTask__99914b932b   has status   DONE\n",
      "INFO:luigi-interface:Informed scheduler that task   ExampleSingleImageProcessingTask__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG:luigi-interface:Asking scheduler for work...\n",
      "DEBUG:luigi.scheduler:Starting pruning of task graph\n",
      "DEBUG:luigi.scheduler:Done pruning task graph\n",
      "DEBUG: Done\n",
      "DEBUG:luigi-interface:Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "DEBUG:luigi-interface:There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=6887437780, workers=1, host=ens13487, username=formanj, pid=17156) was stopped. Shutting down Keep-Alive thread\n",
      "INFO:luigi-interface:Worker Worker(salt=6887437780, workers=1, host=ens13487, username=formanj, pid=17156) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 ran successfully:\n",
      "    - 1 ExampleSingleImageProcessingTask()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "INFO:luigi-interface:\n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 ran successfully:\n",
      "    - 1 ExampleSingleImageProcessingTask()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset1 with base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import logging\n",
    "\n",
    "# logger = logging.getLogger('luigi-interface')\n",
    "# logger.setLevel(logging.ERROR)\n",
    "# logger.propagate = False\n",
    "\n",
    "\n",
    "# Simple Example\n",
    "# Create some fake data using dask arrays\n",
    "import dask.array as da\n",
    "\n",
    "# Create a fake 3D image dataset with dimensions (time, height, width)\n",
    "fake_images = da.random.random((10, 100, 100), chunks=(1, 100, 100))\n",
    "\n",
    "# Create a fake 3D mask dataset with the same dimensions\n",
    "fake_masks = da.random.randint(0, 2, size=(10, 100, 100), chunks=(1, 100, 100))\n",
    "\n",
    "# Save the fake data to disk\n",
    "# Save the fake data to disk as JSON\n",
    "fake_images_list = fake_images.compute().tolist()\n",
    "fake_masks_list = fake_masks.compute().tolist()\n",
    "\n",
    "with open('fake_images.json', 'w') as f:\n",
    "    json.dump(fake_images_list, f)\n",
    "\n",
    "with open('fake_masks.json', 'w') as f:\n",
    "    json.dump(fake_masks_list, f)\n",
    "\n",
    "data = DataBulk(images=fake_images, masks=fake_masks, nas_location='somewhere', history=['step1', 'step2'], p=0, t=0)\n",
    "\n",
    "# Define parameters\n",
    "params = Parameters(\n",
    "    voxel_size_yx=130,\n",
    "    voxel_size_z=500,\n",
    "    spot_z=100,\n",
    "    spot_yx=360,\n",
    "    num_images=10,\n",
    "    images=fake_images,\n",
    "    masks=fake_masks,\n",
    "    image_path='fake_images',\n",
    "    mask_path='fake_masks',\n",
    "    data_dir='data'\n",
    ")\n",
    "\n",
    "with open('params.json', 'w') as f:\n",
    "    json.dump(params.model_dump_json(round_trip=True), f)\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data.model_dump_json(round_trip=True), f)\n",
    "\n",
    "# Instantiate and run the task\n",
    "class ExampleSingleImageProcessingTask(BulkImageProcessingTask):\n",
    "    param_path = 'params.json'\n",
    "    nas_location = 'dataset1'\n",
    "    p = 0\n",
    "    t = 0\n",
    "    previous_task = None\n",
    "\n",
    "    def eval(self, **kwargs):\n",
    "        # Example processing: just return the sum of the image and mask\n",
    "        image_sum = kwargs['images'].sum().compute()\n",
    "        mask_sum = kwargs['masks'].sum().compute()\n",
    "        return {'image_sum': image_sum, 'mask_sum': mask_sum}\n",
    "\n",
    "# Instantiate and run the task\n",
    "task = ExampleSingleImageProcessingTask()\n",
    "luigi.build([task], local_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:luigi:logging already configured\n",
      "DEBUG: Checking if LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2) is complete\n",
      "DEBUG:luigi-interface:Checking if LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2) is complete\n",
      "INFO: Informed scheduler that task   LoadInData_dataset2_params_json_None_5d69d39df8   has status   DONE\n",
      "INFO:luigi-interface:Informed scheduler that task   LoadInData_dataset2_params_json_None_5d69d39df8   has status   DONE\n",
      "DEBUG: Checking if Splitter(nas_location=dataset2, previous_task=LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2), positions=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)), time_points=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6))) is complete\n",
      "DEBUG:luigi-interface:Checking if Splitter(nas_location=dataset2, previous_task=LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2), positions=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)), time_points=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6))) is complete\n",
      "DEBUG: Checking if LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2) is complete\n",
      "DEBUG:luigi-interface:Checking if LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2) is complete\n",
      "INFO: Informed scheduler that task   Splitter_dataset2__np_int64_0___np_LoadInData_param_8cf5dc9d04   has status   PENDING\n",
      "INFO:luigi-interface:Informed scheduler that task   Splitter_dataset2__np_int64_0___np_LoadInData_param_8cf5dc9d04   has status   PENDING\n",
      "INFO: Informed scheduler that task   LoadInData_dataset2_params_json_None_5d69d39df8   has status   DONE\n",
      "INFO:luigi-interface:Informed scheduler that task   LoadInData_dataset2_params_json_None_5d69d39df8   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO:luigi-interface:Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "INFO:luigi-interface:Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG:luigi-interface:Asking scheduler for work...\n",
      "DEBUG:luigi.scheduler:Starting pruning of task graph\n",
      "DEBUG:luigi.scheduler:Done pruning task graph\n",
      "DEBUG: Pending tasks: 1\n",
      "DEBUG:luigi-interface:Pending tasks: 1\n",
      "INFO: [pid 17156] Worker Worker(salt=1239577169, workers=1, host=ens13487, username=formanj, pid=17156) running   Splitter(nas_location=dataset2, previous_task=LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2), positions=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)), time_points=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)))\n",
      "INFO:luigi-interface:[pid 17156] Worker Worker(salt=1239577169, workers=1, host=ens13487, username=formanj, pid=17156) running   Splitter(nas_location=dataset2, previous_task=LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2), positions=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)), time_points=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)))\n",
      "ERROR: [pid 17156] Worker Worker(salt=1239577169, workers=1, host=ens13487, username=formanj, pid=17156) failed    Splitter(nas_location=dataset2, previous_task=LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2), positions=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)), time_points=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)))\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\formanj\\GitHub\\AngelFISH\\.venv\\Lib\\site-packages\\luigi\\worker.py\", line 210, in run\n",
      "    new_deps = self._run_get_new_deps()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\formanj\\GitHub\\AngelFISH\\.venv\\Lib\\site-packages\\luigi\\worker.py\", line 138, in _run_get_new_deps\n",
      "    task_gen = self.task.run()\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\temp\\ipykernel_17156\\948814932.py\", line 21, in run\n",
      "    singleData = bulkData.split(p=p, t=t)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: DataBulk.split() got multiple values for argument 'p'\n",
      "ERROR:luigi-interface:[pid 17156] Worker Worker(salt=1239577169, workers=1, host=ens13487, username=formanj, pid=17156) failed    Splitter(nas_location=dataset2, previous_task=LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2), positions=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)), time_points=(np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)))\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\formanj\\GitHub\\AngelFISH\\.venv\\Lib\\site-packages\\luigi\\worker.py\", line 210, in run\n",
      "    new_deps = self._run_get_new_deps()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\formanj\\GitHub\\AngelFISH\\.venv\\Lib\\site-packages\\luigi\\worker.py\", line 138, in _run_get_new_deps\n",
      "    task_gen = self.task.run()\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\temp\\ipykernel_17156\\948814932.py\", line 21, in run\n",
      "    singleData = bulkData.split(p=p, t=t)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: DataBulk.split() got multiple values for argument 'p'\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "DEBUG:luigi-interface:1 running tasks, waiting for next task to finish\n",
      "DEBUG:luigi.scheduler:Splitter_dataset2__np_int64_0___np_LoadInData_param_8cf5dc9d04 task num failures is 1 and limit is 999999999\n",
      "INFO: Informed scheduler that task   Splitter_dataset2__np_int64_0___np_LoadInData_param_8cf5dc9d04   has status   FAILED\n",
      "INFO:luigi-interface:Informed scheduler that task   Splitter_dataset2__np_int64_0___np_LoadInData_param_8cf5dc9d04   has status   FAILED\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG:luigi-interface:Asking scheduler for work...\n",
      "DEBUG:luigi.scheduler:Starting pruning of task graph\n",
      "DEBUG:luigi.scheduler:Done pruning task graph\n",
      "DEBUG: Done\n",
      "DEBUG:luigi-interface:Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "DEBUG:luigi-interface:There are no more tasks to run at this time\n",
      "DEBUG: There are 1 pending tasks possibly being run by other workers\n",
      "DEBUG:luigi-interface:There are 1 pending tasks possibly being run by other workers\n",
      "DEBUG: There are 1 pending tasks unique to this worker\n",
      "DEBUG:luigi-interface:There are 1 pending tasks unique to this worker\n",
      "DEBUG: There are 1 pending tasks last scheduled by this worker\n",
      "DEBUG:luigi-interface:There are 1 pending tasks last scheduled by this worker\n",
      "INFO: Worker Worker(salt=1239577169, workers=1, host=ens13487, username=formanj, pid=17156) was stopped. Shutting down Keep-Alive thread\n",
      "INFO:luigi-interface:Worker Worker(salt=1239577169, workers=1, host=ens13487, username=formanj, pid=17156) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2)\n",
      "* 1 failed:\n",
      "    - 1 Splitter(...)\n",
      "\n",
      "This progress looks :( because there were failed tasks\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "INFO:luigi-interface:\n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 LoadInData(param_path=params.json, previous_task=None, nas_location=dataset2)\n",
      "* 1 failed:\n",
      "    - 1 Splitter(...)\n",
      "\n",
      "This progress looks :( because there were failed tasks\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More complicated Example\n",
    "# Create some fake data using dask arrays\n",
    "\n",
    "# Create a fake 3D image dataset with dimensions (time, height, width)\n",
    "images = da.random.random((5, 7, 3, 10, 100, 100)) # p, t, c, z, y, x\n",
    "\n",
    "# Save the fake data to disk as JSON\n",
    "fake_images_list = fake_images.compute().tolist()\n",
    "fake_masks_list = fake_masks.compute().tolist()\n",
    "\n",
    "with open('fake_images.json', 'w') as f:\n",
    "    json.dump(fake_images_list, f)\n",
    "\n",
    "with open('fake_masks.json', 'w') as f:\n",
    "    json.dump(fake_masks_list, f)\n",
    "\n",
    "data = DataBulk(images=fake_images, masks=fake_masks, nas_location='somewhere', history=['step1', 'step2'], p=0, t=0)\n",
    "\n",
    "# Define parameters\n",
    "params = Parameters(\n",
    "    voxel_size_yx=130,\n",
    "    voxel_size_z=500,\n",
    "    spot_z=100,\n",
    "    spot_yx=360,\n",
    "    num_images=10,\n",
    "    images=fake_images,\n",
    "    masks=fake_masks,\n",
    "    image_path='fake_images',\n",
    "    mask_path='fake_masks',\n",
    "    data_dir='data'\n",
    ")\n",
    "\n",
    "with open('params.json', 'w') as f:\n",
    "    json.dump(params.model_dump_json(round_trip=True), f)\n",
    "\n",
    "\n",
    "class LoadInData(BulkImageProcessingTask):\n",
    "    def eval(self, image_path, **kwargs):\n",
    "        return {'load_in_data': image_path}\n",
    "\n",
    "class GenerateMasks(SingleImageProcessingTask):\n",
    "    def eval(self, images, **kwargs):\n",
    "        # Create a fake 3D mask dataset with the same dimensions\n",
    "        nuc_mask = da.random.randint(0, 2, size=(10, 100, 100)) # p, t, z, y, x\n",
    "        return {'nuc_mask': nuc_mask}\n",
    "    \n",
    "# Instantiate and run the task\n",
    "class MeasureSomething(SingleImageProcessingTask):\n",
    "    def eval(self, images, nuc_masks, **kwargs):\n",
    "        # Example processing: just return the sum of the image and mask\n",
    "        image_sum = images.sum().compute()\n",
    "        mask_sum = nuc_masks.sum().compute()\n",
    "        return {'image_sum': image_sum, 'mask_sum': mask_sum}\n",
    "\n",
    "# Instantiate and run the task\n",
    "class SaveSomething(BulkImageProcessingTask):\n",
    "    def eval(self, **kwargs):\n",
    "        print('something happened')\n",
    "        return None\n",
    "\n",
    "# Instantiate and run the task\n",
    "task = ExampleSingleImageProcessingTask()\n",
    "luigi.build([LoadInData(nas_location='dataset2', param_path='params.json', previous_task=None), \n",
    "             Splitter(nas_location='dataset2', previous_task=LoadInData(nas_location='dataset2', param_path='params.json', previous_task=None), positions=list(np.arange(5)), time_points=list(np.arange(7)))],\n",
    "            #  *[MeasureSomething(nas_location='dataset2', param_path='params.json', previous_task=Splitter(nas_location='dataset2', previous_task=LoadInData(nas_location='dataset2', param_path='params.json', previous_task=None), positions=list(np.arange(5)), time_points=list(np.arange(7))), p=p, t=t) for p in range(5) for t in range(7)], \n",
    "            #  Merger(nas_location='dataset2', previous_task={p: {t: MeasureSomething(nas_location='dataset2', param_path='params.json', previous_task=Splitter(nas_location='dataset2', previous_task=LoadInData(nas_location='dataset2', param_path='params.json', previous_task=None), positions=list(np.arange(5)), time_points=list(np.arange(7))), p=p, t=t) for t in range(7)} for p in range(5)}, positions=list(np.arange(5)), time_points=list(np.arange(7))),\n",
    "            #  SaveSomething(nas_location='dataset2', param_path='params.json', previous_task=Merger(nas_location='dataset2', previous_task={p: {t: MeasureSomething(nas_location='dataset2', param_path='params.json', previous_task=Splitter(nas_location='dataset2', previous_task=LoadInData(nas_location='dataset2', param_path='params.json', previous_task=None), positions=list(np.arange(5)), time_points=list(np.arange(7))), p=p, t=t) for t in range(7)} for p in range(5)}, positions=list(np.arange(5)), time_points=list(np.arange(7))))], \n",
    "             local_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
